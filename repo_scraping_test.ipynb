{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import acquire\n",
    "import time\n",
    "import pandas as pd\n",
    "from env import github_token, github_username\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://github.com/search?o=desc&p=1&q=petroleum&s=stars&type=Repositories'\n",
    "# headers = {'User-Agent': 'manual search'} \n",
    "# response = get(url, headers=headers)\n",
    "# # print(response.status_code)\n",
    "# soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find_all('a', class_='v-align-middle')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_names(url):\n",
    "    headers = {'User-Agent': 'manual search'} \n",
    "    response = get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    repos = []\n",
    "    for i in range(10):\n",
    "        repos.append(soup.find_all('a', class_='v-align-middle')\\\n",
    "                 [i].text)\n",
    "        time.sleep(1)\n",
    "    return repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://github.com/search?o=desc&p=1&q=petroleum&s=stars&type=Repositories'\n",
    "# get_repo_names(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = [f'https://github.com/search?o=desc&p={i}\\\n",
    "# &q=petroleum+size%3A>100&s=stars&type=Repositories' \\\n",
    "# for i in range(1,11)]\n",
    "# urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = []\n",
    "# for url in urls:\n",
    "#     test += get_repo_names(url)\n",
    "\n",
    "# len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lol = pd.Series(test)\n",
    "# test[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.isfile('data.csv'):\n",
    "#     lol.to_csv('data.csv')\n",
    "# data = pd.read_csv('data.csv', header=None, index_col=0,\n",
    "#                   names=['number','repo'])\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = list(data.repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo = 'gocodeup/codeup-setup-script'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repo': 'kinverarity1/lasio',\n",
       "  'language': 'Lasso',\n",
       "  'readme_contents': '# lasio\\n\\n[![License](http://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/kinverarity1/lasio/blob/master/LICENSE)\\n\\nRead and write Log ASCII Standard files with Python.\\n\\nThis is a Python 2.7 and 3.3+ package to read and write Log ASCII Standard\\n(LAS) files, used for borehole data such as geophysical, geological, or\\npetrophysical logs. It\\'s compatible with versions 1.2 and 2.0 of the LAS file\\nspecification, published by the [Canadian Well Logging\\nSociety](https://www.cwls.org/products/#products-las). Support for LAS 3 is [being worked on](https://github.com/kinverarity1/lasio/issues/5). In\\nprinciple it is designed to read as many types of LAS files as possible,\\nincluding ones containing common errors or non-compliant formatting.\\n\\nDepending on your particular application you may also want to check out\\n[striplog](https://github.com/agile-geoscience/striplog) for\\nstratigraphic/lithological data, and\\n[welly](https://github.com/agile-geoscience/welly) for dealing with data at\\nthe well level. lasio is primarily for reading & writing LAS files.\\n\\nNote this is *not* a package for reading LiDAR data (also called \"LAS files\").\\n\\n## Documentation\\n\\nSee here for the [complete lasio package\\ndocumentation](https://lasio.readthedocs.io/en/latest/).\\n\\n## Quick start\\n\\nFor the minimum working requirements, you\\'ll need numpy installed. Install\\nlasio with:\\n\\n```bash\\n$ pip install lasio\\n```\\n\\nTo make sure you have everything, use this to ensure pandas, cchardet, and\\nopenpyxl are also installed:\\n\\n```bash\\n$ pip install lasio[all]\\n```\\n\\nExample session:\\n\\n```python\\n>>> import lasio\\n```\\n\\nYou can read the file using a filename, file-like object, or URL:\\n\\n```python\\n>>> las = lasio.read(\"sample_rev.las\")\\n```\\n\\nData is accessible both directly as numpy arrays\\n\\n```python\\n>>> las.keys()\\n[\\'DEPT\\', \\'DT\\', \\'RHOB\\', \\'NPHI\\', \\'SFLU\\', \\'SFLA\\', \\'ILM\\', \\'ILD\\']\\n>>> las[\\'SFLU\\']\\narray([ 123.45,  123.45,  123.45, ...,  123.45,  123.45,  123.45])\\n>>> las[\\'DEPT\\']\\narray([ 1670.   ,  1669.875,  1669.75 , ...,  1669.75 ,  1670.   ,\\n        1669.875])\\n```\\n\\nand as ``CurveItem`` objects with associated metadata:\\n\\n```python\\n>>> las.curves\\n[CurveItem(mnemonic=DEPT, unit=M, value=, descr=1  DEPTH, original_mnemonic=DEPT, data.shape=(29897,)),\\nCurveItem(mnemonic=DT, unit=US/M, value=, descr=2  SONIC TRANSIT TIME, original_mnemonic=DT, data.shape=(29897,)),\\nCurveItem(mnemonic=RHOB, unit=K/M3, value=, descr=3  BULK DENSITY, original_mnemonic=RHOB, data.shape=(29897,)),\\nCurveItem(mnemonic=NPHI, unit=V/V, value=, descr=4   NEUTRON POROSITY, original_mnemonic=NPHI, data.shape=(29897,)),\\nCurveItem(mnemonic=SFLU, unit=OHMM, value=, descr=5  RXO RESISTIVITY, original_mnemonic=SFLU, data.shape=(29897,)),\\nCurveItem(mnemonic=SFLA, unit=OHMM, value=, descr=6  SHALLOW RESISTIVITY, original_mnemonic=SFLA, data.shape=(29897,)),\\nCurveItem(mnemonic=ILM, unit=OHMM, value=, descr=7  MEDIUM RESISTIVITY, original_mnemonic=ILM, data.shape=(29897,)),\\nCurveItem(mnemonic=ILD, unit=OHMM, value=, descr=8  DEEP RESISTIVITY, original_mnemonic=ILD, data.shape=(29897,))]\\n```\\n\\nHeader information is parsed into simple HeaderItem objects, and stored in a\\ndictionary for each section of the header:\\n\\n```python\\n>>> las.version\\n[HeaderItem(mnemonic=VERS, unit=, value=1.2, descr=CWLS LOG ASCII STANDARD -VERSION 1.2, original_mnemonic=VERS),\\nHeaderItem(mnemonic=WRAP, unit=, value=NO, descr=ONE LINE PER DEPTH STEP, original_mnemonic=WRAP)]\\n>>> las.well\\n[HeaderItem(mnemonic=STRT, unit=M, value=1670.0, descr=, original_mnemonic=STRT),\\nHeaderItem(mnemonic=STOP, unit=M, value=1660.0, descr=, original_mnemonic=STOP),\\nHeaderItem(mnemonic=STEP, unit=M, value=-0.125, descr=, original_mnemonic=STEP),\\nHeaderItem(mnemonic=NULL, unit=, value=-999.25, descr=, original_mnemonic=NULL),\\nHeaderItem(mnemonic=COMP, unit=, value=ANY OIL COMPANY LTD., descr=COMPANY, original_mnemonic=COMP),\\nHeaderItem(mnemonic=WELL, unit=, value=ANY ET AL OIL WELL #12, descr=WELL, original_mnemonic=WELL),\\nHeaderItem(mnemonic=FLD, unit=, value=EDAM, descr=FIELD, original_mnemonic=FLD),\\nHeaderItem(mnemonic=LOC, unit=, value=A9-16-49, descr=LOCATION, original_mnemonic=LOC),\\nHeaderItem(mnemonic=PROV, unit=, value=SASKATCHEWAN, descr=PROVINCE, original_mnemonic=PROV),\\nHeaderItem(mnemonic=SRVC, unit=, value=ANY LOGGING COMPANY LTD., descr=SERVICE COMPANY, original_mnemonic=SRVC),\\nHeaderItem(mnemonic=DATE, unit=, value=25-DEC-1988, descr=LOG DATE, original_mnemonic=DATE),\\nHeaderItem(mnemonic=UWI, unit=, value=100091604920, descr=UNIQUE WELL ID, original_mnemonic=UWI)]\\n>>> las.params\\n[HeaderItem(mnemonic=BHT, unit=DEGC, value=35.5, descr=BOTTOM HOLE TEMPERATURE, original_mnemonic=BHT),\\nHeaderItem(mnemonic=BS, unit=MM, value=200.0, descr=BIT SIZE, original_mnemonic=BS),\\nHeaderItem(mnemonic=FD, unit=K/M3, value=1000.0, descr=FLUID DENSITY, original_mnemonic=FD),\\nHeaderItem(mnemonic=MATR, unit=, value=0.0, descr=NEUTRON MATRIX(0=LIME,1=SAND,2=DOLO), original_mnemonic=MATR),\\nHeaderItem(mnemonic=MDEN, unit=, value=2710.0, descr=LOGGING MATRIX DENSITY, original_mnemonic=MDEN),\\nHeaderItem(mnemonic=RMF, unit=OHMM, value=0.216, descr=MUD FILTRATE RESISTIVITY, original_mnemonic=RMF),\\nHeaderItem(mnemonic=DFD, unit=K/M3, value=1525.0, descr=DRILL FLUID DENSITY, original_mnemonic=DFD)]\\n```\\n\\nThe data is stored as a 2D numpy array:\\n\\n```python\\n>>> las.data\\narray([[ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\\n       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\\n       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\\n       ...,\\n       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\\n       [ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\\n       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ]])\\n```\\n\\nYou can also retrieve and load data as a ``pandas`` DataFrame, build LAS files\\nfrom scratch, write them back to disc, and export to Excel, amongst other\\nthings.\\n\\nSee the [package documentation](https://lasio.readthedocs.io/en/latest/) for\\nmore details.\\n\\n## License\\n\\nMIT\\n'},\n",
       " {'repo': 'agile-geoscience/welly',\n",
       "  'language': 'Lasso',\n",
       "  'readme_contents': \"welly\\n========\\n\\n**Manage subsurface well data.**\\n\\n.. image:: https://img.shields.io/travis/agile-geoscience/welly.svg\\n    :target: https://travis-ci.org/agile-geoscience/welly\\n    :alt: Travis build status\\n    \\n.. image:: https://img.shields.io/pypi/status/welly.svg\\n    :target: https://pypi.python.org/pypi/welly/\\n    :alt: Development status\\n\\n.. image:: https://img.shields.io/pypi/v/welly.svg\\n    :target: https://pypi.python.org/pypi/welly/\\n    :alt: Latest version\\n    \\n.. image:: https://img.shields.io/pypi/pyversions/welly.svg\\n    :target: https://pypi.python.org/pypi/welly/\\n    :alt: Python version\\n\\n.. image:: https://img.shields.io/pypi/l/welly.svg\\n    :target: http://www.apache.org/licenses/LICENSE-2.0\\n    :alt: License\\n\\nWelly is a family of classes to facilitate the loading, processing, and analysis of subsurface wells and well data, such as striplogs, formation tops, well log curves, and synthetic seismograms.\\n\\n\\nPhilosophy\\n==========\\n\\nThe `lasio <https://github.com/kinverarity1/lasio>`_ project provides a very nice way to read and \\nwrite `CWLS <http://www.cwls.org/>`_ Log ASCII Standard files. The result is an object, based on\\n``OrderedDict``, that contains all the LAS data —\\xa0it's more or less analogous to the LAS file.\\n\\nSometimes we want a higher-level object, for example to contain methods that have nothing to do \\nwith LAS files. We may want to handle other well data, such as deviation surveys, tops (aka picks),\\nengineering data, striplogs, synthetics, and so on. This is where ``welly`` comes in.\\n\\n``welly`` uses ``lasio`` for data I/O, but hides much of it from the user. We recommend you look at \\nboth projects before deciding if you need the 'well-level' functionality that ``welly`` provides.\\n\\nLinks\\n==========\\n`Documentation <https://welly.readthedocs.io/en/latest/>`_ \\n\\nContributing\\n============\\n\\nWe welcome contributions! Please fork the project and submit pull requests, or get in touch with us\\nat `hello@agilegeoscience.com <mailto:hello@agilegeoscience.com>`_\\n\"},\n",
       " {'repo': 'akashlevy/Deep-Learn-Oil',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# QRI Project at TRiCAM\\n\\n<img src=\"logos/iacs.png\" alt=\"IACS Logo\" height=\"100\"/><img src=\"logos/qri.png\" alt=\"QRI Logo\" height=\"100\"/>\\n\\nThis repository contains the source files required to reproduce the results in \"Applying Deep Learning to Petroleum Well Data.\" This README will explain how to use these files.\\n\\n## Dependencies\\n\\n- [Python 2.7](https://www.python.org/)\\n- [Numpy/Matplotlib](http://www.scipy.org/)\\n- [Theano](http://deeplearning.net/software/theano/)\\n- [Keras](http://keras.io/)\\n\\n## Usage\\n\\n### Preprocessing\\n\\nIn order to preprocess the data, you will need to go into the folder `datasets/` and run the script `dataset_gen.py`. This script reads in the CSV files from `data/` and converts it into chunks. It does this based on several parameters. `IN_MONTHS`, `OUT_MONTHS` and `STEP_MONTHS`, specify how many months of input, how many months of output and how often to sample for chunks. It also requires two preprocessing parameters, `REMOVE_ZEROS` and `NORMALIZE_DATA`. `REMOVE_ZEROS`, when set to true, will eliminate all zeros from the datasets and push the points together. `NORMALIZE_DATA` will normalize each chunk with respect to the input portion. The random seed `SEED` determines how the data is shuffled. As the data from each well is made into chunks, the chunks are assigned to the training, validation, and testing datasets. The wells are assigned in a train:valid:test = 6:1:1 ratio. Each dataset is represented as a tuple in Python; the first element of the tuple is a NumPy array containing the chunk inputs (the \"x\"), and the second element of the tuple is a NumPy array containing the chunk outputs (the \"y\"). The three datasets are then pickled and stored in a gzipped file called `qri.pkl.gz`. After the dataset is careated, the chunks are plotted using matplotlib.\\n\\n### Testing a Single Model\\nIn the `keras/` folder, there are several scripts with names of different neural network architectures. Each contains the code required to construct a single neural network. Each file consists of a similar structure.\\n\\n#### Structure\\n\\nAfter importing the necessary libraries, a model name is specified through `MDL_NAME`. Next, NumPy\\'s random number generator is seeded with a number to ensure reproducibility of the neural network\\'s results. Then the QRI data is loaded from the gzipped pickle file `qri.pkl.gz` and split into either 2D or 3D datasets. After this comes the architecture specification. The stochastic gradient descent algorithm parameters are then specified; `lr` refers to the learning rate, `momentum` specifies the extent to which past gradient values should be incorporated into the optimization, `decay` specifies the rate at which the learning rate decreases, and `nesterov` specifies whether or not Nesterov\\'s formula should be used to compute the gradient. After the optimization technique is specified, the model is compiled with Theano using a particular loss function.\\n\\nNext, the early stopping parameters are specified. The validation loss is monitored and `patience` specifies how long the neural network should wait to observe a new best validation loss. The best model is saved to the subfolder `models/<MDL_NAME>.mdl`. These features are incorporated using a callback mechanism during training.\\n\\nThe model is then trained. The lines\\n```python\\nt0 = time.time()\\n```\\nand\\n```python \\ntime_elapsed = time.time() - t0\\n```\\nare used to determine how long training took. There are three parameters to the training function `model.fit`; the first is `verbose` that specifies how often data should be printed to the console. The second is `nb_epoch` that specifies the maximum number of training steps. The last is `batch_size` that specifies the number of chunks that should be trained on at once.\\n\\nAfter the model is done training, the best model is loaded from the MDL file. Then the model is evaluated on the testing set and the training time and testing set error are displayed. The results and the training/validation error are saved to `results/<MDL_NAME>.out` and `models/<MDL_NAME>.hist` respectively. Then the training and validation error are plotted as well as the test predictions.\\n\\n#### Model Specifics\\n\\nEvery model begins with \\n```python\\nmodel = Sequential()\\n```\\nwhich denotes that the neural network consists of a series of stacked layers. There are many different kinds of layers:\\n- **Dense**: a regular fully-connected layer; specify number of inputs, number of outputs, and activation function\\n- **Convolution1D**: a convolutional layer; specify *stack size* (how many filters you used in the previous layer, 1 if first layer), number of kernels per filter, and activation function\\n- **SimpleRNN, GRU, LSTM, MUT123**: different kinds of recurrent layers; specify number of inputs, number of outputs, and activation function\\n- **SimpleDeepRNN**: a multi-layer recurrent network; specify number of inputs, number of outputs, number of layers, and activation function\\n- **Dropout**: used to make a network more sparse; specify the fraction of inputs to randomly set to 0\\n- **Flatten**: convert a multi-dimensional input into a 1D input.\\n\\nUsing these Keras layers, we can construct custom neural networks to perform time series prediction on oil wells.\\n\\n#### Custom Neural Network Tools (found in `qri.py`)\\n\\n- `load_data`: loads the data from `qri.pkl.gz`\\n- `plot_test_predictions`: plots each chunk from the test set along with the prediction made for that set\\n- `plot_train_valid_loss`: plots how the training and validation error decreased in training\\n- `print_output_graph`: prints the computational graph for producing predictions to filename in a specified image format; useful for debugging and seeing how the network actually works\\n- `plot_weights`: plots the weight matrix for each layer in the neural network; useful for understanding what the neural network is learning\\n- `mae_clip`: provides a Theano expression for the mean absolute error with clipping to provide resistance to outliers; the `CLIP_VALUE` can be changed to adjust the number of standard deviations at which to begin clipping\\n- `save_results`: pickles the results and saves them to a file\\n- `save_history`: saves the training and validation loss history to a file\\n\\n### Hyperparameter Optimization using Grid Search\\n\\nWe used variants of the scripts provided in `cluster` to run our models on Harvard\\'s Odyssey computing cluster. They can be modified to work on different kinds of clusters.\\n\\n### Bayesian Hyperparameter Optimization\\nFor more information, see [Spearmint](https://github.com/JasperSnoek/spearmint).\\n\\n## Cited By\\n- Stamp, Alexander. \"The relationship between weather forecasts and observations for predicting electricity output from wind turbines.\" (2017).\\n\\n## Contact\\nPlease contact <akashlevy@gmail.com>, <janette_garcia08@hotmail.com>, <albert.tung0902@my.riohondo.edu> or <michelleyang@berkeley.edu> with any questions about this repository. Thank you!\\n'},\n",
       " {'repo': 'Flaxbeard/ImmersivePetroleum',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': \"![Contribution guidelines for this project](https://i.imgur.com/GVGvRcD.png)\\n==============\\n\\n**Immersive Petroleum** is a Minecraft mod, an addon for Immersive Engineering for 1.10.2-1.12.2. It adds oil, oil extraction, and oil processing to Immersive Engineering's tech progression.\\n\\nFor more information and downloads, see the [CurseForge page](https://minecraft.curseforge.com/projects/immersive-petroleum).\\n\\nIf you are a modpack developer, see the [Configuration Guide for Pack Developers](https://github.com/Flaxbeard/ImmersivePetroleum/wiki/Tips-for-Modpack-Developers).\\n\"},\n",
       " {'repo': 'mutolisp/kh_pplines',\n",
       "  'language': None,\n",
       "  'readme_contents': '高雄市石化管路\\n==============\\n\\nPetroleum and gas pipelines of Kaohsiung\\n\\n註：管線圖層和高雄81氣爆事件並不一定有關連，僅供參考。\\n\\n\\nsrc/ 中為已定位過的管線圖影像檔（潘建志醫師提供的）\\n\\nlayers/ 資料夾中為數化過的 GIS 圖層\\n\\n提供的檔案格式為 ESRI Shapefile 及 GeoJSON (屬性編碼皆為 UTF-8) \\n\\n* kh_petroleum_pipelines_[twd97|wgs84]  為 TWD1997 TM2/WGS84 經緯度的石化輸油管線圖層\\n* kh_ng_pipelines_[twd97|wgs84]  為 TWD1997 TM2/WGS84 經緯度的欣高公司天然氣管線圖層\\n* cpc/ 中油輸油管線圖層\\n\\n![pipelines of explosion area](https://raw.github.com/mutolisp/kh_pplines/master/kh_pplines.png)\\n'},\n",
       " {'repo': 'mycarta/Data-science-tools-petroleum-exploration-and-production',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': '## Data science tools for petroleum exploration and production\\n\\nThis repo is a Python and R extention of the work from two Python notebooks in the [Predict repository](https://github.com/mycarta/predict) (_some of the code in those notebooks may be obsolete, but they are still a good read_): \\n\\n- [Geoscience ML notebook 2](https://github.com/mycarta/predict/blob/master/Geoscience_ML_notebook_2.ipynb) \\n\\n- [Geoscience_ML_notebook 3](https://github.com/mycarta/predict/blob/master/Geoscience_ML_notebook_3.ipynb)\\n\\n**We illustrate the use of Data Science tools, specifically focusing on exploratory data analysis\\nand predictive modeling, using a small toy data set with oil production data.**\\n\\n## Talk at the 2018 Calgary CSEG / CSPG Geoconvention\\n**Data science tools for petroleum exploration and production**\\n* For pre-talk abstract see [here](https://www.geoconvention.com/uploads/2018abstracts/290_GC2018_Data_science_tools_for_petroleum_e_and_p.pdf)\\n* For slides see `/Talk slides/` above\\n* Annotated slides coming soon on Speaker Deck\\n\\n## Contributors\\n\\n### For Python\\n[Matteo Niccoli](https://github.com/mycarta),  see `/Python/` above.\\n\\nBlog posts:\\n* [Visual data exploration in Python: correlation, confidence, spuriousness](https://mycarta.wordpress.com/2019/03/17/visual-data-exploration-in-python-correlation-confidence-spuriousness/)\\n* [Data exploration in Python: distance correlation and variable clustering](https://mycarta.wordpress.com/2019/04/10/data-exploration-in-python-distance-correlation-and-variable-clustering/)\\n* [Variable selection in Python, Part I](https://mycarta.wordpress.com/2019/04/30/variable-selection-in-python-part-i/)\\n* Understanding and using confidence interval for the correlation coefficient\\n* Variable selection in Python, Part II\\n* Regression model in Python\\n\\n\\n#### Interactive notebooks:\\n* click on the `Binder button` below, browse to the `/Python/notebooks` directory, and open one of the notebooks\\n\\n[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/mycarta/Data-science-tools-petroleum-exploration-and-production/master)\\n\\n#### Static notebooks:\\n* [Data loading, visualization, significance testing](https://github.com/mycarta/Data-science-tools-petroleum-exploration-and-production/blob/master/Python/notebooks/Python_data_science_tools%20_petroleum_exploration_production.ipynb)\\n* __NEW__: [Confidence interval for the correlation coefficient - (interactive)](https://github.com/mycarta/Data-science-tools-petroleum-exploration-and-production/blob/master/Python/notebooks/Python_confidence_interval_interact_matplotlib.ipynb)\\n* __NEW__: [OLS reggression confidence interval and prediction interval](https://github.com/mycarta/Data-science-tools-petroleum-exploration-and-production/blob/master/Python/notebooks/Python_OLS_confidence_interval_and_prediction_interval.ipynb)\\n* [Distance correlation and clustering](https://github.com/mycarta/Data-science-tools-petroleum-exploration-and-production/blob/master/Python/notebooks/Python_data_science_tools%20_petroleum_exploration_production_distance_correlation_and_clustering.ipynb)\\n* [Variable selection with distance correlation](https://github.com/mycarta/Data-science-tools-petroleum-exploration-and-production/blob/master/Python/notebooks/variable_selection_01_DCOR.ipynb)\\n* [Variable selection with Variance Inflation Factor](https://github.com/mycarta/Data-science-tools-petroleum-exploration-and-production/blob/master/Python/notebooks/variable_selection_02_VIF.ipynb)\\n* [Variable selection with Sequential Feature Selection](https://github.com/mycarta/Data-science-tools-petroleum-exploration-and-production/blob/master/Python/notebooks/variable_selection_03a_Exhaustive_Feature_Selector.ipynb)\\n* [Variable selection with Sequential Feature Selection](https://github.com/mycarta/Data-science-tools-petroleum-exploration-and-production/blob/master/Python/notebooks/variable_selection_03b_Sequential_Feature_Selector.ipynb)\\n* [Variable selection with Random Forest drop-column variable importance AND variable dependance](https://github.com/mycarta/Data-science-tools-petroleum-exploration-and-production/blob/master/Python/notebooks/variable_selection_04_RF_drop-column_importances_and_dependence.ipynb)\\n* [Variable selection with Linear regression and permutation importance](https://github.com/mycarta/Data-science-tools-petroleum-exploration-and-production/blob/master/Python/notebooks/variable_selection_05_LR_permutation_importance.ipynb)\\n* [Variable selectrions with SHAP values and linear regression](https://github.com/mycarta/Data-science-tools-petroleum-exploration-and-production/blob/master/Python/notebooks/variable_selection_07_LR_SHAP_values.ipynb)\\n* [Variable selection with conditional statistics](https://github.com/mycarta/Data-science-tools-petroleum-exploration-and-production/blob/master/Python/notebooks/variable_selection_08_conditional_statistics.ipynb)\\n\\n\\n\\n\\n### For R\\n[Thomas Speidel](https:/github.com/tspeidel/), see `/R/` above.\\n* For an interactive notebook, see [here](http://speidel.duet.to/Pages/geoconvention-2018/geoconvention-2018.html)\\n* For the Rmarkdown see [here](https://github.com/mycarta/Niccoli_Speidel_2018_Geoconvention/blob/master/R/geoconference_2018.Rmd)\\n\\n\\n\\n'},\n",
       " {'repo': 'GeostatisticsLessons/GeostatisticsLessonsNotebooks',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': '# Geostatistics Lessons Notebooks\\n\\n[Geostatistics Lessons](http://geostatisticslessons.com/) is an open disclosure of some guidance in geostatistical modeling reviewed by an editorial board. These python notebooks and data are prepared by Lesson authors to supplement their Lesson. As new Lessons are authored and notebooks created, this repository will be updated. \\n\\n## Lessons\\n\\nLessons with notebooks and data available include:\\n\\n* An Application of Bayes Theorem to Geostatistical Mapping ([notebook](notebooks/bayesmapping/bayesmapping.ipynb) and [lesson](http://geostatisticslessons.com/lessons/bayesmapping)), Jared Deutsch and Clayton Deutsch, 2018\\n* Multidimensional Scaling ([notebook](notebooks/mds/mds.ipynb) and [lesson](http://geostatisticslessons.com/lessons/mds)), Steven Mancell and Clayton Deutsch, 2019\\n\\n## Dependencies\\n\\nNotebooks are implemented in Python 3 using the scientific python stack (including NumPy, Pandas, Matplotlib). Refer to the individual notebooks for any particular dependencies. \\n\\n## License\\n\\nNotebooks are licensed under the [MIT](LICENSE) license separately from Lessons. Refer to [Geostatistics Lessons](http://geostatisticslessons.com/) for licensing information on the Lessons.\\n'},\n",
       " {'repo': 'f0nzie/rNodal',\n",
       "  'language': 'R',\n",
       "  'readme_contents': '---\\noutput: github_document\\n---\\n\\n<!-- README.md is generated from README.Rmd. Please edit that file -->\\n\\n```{r setup, include = FALSE}\\nknitr::opts_chunk$set(\\n  collapse = TRUE,\\n  comment = \"#>\",\\n  fig.path = \"man/figures/README-\",\\n  out.width = \"100%\"\\n)\\n```\\n# rNodal\\n\\n\\nThe goal of rNodal is to provide nodal analysis for oil and gas wells.\\n\\n## Installation\\n\\nYou can install rNodal from github with:\\n\\n```{r gh-installation, eval = FALSE}\\n# install.packages(\"devtools\")\\ndevtools::install_github(\"f0nzie/rNodal\")\\n```\\n\\n## Example\\n\\nThis is a basic example which shows you how to solve a common problem:\\n\\n```{r example}\\n## basic example code\\n```\\n\\n\\n## Using `zFactor`\\n`zFactor` is a R package. Calling from CRAN.\\n\\nWe will use zFactor for gas compressibility calculations.\\n\\n```{r}\\n# use the new library zFactor\\nlibrary(zFactor)\\n\\nz.HallYarborough(pres.pr = 4.5, temp.pr = 1.4)\\nz.DranchukAbuKassem(pres.pr = 4.5, temp.pr = 1.4)\\nz.BeggsBrill(pres.pr = 4.5, temp.pr = 1.4)\\nz.Ann10(pres.pr = 4.5, temp.pr = 1.4)\\nz.Papp(pres.pr = 4.5, temp.pr = 1.4)\\n```\\n\\n\\n## How rNodal works\\nStart by looking at the examples in the vignettes. We will use in this example `VLP Brown - Example C13`.\\n\\nThis is example C.13 in the Kermit Brown book.\\n\\n### Input well data\\nWe enter the well data with the function `setWellInput`:\\n\\n    input.example.C13 <- setWellInput(field.name = \"HAGBR.MOD\",\\n                                        well.name = \"Brown_C13\", \\n                                        depth.wh = 0, depth.bh = 2670, \\n                                        diam.in = 1.995, \\n                                        GLR = 500, liq.rt = 1000, wcut = 0.6, \\n                                        thp = 500, tht = 120, bht = 150, \\n                                        API = 22, gas.sg = 0.65, \\n                                        wat.sg = 1.07, if.tens = 30)\\n                                        \\n                                        \\n\\n\\nThe field name and well name are used for archival purposes.\\n\\n### Enter the parameters of the VLP model\\nThe parameters of the model consist of:\\n\\n`vlp.model`: the correlation or mechanistical model \\n\\n`segments`: the number of segments to split the well\\n\\n`tol`: the tolerance of the delta-P iterations\\n\\n\\n    well.model <- setVLPmodel(vlp.model = \"hagbr.mod\", \\n                               segments = 11, \\n                                    tol = 0.000001)\\n                                    \\n                                    \\n                                    \\n### Run the model\\nTo run the model is necessary to provide:\\n\\n`well.input`: all the well parameters as entered in the first step\\n\\n`well.model`: the VLP model as entered in the second step\\n\\n\\n    runVLP(well.input = input.example.C13, well.model))\\n    \\n\\n### Results\\nThe results are given in the form of a dataframe where the rows represent the number of segment plus one and the columns are the calculations or variables.\\n\\n```{r, out.width = \"800px\", echo=F}\\nknitr::include_graphics(\"man/figures/README-results_df.jpg\")\\n```\\n'},\n",
       " {'repo': 'ICHEC/ExSeisDat',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# ExSeisDat\\n\\nExSeisDat is a High Performance I/O library for seismic data files, primarily\\nthe SEG-Y format.\\n\\nIt includes a low-level parallel file I/O library, ExSeisPIOL, primarily\\ntargeting MPI I/O, containing routines for selectively loading and processing\\nmeta-data and trace data.\\n\\nAlso included is a high-level library, ExSeisFlow, which uses ExSeisPIOL to\\nimplement a number of seismic data workflows, e.g. sorting data, binning, trace\\nanalysis, file concatenation, etc.\\n\\n\\n### Talks / Posters\\n\\n[EAGE Upstream 2017 Slides](doc/dissemination/EAGE_Upstream_2017.pdf)  \\n[Rice HPC Oil & Gas 2017 Slides](doc/dissemination/Rice_HPC_O&G_2017.pdf)  \\n[Rice HPC Oil & Gas 2018 Poster](doc/dissemination/Rice_HPC_O&G_2018_Poster.pdf)\\n\\n\\n## Building Instructions\\n\\nTo configure and build ExSeisDat, a C++14 compatible compiler is needed, and\\nCMake >= 3.5.2.\\n\\n    # Make a directory to hold the temporary build files\\n    mkdir -p /path/to/build/directory\\n\\n    # Change directory to the build directory\\n    cd /path/to/build/directory\\n\\n    # Configure the ExSeisDat build using CMake\\n    cmake \\\\\\n        -DCMAKE_INSTALL_PREFIX=/path/to/install/directory \\\\\\n        /path/to/the/exseisdat/project\\n\\n    # Build the ExSeisDat project\\n    make\\n\\n    # Test the ExSeisDat project\\n    make test\\n\\n    # Install the ExSeisDat project\\n    make install\\n\\n    # (Optional) clean up all the temporary build files\\n    # cd $HOME\\n    # rm -rf /path/to/build/directory\\n\\nCMake should find the necessary MPI and FFTW libraries automatically. It also searches for the C and C++ compilers in the CC and CXX environmental variables.\\n\\n\\n### Specifying MPI\\n\\nIf CMake can\\'t find MPI, or finds the wrong version of MPI, pass the options\\n`-DMPI_C_COMPILER=/path/to/mpicc` and `-DMPI_CXX_COMPILER=/path/to/mpicxx` to\\nthe `cmake` command, e.g.\\n\\n    cmake /path/to/the/exseisdat/project \\\\\\n        -DCMAKE_INSTALL_PREFIX=/path/to/install/directory \\\\\\n        -DMPI_C_COMPILER=/path/to/mpicc \\\\\\n        -DMPI_CXX_COMPILER=/path/to/mpicxx\\n\\nwhich should automatically reconfigure and recompile only the minimum necessary\\nfiles.\\n\\nTo specify the MPI compiler *and* the C and C++ compilers, use the\\n`-DCMAKE_C_COMPILER` and `-DCMAKE_CXX_COMPILER` options along with\\n`-DMPI_C_COMPILER` and `-DMPI_CXX_COMPILER`, e.g.\\n\\n    cmake /path/to/the/exseisdat/project \\\\\\n        -DCMAKE_INSTALL_PREFIX=/path/to/install/directory \\\\\\n        -DMPI_C_COMPILER=/path/to/mpicc \\\\\\n        -DMPI_CXX_COMPILER=/path/to/mpicxx \\\\\\n        -DCMAKE_C_COMPILER=/path/to/cc \\\\\\n        -DCMAKE_CXX_COMPILER=/path/to/c++\\n\\nAlternatively, set `export CC=/path/to/mpicc` and `export CXX=/path/to/mpicxx`\\nin your environment (optionally, also the appropriate MPI compiler variables,\\ne.g. `I_MPI_CC` and `I_MPI_CXX`, to find the correct `C` and `CXX` compilers)\\nbefore running the `cmake` command to find the correct MPI libraries.\\nIf CMake doesn\\'t find them after setting `CC` and `CXX`, you may need to delete\\nthe build directory and reconfigure.\\n\\n\\n### Specifying FFTW\\n\\nIf CMake can\\'t find FFTW, or finds the wrong version, try passing\\n`-DFFTW3_DIR=/path/to/fftw` to the `cmake` command.\\nThe directory `/path/to/fftw` should be the directory including\\n`include/fftw3.h`, for a full path of `/path/to/fftw/include/fftw3.h`.\\nCMake will also look for `/path/to/fftw/local/include/fftw3.h`.\\nThe FFTW library should be found at `/path/to/fftw/lib/libfftw3f.so`.\\nCMake will also look in the `lib64` and `bin` directories, and the `local/lib`\\ndirectory (also `lib64` and `bin`) if fftw3.h was found in `local/include`.\\n\\nIf the Intel compiler is used, CMake will use the `-mkl` flag to link the Intel\\nMKL library instead of FFTW.\\n\\n\\n### Build Flags\\n\\nCMake has a number of pre-configured build modes: mainly Release and Debug.\\n\\nTo build ExSeisDat in its release configuration\\n(`-O3`, no asserts, no debug symbols)\\npass `-DCMAKE_BUILD_TYPE=Release` to the `cmake` command.\\nThis is the default if this option isn\\'t passed.\\n\\nTo build ExSeisDat in its debug configuration\\n(asserts enabled, debug symbols added)\\npass `-DCMAKE_BUILD_TYPE=Debug` to the `cmake` command.\\n\\nTo add extra flags to the build, use the `-DCMAKE_C_FLAGS` and\\n`-DCMAKE_CXX_FLAGS` options.\\nFor example, to include `-O1` and address sanitization for debug builds, pass\\n\\n    cmake \\\\\\n        -DCMAKE_CXX_FLAGS=\"-O1 -fsanitize=address\" \\\\\\n        /path/to/the/exseisdat/project\\n\\n\\n### A Suggestion\\n\\nOne easy way of developing ExSeisDat with CMake is to put the configuration /\\nbuild commands in a script file, have the script file in the project directory,\\nand have the build directory as a subdirectory in the project directory.\\n\\nIf we call it e.g. make.sh, we can write the following:\\n\\n    #!/usr/bin/env bash\\n    # Contents of make.sh:\\n\\n    # Exit script on first non-zero return\\n    set -o errexit\\n\\n    # Get the directory containing this script!\\n    script_dir=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\"\\n\\n    source_dir=${script_dir}\\n    build_dir=${script_dir}/build/build\\n    install_dir=${script_dir}/build/install\\n\\n    mkdir -p ${build_dir}\\n    mkdir -p ${install_dir}\\n\\n    # Load whatever modules are necessary\\n    # e.g. using the Intel compiler on Fionn:\\n    # module load dev\\n    # module load cmake/intel/3.5.2\\n    # module load intel/latest\\n    # module load openmpi/intel/1.8.3\\n\\n    # Use gcc and g++ instead of icc and icpc for GCC!\\n    cd ${build_dir}\\n    cmake ${source_dir} \\\\\\n        -DCMAKE_INSTALL_PREFIX=${install_dir} \\\\\\n        -DCMAKE_BUILD_TYPE=Debug \\\\\\n        -DCMAKE_C_COMPILER=icc \\\\\\n        -DCMAKE_CXX_COMPILER=icpc \\\\\\n        -DCMAKE_C_FLAGS=\"-O1 -fsanitize=address\" \\\\\\n        -DCMAKE_CXX_FLAGS=\"-O1 -fsanitize=address\"\\n    cd ${script_dir}\\n\\n    make -C ${build_dir}\\n    make -C ${build_dir} test\\n    make -C ${build_dir} install\\n\\nNow, running `./make.sh` should configure, build, test, install all the\\nnecessary files and components automatically.\\nWhen any files are changed in the project, running this command should\\nreconfigure and recompile only the minimum necessary files, and run the test\\nsuite over the whole project.\\nReconfiguration should be reasonably cheap, unless the CMakeLists.txt files have\\nbeen altered.\\n\\nIn this manner, flags and configuration options can be quickly changed, and\\neasily tracked.\\nTesting can also be disabled by simply commenting out the\\n`make -C ${build_dir} test` command.\\n\\nNote: It can be easy to forget the trailing `\\\\` for line continuation in the\\n`cmake` command!\\nIf you get error messages like `Cannot find command -DCMAKE_...`, a missing `\\\\`\\nis the likely culprit!\\n\\n\\n#### Modulefiles\\n\\nPutting module commands here also makes the compilation environment clear.\\n\\nSome care may be taken, as some environments aren\\'t configured for using\\n`module` commands in a script by default.\\nUsers may need to use `source /path/to/modules/init` to make `module` available\\nin the script.\\nOn Fionn, for example, use `source /usr/share/modules/init/bash`.\\n\\n\\n### Useful Options\\n\\n#### Setting Compile / Link Flags\\nOption | Effect\\n------ | ------\\n`-DCMAKE_C_COMPILER=...`   | Set the C compiler.\\n`-DCMAKE_CXX_COMPILER=...` | Set the C++ compiler.\\n`-DCMAKE_BUILD_TYPE=...`   | Set to `Release` or `Debug` for release or debug builds. Sets a number of flags by default.\\n`-DCMAKE_C_FLAGS=...`      | Set the flags to pass to the C compiler. Overrides the default flags.\\n`-DCMAKE_CXX_FLAGS=...`    | Set the flags to pass to the C++ compiler. Overrides the default flags.\\n`-DBUILD_SHARED_LIBS=...`  | Set to `ON` to build shared libraries, of `OFF` for static libraries.\\n`-DCMAKE_SHARED_LINKER_FLAGS=...` | Set the flags to pass to the shared library linker.\\n`-DCMAKE_STATIC_LINKER_FLAGS=...` | Set the flags to pass to the static library linker.\\n`-DCMAKE_EXE_LINKER_FLAGS=...`    | Set the flags to pass to the executable linker.\\n`-DCMAKE_VERBOSE_MAKEFILE=ON`     | Output the build commands as they\\'re run.\\n\\n#### Dependencies\\nOption | Effect\\n------ | ------\\n`-DMPI_C_COMPILER=...`   | Set the MPI C compiler wrapper (i.e. mpicc).\\n`-DMPI_CXX_COMPILER=...` | Set the MPI C++ compiler wrapper (i.e. mpicxx).\\n`-DFFTW3_DIR=...`        | Set the root directory for FFTW, where CMake should look for `fftw3.h` and `libfftw3f.so`.\\n`-DFFTW3_INCLUDES=...`   | Explicitly set the `fftw3.h` header file to use.\\n`-DFFTW3_LIBRARIES=...`  | Explicitly set the `libfftw3f.so` library file or flag (e.g. `-lfftw3f` or `-mkl`) to use.\\n\\n#### Enabling / Disabling Sections of ExSeisDat\\nOption | Effect\\n------ | ------\\n`-DEXSEISDAT_BUILD_UTILITIES=...` | Set to `ON` to build ExSeisDat utilities or `OFF` to skip (Default `ON`).\\n`-DEXSEISDAT_BUILD_EXAMPLES=...`  | Set to `ON` to build ExSeisDat example programs or `OFF` to skip (Default `ON`).\\n`-DEXSEISDAT_BUILD_TESTS=...`     | Set to `ON` to build ExSeisDat tests and enable the `make test` target, or `OFF` to skip (Default `ON`).\\n\\n#### Installation Directories\\nOption | Effect\\n------ | ------\\n`-DCMAKE_INSTALL_PREFIX=...` | Set the root install directory for the compiled libraries and programs.\\n`-DCMAKE_INSTALL_BINDIR=...` | Set the install directory for ExSeisDat executables. Use a relative path to set the path relative to `${CMAKE_INSTALL_PREFIX}`. (Default `bin`)\\n`-DCMAKE_INSTALL_LIBDIR=...` | Set the install directory for ExSeisDat libraries. Use a relative path to set the path relative to `${CMAKE_INSTALL_PREFIX}`. (Default `lib`)\\n'},\n",
       " {'repo': 'unifloc/unifloc_vba',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': '# UNIFLOC VBA #\\n\\nАнализ работы скважины и скважинного оборудования \\nс использованием VBA макросов Excel.\\n\\n### Описание ###\\n\\n* Инженерные расчеты по добыче Унифлок VBA\\n* Версия 7.24\\n\\nРасчетные модули разработаны для обучения студентов и специалистов методам проведения инженерных расчетов добывающих скважин. Рекомендуется применение для учебных целей. Авторы стараются избегать ошибок в расчетных модулях, но не гарантируют их отсутствия. \\n\\n\\nВозможности на текущий момент\\n- Расчет физико-химических свойств пластовых флюидов по двум наборам корреляций\\n- Расчет многофазного потока в трубах и скважине по нескольких корреляциям (Беггс Брилл, Ансари, Унифицированная TUFFP и некоторые другие)\\n- Расчет многофазного потока в штуцере. Корреляция Перкинса.\\n- Расчет характеристик УЭЦН и база характеристик УЭЦН для некоторых типов насосов\\n- Расчет распределения давления в фонтанирующей скважине\\n- Расчет распределения давления в скважине с УЭЦН\\n- Расчет распределения давления в скважине с газлифтом\\n\\nИзменения в версии 7.24\\n- добавлены функции для расчета гидратов (от Горидько К)\\n- исправления в пользовательских функциях расчета ЭЦН. Теперь выводится больше информации о расчете. Работает калибровка по одному параметру. Поправлена модель задания поправки на газ.\\n\\nИзменения в версии 7.23\\n- проверена и восстановлена работа python API\\n\\nИзменения в версии 7.22\\n- Исправление ошибок и рефакторинг кода PVT и расчета корреляций\\n- Приведение в порядок упражнений \\n\\nИзменения в версии 7.21\\n- Появилась мелкое обновление с номером версии 7.21.1. Исправлена ошибка в функции MF_p_pipe_atma - теперь она правильно понимает отрицательные углы.\\n- В попытке исправить баги и сделать лучше изменены аргументы функций расчета трубы. Подробности будут в сообщениях на unifloc.ru\\n- Добавлен ряд пользовательских PVT функций. В частности для расчета теплоемкостей и сжимаемостей флюидов.\\n- Добавлен пример расчета z фактора \\n- Обновлены примеры расчета трубы \\n- Обновлен мануал в части описания идеологии расчета трубы и трубопровода по новому\\n\\nИзменения в версии 7.20\\n- исправлена ошибка при загрузке базы ЭЦН. При скачивании с гитхаба в базе насосов заменялся символ переноса каретки, из за чего считывание проходило некорректно. Исправлено и немного оптимизировано\\n- добавлены варианты расчета калибровок для модели трубы - подстройка по дебиту жидкости и свободного газа\\n- добавлены варианты расчета калибровок для модели штуцера аналогично трубе \\n\\nИзменения в версии 7.19\\n- сделан скрип автоматизированной проверки заданий/упражнений для дистанционных курсов. Если скачать сразу много заданий можно проверить их корректность по ключевым файлам.\\n- подготовлен ряд заданий для проверки с использованием скрипта\\n- исправлены мелких ошибки (инициализация перемернных, отладка сообщений в логе)\\n\\nИзменения в версии 7.18\\n- исправлен и доработан расчет ЭЦН. обновлен пример расчета ЭЦН\\n- новый вариант работы с базой ЭЦН - база читается из текстового файла\\n\\nИзменения в версии 7.17\\n- изменения в наборе примеров и упражнений\\n- исправлены некоторые мелкие ошибки\\n\\nИзменения в версии 7.16\\n- Продолжение доработки документации - добавлены некоторые картинки по трубопроводам, одновлена диаграмма классов\\n- Вызов расчета трубы и трубопровода унифицирован - и там и там вывод одинаковый\\n- Исправлены мелкие ошибки при расчете PVT - при некоторых условиях неправильно инициалировались газовый фактор и газосодержание\\n- Проведен рефакторинг класса CESPpump - убран лишний код (строк меньше стало в два раза). Изменена схема расчета характеристики ЭЦН. Теперь она всегда интерполируется по заданным в базе точкам. При этом в коде больше не используются функции Application (должно позволить проще портировать код). \\n- Для ЭЦН добавлена фича, что при дебите больше максимального напор не обнуляется, а становится отрицательным. Для контроля введены два контрольных параметра turb_head_factor, turb_rate_factor\\n- База насосов вынесена из надстройки в отдельный файл ESP_db.xlsx - для корректной работы файл должен быть рядом с надстройкой\\n- Исправлен расчет скважины по кнопке в рабочей книге в папке apps \\n- Вроде все примеры работают но версия промежуточная - будут еще доработки в ближайшее время.\\n\\n\\nИзменения в версии 7.15\\n- Документация актуализирована с текущим состоянием кода \\n- Исправлена ошибка при расчете функций скважины \\n- Мелкие исправления ошибок и неточностей в наименовании аргументов. Должно стать немного логичнее и понятнее\\n\\nИзменения в версии 7.14\\n- Доработка документации - описание разделено на несколько документов\\n- Исправление ошибок\\n\\nИзменения в версии 7.13\\n- Доработка и исправлении версии 7.12 \\n- Изменения для повышения удобства использования расчетных функций при написании макросов (не из интерфейса Excel)\\n\\nИзменения в версии 7.12\\n- Изменены интерфейсы для пользовательских функций расчета скважины с префиксом well_\\n- Добавлен расчет трубопровода - отличается от трубы возможностью задания траектории и изменения диаметров по длине\\n- Возможность учета инклинометрии в скважине \\n- Расчет трубопровода и скважины возвращает в Excel расширенный набор информации о расчете\\n- Добавлены упражнения для новых функций\\n- версия 7.12 промежуточная - после больших обновлении в расчете трубы и скважины возможны баги. Следующие версии будут направлены на стабилизацию кода.\\n\\nИзменения в версии 7.11\\n- Добавлен автоматически генерируемый программный интерфейс API для доступа к пользовательским функциям unifloc VBA через python\\n- Добавлены функции расчета неустановившегося режима работы скважины после запуска с постоянным дебитом\\n- Описание разделено на две части - руководство пользователя и сборник упражнений\\n\\nИзменения в версии 7.10\\n- При запуске надстройки создается закладка с кнопками на панели управления Excel. Там есть кнопки для проверки версии и исправления ссылок на надстройку при переносе ее между компами.\\n- Мелкие исправления в названиях функций\\n\\nИзменения в версии 7.9\\n- Добавлены функции для расчета газлифтных клапанов\\n- Добавлен режим расчета распределения давления газа в трубе (без трения)\\n- Добавлены функции расчета параметров газлифтной скважины (расчет снизу вверх и сверху вниз)\\n\\nИзменения в версии 7.8.\\n- Добавлены функции для работы с кривыми в интерфейсе Excel (префикс \"crv_\")\\n\\nИзменения в версии 7.7.\\n- Упорядочены упражнения по использованию расчетных функций\\n- Изменены названия ряда пользовательских функций для удобства использования\\n- Функции для расчета скважин (фонтан, ЭЦН, газлифт)\\n- Расчет асинхронного двигателя ПЭД\\n- Оценка коэффициента сепарации газосепаратора \\n- Исправление мелких ошибок\\n- Доработано описание - добавлен раздел с автоматически генерируемыми описаниями функций. Компиляция документации в LuaTex\\n\\nИзменения в версии 7.6\\n- Добавлен учет плотности газа в затрубном пространстве при расчете скважины с УЭЦН\\n- Обновлена схема работы со скважинами и расчета узлового анализа. Введен интерфейсный класс IWell скважины обеспечивающий одинаковое поведение всех типов скважин и типовые расчеты узлового анализа\\n- Сделан единий механизм работы с расчетными кривыми для всех базовых классов. Механизм реализуется классом CCurves\\n- Переработаны методы инициализации скважин для устранения неоднозначностей. Отдельно задается конструкция, отдельно задается распределение температуры и метод расчета температуры скважины\\n- В связи с переработками поменялись названия ряда методов классов скважины, поэтому изменен номер версии.\\n\\nна текущий момент версия 7.6 еще в доработке. \\n\\nИзменения в версии 7.5\\n- Обновлен класс CWell обеспечивающий расчет по скважине с УЭЦН. \\n- Добавлен класс описывающий упрощенную систему УЭЦН CESPSystemSimple. Класс обеспечивает расчет сепарации и электрических параметров по работе УЭЦН. Встраивается в CWell и обеспечивает учет работы УЭЦН в скважине. Текущие ограничения - один тип насоса (нет конусной сборки, единая кабельная линия, сопротивление кабеля не меняется по глубине с температурой, двигатель с постоянным КПД и постоянными параметрами электрическими)\\n- Вернулся расчет барботажа (потока газа в неподвижной жидкости) в блок многофазных расчетов и блок скважины. Сейчас барботаж считается по корреляции Ансари при очень маленьком дебите жидкости\\n- Добавлены интерфейсные функции Excel для расчета скважины.\\n- Изменена схема именования надстройки - теперь содержит только главный номер версии. Для того чтобы проще было обновлять расчетные модули на компе в ходе разработки. \\n\\n\\nИзменения в версии 7.3\\n- Дополнена модель ПЭД\\n- Ускорены расчеты по скважине\\n- Изменился вызов некоторых функций Excel \\n- Систематизирован расчет скважины \\n\\n\\n### Контакты ###\\n\\n* Хабибуллин Ринат\\n* khabibullin.ra@gubkin.ru\\n\\n'},\n",
       " {'repo': 'Petroware/NpdIo',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# NPD I/O - NPD access library\\n\\nThe [Norwegian Petroleum Directorate (NPD)](http://npd.no)\\nmaintains a comprehensive database of the activities in the North Sea.\\nThis is public real-time information updated on a daily basis and\\nmade public through the [NPD fact pages](http://factpages.npd.no/factpages/).\\n\\nThe NPD database contains information on more than 7500 wells, 4000 surveys,\\n120 fields, 1000 facilities, 700 companies, 1200 licenses and more.\\n\\n<img hspace=\"100\" src=\"https://petroware.no/images/NpdIoBox.250.png\">\\n\\nNPD I/O is a Java library for accessing this information through a clean and\\nsimple to use Java API. NPD I/O is available for Java (`NpdIo.jar`).\\nThe library is lightweight (< 0.1MB) and self-contained; It has no external\\ndependencies. NPD I/O access data directly through HTTP - no login or\\npassword is required.\\n\\nNPD I/O web page: https://petroware.no/html/npdio.html\\n\\n\\n## Setup\\n\\nCapture the NPD I/O code to local disk by:\\n\\n```\\n$ git clone https://github.com/Petroware/NpdIo.git\\n```\\n\\n\\n## Dependencies\\n\\nNPD I/O has no external dependenies.\\n\\n\\n## Javadoc\\n\\nPublic Javadoc: https://petroware.no/products/npdio/javadoc/index.html\\n\\n\\n## Example application\\n\\nThis video shows how NPD I/O can be used in an application:\\n\\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=z2kM-H1VF_Y\"\\n   target=\"_blank\">\\n   <img src=\"http://img.youtube.com/vi/z2kM-H1VF_Y/0.jpg\" alt=\"YouTube video\"\\n   hspace=\"100\" width=\"500\" border=\"10\"/>\\n</a>\\n\\nThe example is taken from [Log Studio](https://petroware.no/html/logstudio.html) from [Petroware](https://petroware.no).\\n\\n\\n## Programming examples\\n\\n<p>\\nBelow are a few examples on how to access some of the main data types from the NPD database.\\n\\n```java\\n   import no.petroware.npdio.field.*;\\n   import no.petroware.npdio.well.*;\\n   :\\n\\n   //\\n   // Read all NPD development wellbores\\n   //\\n   List<NpdWellbore> wellbores = NpdDevelopmentWellboreReader.readAll();\\n\\n   // Loop over the wellbores and write to console\\n   for (NpdWellbore wellbore : wellbores)\\n     System.out.println(wellbore);\\n\\n   :\\n\\n\\n   //\\n   // Read NPD fields\\n   //\\n   List<NpdField> fields = NpdFieldReader.readAll();\\n\\n   // Loop over the fields and read production for each\\n   for (NpdField field : fields) {\\n\\n     System.out.println(field.getName());\\n\\n     // Read production data\\n     ProductionReader.readAll(field);\\n\\n     // Write oil production to console\\n     for (Production.Entry productionEntry : field.getProduction().getEntries()) {\\n       int year = productionEntry.getYear();\\n       int month = productionEntry.getMonth();\\n       double oilProduction = productionEntry.getOil();\\n\\n       System.out.println(\"Oil production: \" + year + \"/\" + month + \": \" + oilProduction);\\n     }\\n   }\\n\\n   :\\n```\\n\\nNote that in an actual client implementation the reading process would better be\\ndone asynchronous in threads. The NPD I/O library is all thread-safe.\\n\\nNPD I/O includes Java implementations for the NPD data types _wellbore_, _license_,\\n_field_, _company_, _survey_, _facility_, _discovery_, _business arrangement area_,\\n_pipeline_ and _stratigraphy_.\\n\\nNPD I/O access data through HTTP. As the URL for each data type may change over time,\\nthey can be provided explicitly by the client.\\n\\nAs of October 2019 the correct URLs for each NPD data type is as follows:\\n\\n| Data type                   | URL                    |\\n|-----------------------------|------------------------|\\n| ``NpdCompany``              | ``https://npdfactpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/company&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=CSV&Top100=false&IpAddress=92.221.121.112&CultureCode=en`` |\\n| ``NpdDevelopmentWellbore``  | ``https://npdfactpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_development_all&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=CSV&Top100=false&IpAddress=80.239.106.206&CultureCode=en`` |\\n| ``NpdDiscovery``            | ``https://npdfactpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/discovery&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=CSV&Top100=false&IpAddress=92.221.71.51&CultureCode=en`` |\\n| ``NpdExplorationWellbore``  | ``https://npdfactpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_exploration_all&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=CSV&Top100=false&IpAddress=80.239.106.206&CultureCode=en`` |\\n| ``NpdField``                | ``https://npdfactpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/field&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=CSV&Top100=false&IpAddress=92.221.121.112&CultureCode=en`` |\\n| ``NpdFixedFacility``        | ``https://npdfactpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/facility_fixed&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=CSV&Top100=false&IpAddress=92.221.121.112&CultureCode=en`` |\\n| ``NpdLicense``              | ``https://npdfactpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/licence&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=CSV&Top100=false&IpAddress=213.225.65.178&CultureCode=en`` |\\n| ``NpdMoveableFacility``     | ``https://npdfactpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/facility_moveable&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=CSV&Top100=false&IpAddress=92.221.121.112&CultureCode=en`` |\\n| ``NpdOtherWellbore``        | ``https://npdfactpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_other_all&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=CSV&Top100=false&IpAddress=80.239.106.206&CultureCode=en`` |\\n| ``NpdPipeline``             | ``https://npdfactpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/tuf_pipeline_overview&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=CSV&Top100=false&IpAddress=92.221.71.51&CultureCode=en`` |\\n| ``NpdProduction``           | ``https://npdfactpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/field_production_monthly&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=CSV&Top100=false&IpAddress=213.225.65.178&CultureCode=en`` |\\n| ``NpdSurvey``               | ``https://npdfactpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/survey&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=CSV&Top100=false&IpAddress=213.225.65.178&CultureCode=en`` |\\n\\n\\n## About Petroware\\n\\nPetroware AS is a software company within the data management, data analytics,\\npetrophysics, geology and reservoir engineering domains.\\n\\nPetroware creates highly advanced software components and end-user products that\\nacts as a research platform within software architecture and scalability, system design,\\nparallelism and multi-threading, user experience (UX) and usability analysis as well\\nas development methodologies and techniques.\\n\\n**Petroware AS**<br>\\nStavanger - Norway<br>\\n[https://petroware.no](https://petroware.no)<br>\\ninfo@petroware.no\\n\\n'},\n",
       " {'repo': 'f0nzie/rNodal.oilwells',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': '---\\ntitle: \"Transforming Excel well raw data into datasets\"\\noutput:\\n  html_document:\\n    keep_md: yes\\n    toc: yes\\n  html_notebook:\\n    toc: yes\\n  pdf_document:\\n    toc: yes\\n---\\n\\n```{r setup, include=F, error=T, message=F, warning=F}\\nknitr::opts_chunk$set(echo=T, comment=\"#:>\", error=T, warning=F, message = F, fig.align = \\'center\\', collapse=TRUE)\\n```\\n\\n\\n<!-- README.md is generated from README.Rmd. Please edit that file -->\\n\\n\\n## This is part of the R package `rNodal.oilwells`\\n\\nThe goal of `rNodal.oilwells` is to start processing well raw data and transforming it into tidy data.\\n\\n## Installation\\n\\nYou can install rNodal.oilwells from github with:\\n\\n```{r gh-installation, eval = FALSE}\\n# install.packages(\"devtools\")\\ndevtools::install_github(\"f0nzie/rNodal.oilwells\")\\n```\\n\\nSoon to be presented to CRAN.\\n\\n## Motivation\\nOne of the big challenges of this new era of data science. machine learning and artificial intelligence is getting unhooked from the habit of working with spreadsheets. They have been around for 30+ years and were awesome. But spreadsheets - or worksheets - do not scale well with massive amounts of data; or continuous streams of data; or other characteristics that are key for taking good and sound decisions such as **reproducibility**. Besides, spreadsheets have not kept with the times so we have seen the plotting capabilities getting very much behind of other software.\\n\\n*Plots are the most expressive way that you can show your data and analysis.*\\n\\nThis time we will start with some well raw data. This data is part of the input data that we require to create well models for nodal analysis, IPR/VLP calibration with well test data, troubleshooting, plan a stimulation job, or reviewing the well technical potential. In my case, this data was input for Petroleum Experts\\'s **Prosper**. But the same could have been used with Schlumberger\\'s **Pipesim**, or any other.\\n\\n\\nAgain, we will use R for these tasks. What we will do is:\\n\\n* Read the Excel data into R\\n* Perform a basic statistics on the raw data\\n* Find problems with data: data missing or improperly entered\\n* Deal with missing data and correct typing issues\\n* Convert the raw data to tidy data before analysis and plotting\\n* Save the tidy data\\n* See what story the data is trying to tell us\\n* Present our discoveries\\n\\n## Setting the stage\\n\\nIn order for you to be able to reproduce this analysis, you will need to install R, Rtools and RStudio. They are very easy to install. And the best of all, they are free.\\n\\nDon\\'t be mistaken. This is high quality software that will lead you to a world full of discoveries. So, I am assuming that at least you have installed R and that you already have your RStudio screen in front of you. This is supposed to be a sort of introductory session to R, so, I am assuming that you have little or no previous experience with R either. If you are an experienced user, you will skip to the end very quick.\\n\\nRemember, R has been designed by scientists for the use of scientists and engineers. It is not only a tool for discovery but for development. I showed a little bit of it with the article on the [compressibility factor](https://www.linkedin.com/pulse/building-your-own-petroleum-engineering-library-r-humble-reyes).\\n\\n\\n\\n## The Raw Data\\n\\nWe will start by reading the raw data. Raw data is data as-is. It hasn\\'t been cleaned up or checked or organized. Although this raw data has had some treatment to allow us focus on the main goal. You will have access to the raw data via GitHub. I will publish all the material there: raw data, datasets, scripts, notebooks, etc. I may even publish a R package to make the installation much easier for you.\\n\\nThe raw is about input data for 100 wells. The input data is the minimum required to create a well model under any nodal analysis software. The well data could be grouped as: general data (well name, field, platform), well type data (fluid, completion type, artificial lift method), PVT data, IPR data, VLP data, geothermal data, gas lift data (for those wells that have artificial lift), and well test data.\\n\\n**General data**\\n\\n```{r, out.width = \"500px\", echo=FALSE}\\nknitr::include_graphics(\"https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAyfAAAAJDk5ZWYxMjRmLTA1NzctNDZmNy05YmMzLWI4MzMwMTA3NGFiMQ.jpg\")\\n```\\n\\n**Well type data**\\n\\n```{r, out.width = \"500px\", echo=FALSE}\\nknitr::include_graphics(\"https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAA2qAAAAJDkxN2Y5OTA3LWJiNWQtNDE3Yy04NTgxLTBlYjhhNGZkMzNkZA.jpg\")\\n```\\n\\n**PVT data:**\\n\\n```{r, out.width = \"600px\", echo=FALSE}\\nknitr::include_graphics(\"https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAo-AAAAJDgxZmIyOTVhLTU0OTgtNGIzNy04Y2JlLTJjODJhOWU3ZDcxYQ.jpg\")\\n```\\n\\n** IPR data: **\\n\\n```{r, out.width = \"500px\", echo=FALSE}\\nknitr::include_graphics(\"https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAqNAAAAJDljYjEyM2IxLWVhYzgtNDYyMi04ZmYyLWQ4MjIxNmNkMWQ0ZQ.jpg\")\\n```\\n\\n\\n**Well test data:**\\n\\n```{r, out.width = \"800px\", echo=FALSE}\\nknitr::include_graphics(\"https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAmwAAAAJDkyNjVmZWE1LWI1MzYtNGM4OC04NWNjLWY1YWY0OTUxNjdiYg.jpg\")\\n```\\n\\nThe well test data transformation into tidy data will be a major task but that\\'s life. That\\'s how raw data comes. And then we use tools like R for the data munging. It will be fun.\\n\\n\\n## Reading the raw data\\n\\nNow, back to our RStudio screen. R can read virtually any data format out there. If you just installed R and haven\\'t installed anything else what you have is r-base. You can do a lot of stuff with it. But you wouldn\\'t able to read an Excel spreadsheet. You have to install a package for that. The packages are supplements to the base R. If you need some specific type of plot or a statistical distribution that you didn\\'t find in r-base you just install the package. There are 11,000+ of them. They can also be installed directly from the internet. We will start by installing the package xlsx which will allow us to read Excel .xlsx files.\\n\\n\\n```{r eval=FALSE}\\ninstall.packages(\"xlsx\")\\n```\\n\\nOnce the package is installed we proceed to read the raw data:\\n\\n```{r}\\n# load the library xlsx\\nlibrary(xlsx)\\n\\n# read the raw data\\nmyXl <- read.xlsx(\"./inst/extdata/oilfield_100w_raw_data.xlsx\", \\n                  sheetIndex = 1, stringsAsFactors=FALSE)\\n```\\n\\nI placed the raw data file under the directory ./inst/extdata, that why the long path. In R packages is very usual to place the raw data under this folder.\\n\\nThe first part of the command we see myXl, which is an object that will be holding whatever the data is inside the file. read.xlsx is the function that reads the Excel file. Then comes the long string \"./inst/extdata/oilfield_100w_raw_data.xlsx\", then a comma and a number \"1\" that means the sheet number.\\n\\nAfter you run this command take a look the top right side of your screen. Specifically, the Environment tab. You will see that the object myXl is showing this:\\n\\n```{r, out.width = \"500px\", echo=FALSE}\\nknitr::include_graphics(\"https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAxtAAAAJGE3NGVmZDQ5LWY4ZDYtNDExNy04YzE3LTFmMDk0YmZjMTcxNQ.jpg\")\\n```\\n\\n\\nThat means 100 **observations** or rows and 61 **variables** or columns. The raw data is already living in R. That is how rows and columns are called in data science jargon: observations and variables. Remember that because you will be seeing it a lot.\\n\\nNow, if you double-click on the myXL object R automatically will open a data viewer for you.\\n\\n```{r, out.width = \"600px\", echo=FALSE}\\nknitr::include_graphics(\"https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAuBAAAAJGRjZWM4OWNiLWY1NDMtNDZhMS04OWE3LWY1ZjZiYWM3ZjI3MA.jpg\")\\n```\\n\\n\\nYou can get the raw data file `oilfield_100w_raw_data.xlsx` via [GitHub](https://github.com/f0nzie/rNodal.oilwells/blob/master/inst/extdata/oilfield_100w_raw_data.xlsx). Download the file and start practicing opening the file and loading it in R.\\n\\n\\n## The notebook is your friend\\n\\nAnother thing that you will notice in this lecture is that we can combine text, math, equations and results in the same document. As a matter of fact, I am writing all of this in a R Markdown document or notebook. You can see it as the README of the package in GitHub here. It is the file README.md in green highlight.\\n\\n```{r, out.width = \"600px\", echo=FALSE}\\nknitr::include_graphics(\"https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAvDAAAAJGMwYzIyODg4LTRkOTMtNDgyOS1hYjk1LTUwMjczNjc1OGI0OA.jpg\")\\n```\\n\\nWriting project or analysis documentation this way is not only useful but a time saver. You don\\'t need to type your text in Word, for instance, and copy-paste the calculations or plots in the document afterwards. And most important of all, you reduce the chance of errors. You will see for yourself later when we mix calculations inside and together with the text.\\n\\n# What\\'s next?\\n\\n* Data introspection\\n\\n* Summarizing data\\n\\n* Finding and filling missing data\\n\\n* Grouping, categorizing the data\\n\\n* Analysis and plotting of the numeric data\\n\\n* Converting the well text data that is bar-separated to columnar format\\n\\n* Join tables by a key variable\\n\\n* Well naming convention\\n\\nBefore we begin some tips about the well naming that is used for classification. We will use this later for summarizing data such as how many wells per platform, what type of completion has the best producers, what is the platform with wells with high watercut, etc.\\n\\nAs the figure explains, the first four letters is the abbreviated name of the field. Since we are working with one field only in this lecture, all of the wells should have the same field name. After the dash, next is the platform. It is only one letter. There are four platforms M, Q, R and S. They should be in uppercase. Next after that is thee 3 digit well number. Not four or two or one; it is 3-digit number. Then a dash, and a two-letter completion type. Because we are using gas lift wells and have two producing zones we require dual completions, one with the long string (LS) and the other with the short string (SS). Wells with a unique tubing string are marked (TS).\\n\\n\\n```{r, out.width = \"600px\", echo=FALSE}\\nknitr::include_graphics(\"https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAuhAAAAJDQ2NDZjYmE3LTBlZWEtNGMzZC05OWIwLWIzZGI2ZmQzNzNlNw.jpg\")\\n```\\n\\nSo, our first task is to ensure the wells are named correctly. That is essential for the classification and analysis that we will perform later. Likely what we are going to find is:\\n\\n* Typos\\n\\n* Combination of uppercase and lowercase\\n\\n* Omitting the dashes; omitting letters\\n\\n* Using arbitrary well numbers instead of 3-digit; or\\n\\n* Absence of well name at all\\n\\n* We will address this using R.\\n\\n\\nNext, is [5.1 Data introspection](https://github.com/f0nzie/rNodal.oilwells/blob/master/inst/notebooks/Part_05.1.md)\\n\\n'},\n",
       " {'repo': 'AnneEstoppey/EasyDataTools',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': '# EasyDataTools\\nRepository with friendly workflows in Jupyter Notebooks to work with mainly wellbore datasets from Norwegian Petroleum Directorate (NPD, public data, API). \\n#\\n## Geospatialisation - Interactive map with Folium\\nIn the following notebook:\\n```\\n/notebooks/interactive_map_Folium.ipynb\\n```\\nwe will create an interactive map with Folium. We worked with clustered markers, layers which can be switched on/off, and added a fullscreen button.\\n\\nThe datasets are shapefiles coming from the Norwegian Petroleum Directorate, which is public data, and can be found [here](https://www.npd.no/en/about-us/information-services/available-data/map-services/). We used the wells and the production licences shapefiles.\\n\\nThis is a first version of this notebook, and we hope to add more functionalities in future. \\n\\n<br>\\n<img src=\"images/folium_map_view1.jpg\" width=\"700\">\\n<br>\\n<img src=\"images/folium_map_view2.jpg\" width=\"700\">\\nOpen Folium map notebook in nbviewer here: (you\\'ll have to run the notebook to display the map) https://nbviewer.jupyter.org/github/AnneEstoppey/EasyDataTools/blob/master/notebooks/interactive_map_Folium.ipynb.<br>\\n<br>\\n\\n**NEW: we have created a web app with our folium map which is in the following repository:**\\nhttps://github.com/AnneEstoppey/flask-folium-app\\n\\n#\\n## Geospatialisation - Mapbox\\nIn the following notebook:\\n```\\n/notebooks/geospatialisation_well_data_norge.ipynb\\n```\\nwe will work with both the Exploration wellbore and the lithostratigraphy datasets from Norwegian Petroleum Directorate (NPD). The wellbore datasets are public domain. They are available [here](https://data.norge.no/data/oljedirektoratet/br%C3%B8nnbane-wellbore). We will use the API connection to get the datasets <br> <br>\\n\\n### Description of the different data \\'wrangling\\' steps:\\n1. Get the Exploration dataset, select columns and the wells for North Sea and Norwegian Sea only.\\n\\n2. Get the lithostratigraphy dataset which has information about Formations and Groups for each well (including top and bottom depths). Select specific columns and only North Sea and Norwegian Sea areas. \\n\\n3. Merge a selection of columns from the Exploration dataframe together with the lithostratigraphy dataframe.\\n\\n4. Add Geological age in the merged dataframe.\\n\\n5. The resulting is a dataframe for all Explorations wells in North Sea and Norwegian Sea, with Formation, Group and Age for each row. Each row also has coordinates, which will allow us to export to a point SHAPEFILE and examine our dataset in a geospatial context, here we are using MAPBOX.\\n\\n### Display in MapBox - wells with Triassic\\nClick [HERE](https://api.mapbox.com/styles/v1/annee/cjtxe6kxk2n9v1fpiqravigpi.html?fresh=true&title=true&access_token=pk.eyJ1IjoiYW5uZWUiLCJhIjoiY2puYng1ZjNlMDU3djN2c2YxdGxqeTk5biJ9.Iq8vP18GcAukiVpWNqJ5Pg#5.5/59.620860/7.760410/0) to open map in MapBox.\\n<br>\\n<img src=\"images/MapBox_wells_with_Triassic.jpg\" width=\"700\">\\n<br>\\n#\\n## Visualisation - wells stack bar graph\\nIn the following notebook:\\n```\\n/notebooks/well_data_norge_API_altair_graph.ipynb\\n```\\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AnneEstoppey/EasyDataTools/master?filepath=%2Fnotebooks%2Fwell_data_norge_API_altair_graph.ipynb)\\n\\nwe will work with the Exploration Wellbore Dataset from Norwegian Petroleum Directorate (NPD). The wellbore datasets are public domain. They are available [here](https://data.norge.no/data/oljedirektoratet/br%C3%B8nnbane-wellbore). <br> <br>\\nWe will generate several stack bar graphs, using the ALTAIR library. The graphs will show us:\\n1. Status of all Exploration wells on the Norwegian Shelf since beginning of exploration, by year, number of wells and well content (oil/gas/condensate or any combination)\\n2. Status of Exploration wells between 1980 and 2019, by region: NORTH SEA, NORWEGIAN SEA and BARENTS SEA\\n\\nWe wished to make this notebook easy to follow so that anybody could understand the different steps of data wrangling to finally the stack bar graphs. We hope you enjoy its simplicity! <br><br>\\n<img src=\"images/NPD_wellbore_altair_graph1_M_v2.jpg\" width=\"900\">\\n\\n'},\n",
       " {'repo': 'ontop/npd-benchmark',\n",
       "  'language': 'TSQL',\n",
       "  'readme_contents': 'npd-benchmark (v1.10.0)\\n====================\\n\\nOfficial Documentation: \\n\\n* http://ontop.github.io/npd-benchmark/ (All versions)\\n\\nComponents of the NPD Benchmark (v1.10.0)\\n\\n* **NPD Dataset** (mysql dump of the original dataset, postgres and mysql schemas)\\n\\n* **Vig v1.8.1**: Generates custom datasets starting from the NPD Dataset (https://github.com/ontop/vig)\\n\\n* **OWL 2 QL NPD ontology** (containing information about petroleum activities on the Norwegian continental shelf)\\n\\n* **R2RML and OBDA mappings** for PostgreSQL and MySQL\\n\\n* **31 between Real World and Technical Queries**:\\n\\n  * 13 BGP Queries\\n  * 2 Queries with OPTIONAL\\n  * 7 Queries with Aggregate Functions\\n  * 9 Queries with Tree-witnesses (currently formulated as BGPs)\\n\\n* **OBDA-Mixer v1.2.0**: Automatized Test Platform (https://github.com/ontop/obda-mixer) for OBDA Systems\\n\\nCredits & Disclaimer\\n--------\\n\\nThis benchmark is based on the [Norwegian Petroleum Directorate](http://sws.ifi.uio.no/project/npd-v2/)\\n(March 2014). \\nNPD does not provide any warranty nor takes any responsibility on this work nor on any of the provided files.\\n\\nThe mappings, ontology, and query files have been adapted for OBDA benchmarking purposes.\\n\\nPublications \\n---------\\n\\n* **The NPD benchmark: Reality check for OBDA systems**. Davide Lanti, Martin Rezk, Guohui Xiao, and Diego Calvanese. In Proc. of the 18th Int. Conf. on Extending Database Technology (EDBT 2015). ACM Press, 2015. (http://openproceedings.org/2015/conf/edbt/paper-350.pdf)\\n\\n* **A Scalable Benchmark for OBDA Systems**. Preliminary Report Calvanese, D.; Lanti, D.\\n; Rezk, M.; Slusnys, M.; and Xiao, G. In In Proc. of the 3rd Int. Workshop on OWL Re\\nasoner Evaluation (ORE 2014), 2014. (http://www.ghxiao.org/publications/2014-ore-npd.pdf)\\n\\n* **The NPD Benchmark for OBDA Systems**. Lanti, D.; Rezk, M.; Slusnys, M.; Xiao, G.; and\\n Calvanese, D. In Proc. of the 10th International Workshop on Scalable Semantic Web \\nKnowledge Base Systems (SSWS 2014), 2014. (http://www.ghxiao.org/publications/2014-ssws-npd.pdf) \\n\\nPresentations and Results\\n----------\\n\\n[(Outdated) Talk at EDBT 2015](https://github.com/ontop/npd-benchmark/tree/master/evaluations/edbt2015Talk)\\n\\nContacts\\n----------\\n\\n* [Davide Lanti](http://www.inf.unibz.it/~dlanti/)\\n* [Guohui Xiao](http://www.ghxiao.org)\\n* [Diego Calvanese](http://www.inf.unibz.it/~calvanese/)\\n'},\n",
       " {'repo': 'mwentzWW/petrolpy',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': '# petrolpy\\n\\nThis repository is meant to serve as an open source option for petroleum engineers and geoscientists with Python packages/modules. Check the wiki page for current projects. Please share any project ideas you may have. **All projects are a work in progress**.\\n\\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/mwentzWW/petrolpy/master)\\n\\n---\\n\\nYou can run the notebooks by clicking the \"launch binder\" button above. The required packages are in the environment.yaml file.\\n\\n---\\n\\n## Highlights\\n\\n* Log Normal Distribution Probability Density Function from data (EUR Example)\\n![alt text](petrolpy/Examples/Example_plots/pdf_example_output.png \"PDF Plot\")\\n* Log Normal Distribution Probit Plot (EUR Example)\\n![alt text](petrolpy/Examples/Example_plots/probit_example_output.png \"Probit Plot\")\\n'},\n",
       " {'repo': 'f0nzie/evolution_data_science_petroleum_engineering',\n",
       "  'language': None,\n",
       "  'readme_contents': '---\\noutput: github_document\\n---\\n\\n<!-- README.md is generated from README.Rmd. Please edit that file -->\\n\\n\\n```{r setup, include = FALSE, error=TRUE, message=FALSE, warning=FALSE}\\nknitr::opts_chunk$set(echo = TRUE, \\n                      comment = \"#>\",\\n                      collapse = TRUE,\\n                      error = TRUE,\\n                      warning = FALSE,\\n                      message = FALSE,\\n                      cache = TRUE,\\n                      fig.align = \\'center\\')\\n```\\n\\n# Evolution of data science, machine learning and artificial intelligence in Petroleum Engineering papers\\n\\n## README\\nThis is the standalone version of the article that appeared in LinkedIn at this link https://www.linkedin.com/pulse/evolution-data-science-machine-learning-artificial-petroleum-reyes/https://www.linkedin.com/pulse/evolution-data-science-machine-learning-artificial-petroleum-reyes/, under the title **Evolution of data science, machine learning and artificial intelligence in Petroleum Engineering papers**.\\n\\nThis notebook uses primarily the package [petro.One](https://cran.rstudio.com/web/packages/petro.One/index.html) that can be installed easily in R or RStudio. The package facilitates the search of papers by collecting papers in a search in tables or dataframes that can later be saved as CSV or Excel files.\\n\\n## Motivation\\nI have been lately curious about how the terms `data science`, `machine learning` and `artificial intelligence` evolved over the years in petroleum engineering papers. It is not that machine learning or data science are brand new fields of study; quite the contrary, they have been around for decades, as it has been artificial intelligence. \\n\\n## Procedure\\nThe current hype is due to (i) the sheer computational power available; (ii) the availability of fast engines in the cloud; (iii) open source software such as Python, R, Octave, Hadoop, Spark, Scala, MongoDB, Javascript, VirtualBox, containerization tools;  (iv) an operating system that runs under all platforms Unix/Linux and different microprocessor architectures;; (v) the almost-free online education by some of the best professors in the world; (vi) and the dissemination of knowledge through the internet (books, papers, articles). It all have commingled at the right time, at the right places (everywhere).\\n\\nInitially, I will do the search using the R package [petro.One](https://cran.rstudio.com/web/packages/petro.One/index.html) - that I have updated few days ago (thanks, William \"Bill\" Donovan for your inquiries)-,  while on a quest for finding the right papers given few keywords on the subject. \\n\\nI will not give an opinion on what the trends mean for the moment. Let\\'s see the numbers to sink in for couple of days. Besides, I might not have come with all the right keywords for this quick study. \\n\\nCan you think of keywords relevant to the matter in discussion?\\n\\nI will start with the broad terms of the moment: `artificial intelligence`, `machine learning` and `data science`. From there, we could try some combinations, precursors and derivatives of the terms, and we\\'ll see what is the trend.\\n\\nThe number of papers is on **y-axis**, the year on the **x-axis**. I am using a [Rmarkdown](https://rmarkdown.rstudio.com/) notebook for this article; [R version 3.5.2](https://cloud.r-project.org/) and [RStudio 1.2.1226](https://www.rstudio.com/products/rstudio/download/preview/). To get the data, I am interrogating the [OnePetro](https://www.onepetro.org/) website. The R package I wrote, petro.One, simplifies the search by extracting the papers in multiples of one thousand (the maximum allowed by OnePetro), and creates the corresponding datasets in the shape of tables. You can take a look at the petro.One website [here](https://github.com/f0nzie/petro.One).\\n\\n\\n## Artificial Intelligence\\n* First, what year do you see mentions of the keyword -the whole term \"artificial intelligence\" (we are not looking for separate words), or starts being used in petroleum engineering papers?\\n\\n* When do we see a fast increment on papers with mentions of `artificial intelligence`?\\n\\n* Do we see a peak?\\n\\n* Is the trend ascendant or descendant?\\n\\n\\n```{r}\\n# artificial intelligence\\nlibrary(petro.One)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n\\n# provide two different set of keywords to combine as vectors\\nmajor   <- c(\"artificial intelligence\")\\n\\nresults_ai <- run_papers_search(major, \\n                             get_papers = TRUE,       # return with papers\\n                             verbose = FALSE,         # show progress\\n                             len_keywords = 4,        # naming the data file\\n                             allow_duplicates = FALSE) # by paper title and id\\n\\n(papers_ai <- results_ai$papers)\\n\\n# write to CSV file\\nout_dir <- \"./inst/rawdata\"\\nwrite.csv(papers_ai, file = file.path(out_dir, \"papers_ai.csv\"), \\n          row.names = FALSE)\\n\\n# plot on AI by year of publication\\npapers_ai %>% \\n    group_by(year) %>% \\n    na.omit() %>% \\n    summarize(n = n()) %>% \\n    {. ->> papers_ai_by} %>% \\n    ggplot(., aes(x = year, y = n)) +\\n    geom_point() +\\n    geom_smooth(method = \"loess\") +\\n    labs(title = \"\\'Artificial Intelligence\\' papers by Year\")\\n```\\n\\n### Computational Intelligence\\n`Computational Intelligence` is another term that have seen frequently used by scholars instead of artificial intelligence. Some of them think that there is no artificial intelligence but that originated from computers. Makes sense. Unless we are talking of biological duplication of human intelligence. You will see the term computational intelligence in books and papers outside petroleum engineering.\\n\\n```{r}\\n# computational intelligence\\nlibrary(petro.One)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n\\n# provide two different set of keywords to combine as vectors\\nmajor   <- c(\"computational intelligence\")\\n\\nresults_ci <- run_papers_search(major, \\n                             get_papers = TRUE,       # return with papers\\n                             verbose = FALSE,         # show progress\\n                             len_keywords = 4,        # naming the data file\\n                             allow_duplicates = FALSE) # by paper title and id\\n\\n(papers_ci <- results_ci$papers)\\nout_dir <- \"./inst/rawdata\"\\nwrite.csv(papers_ci, file = file.path(out_dir, \"papers_ci.csv\"), \\n          row.names = FALSE)                               # write to CSV file\\n\\n# plot on CI by year of publication\\npapers_ci %>% \\n    group_by(year) %>% \\n    na.omit() %>% \\n    summarize(n = n()) %>% \\n    {. ->> papers_ci_by} %>% \\n    ggplot(., aes(x = year, y = n)) +\\n    geom_point() +\\n    geom_smooth(method = \"loess\") +\\n    labs(title = \"Computational Intelligence papers by Year\")\\n```\\n\\n### Intelligence\\nWe humans have been very passionate about bringing about certains forms of intelligence. From ancient times we were just looking to replace humans in certain repetitive tasks. We could safely says that the endeavor is 3000 years old. Plato and Aristoteles are mentioned in literature on the origins of AI, but really goes further back in time if we investigate other civilizations.\\n\\n```{r}\\n# intelligence\\nlibrary(petro.One)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n\\n# provide two different set of keywords to combine as vectors\\nmajor   <- c(\"intelligence\", \"intelligent\")\\n\\nresults_ii <- run_papers_search(major, \\n                             get_papers = TRUE,       # return with papers\\n                             verbose = FALSE,         # show progress\\n                             len_keywords = 4,        # naming the data file\\n                             allow_duplicates = FALSE) # by paper title and id\\n\\n(papers_ii <- results_ii$papers)\\nout_dir <- \"./inst/rawdata\"\\nwrite.csv(papers_ii, file = file.path(out_dir, \"papers_ii.csv\"), \\n          row.names = FALSE)                               # write to CSV file\\n\\n# plot on II by year of publication\\npapers_ii %>% \\n    group_by(year) %>% \\n    na.omit() %>% \\n    summarize(n = n()) %>% \\n    {. ->> papers_ii_by} %>% \\n    ggplot(., aes(x = year, y = n)) +\\n    geom_point() +\\n    geom_smooth(method = \"loess\") +\\n    labs(title = \"On `Intelligence` papers by Year\")\\n```\\n\\n## Machine Learning\\n`Machine Learning` is a term of more recent origin. We can tell just by looking at `linear `regression` as one of the most used or common machine learning algorithm, notwithstanding that liner regression has been around hundreds of years. \\n\\nSo the joke goes:\\n\\n> if you can run a linear regression\\nthen you are doing machine learning.\\nTherefore, you are in the artificial intelligence business.\\n\\n:) You know that something is not right!\\n\\n```{r}\\n# machine learning\\nlibrary(petro.One)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n\\n# provide two different set of keywords to combine as vectors\\nmajor   <- c(\"machine learning\")\\n\\nresults_ml <- run_papers_search(major, \\n                             get_papers = TRUE,       # return with papers\\n                             verbose = FALSE,         # show progress\\n                             len_keywords = 4,        # naming the data file\\n                             allow_duplicates = FALSE) # by paper title and id\\n\\n(papers_ml <- results_ml$papers)\\nout_dir <- \"./inst/rawdata\"\\nwrite.csv(papers_ml, file = file.path(out_dir, \"papers_ml.csv\"), \\n          row.names = FALSE)                               # write to CSV file\\n\\n# plot on ML by year of publication\\npapers_ml %>% \\n    group_by(year) %>% \\n    na.omit() %>% \\n    summarize(n = n()) %>% \\n    {. ->> papers_ml_by} %>% \\n    ggplot(., aes(x = year, y = n)) +\\n    geom_point() +\\n    geom_smooth(method = \"loess\") +\\n    labs(title = \"Papers on \\'Machine Learning\\' by Year\")\\n```\\n\\n### Algorithm\\nThis is another keyword of common use these days. Machine Learning is about algorithms. \\n\\n```{r}\\n# algorithm\\nlibrary(petro.One)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n\\n# provide two different set of keywords to combine as vectors\\nmajor   <- c(\"algorithm\")\\n\\nresults_algo <- run_papers_search(major, \\n                             get_papers = TRUE,       # return with papers\\n                             verbose = FALSE,         # show progress\\n                             len_keywords = 4,        # naming the data file\\n                             allow_duplicates = FALSE) # by paper title and id\\n\\n(papers_algo <- results_algo$papers)\\nout_dir <- \"./inst/rawdata\"\\nwrite.csv(papers_algo, file = file.path(out_dir, \"papers_algo.csv\"), \\n          row.names = FALSE)                               # write to CSV file\\n\\n# plot on algos by year of publication\\npapers_algo %>% \\n    group_by(year) %>% \\n    na.omit() %>% \\n    summarize(n = n()) %>% \\n    {. ->> papers_algo_by} %>% \\n    ggplot(., aes(x = year, y = n)) +\\n    geom_point() +\\n    geom_smooth(method = \"loess\") +\\n    labs(title = \"Papers using the term \\'Algorithm\\' by Year\")\\n```\\n\\n```{r}\\n# predictive analytics\\nlibrary(petro.One)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n\\n# provide two different set of keywords to combine as vectors\\nmajor   <- c(\"predictive analytics\")\\n\\nresults_pa <- run_papers_search(major, \\n                             get_papers = TRUE,       # return with papers\\n                             verbose = FALSE,         # show progress\\n                             len_keywords = 4,        # naming the data file\\n                             allow_duplicates = FALSE) # by paper title and id\\n\\n(papers_pa <- results_pa$papers)\\nout_dir <- \"./inst/rawdata\"\\nwrite.csv(papers_pa, file = file.path(out_dir, \"papers_pa.csv\"), \\n          row.names = FALSE)                               # write to CSV file\\n\\n# plot on algos by year of publication\\npapers_pa %>% \\n    group_by(year) %>% \\n    na.omit() %>% \\n    summarize(n = n()) %>% \\n    {. ->> papers_pa_by} %>% \\n    ggplot(., aes(x = year, y = n)) +\\n    geom_point() +\\n    geom_smooth(method = \"loess\") +\\n    labs(title = \"Papers using the term \\'Predictive Analytics\\' by Year\")\\n```\\n\\n```{r}\\n# statistical model\\nlibrary(petro.One)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n\\n# provide two different set of keywords to combine as vectors\\nmajor   <- c(\"statistical model\")\\n\\nresults_sm <- run_papers_search(major, \\n                             get_papers = TRUE,       # return with papers\\n                             verbose = FALSE,         # show progress\\n                             len_keywords = 4,        # naming the data file\\n                             allow_duplicates = FALSE) # by paper title and id\\n\\n(papers_sm <- results_sm$papers)\\nout_dir <- \"./inst/rawdata\"\\nwrite.csv(papers_sm, file = file.path(out_dir, \"papers_sm.csv\"), \\n          row.names = FALSE)                               # write to CSV file\\n\\n# plot on algos by year of publication\\npapers_sm %>% \\n    group_by(year) %>% \\n    na.omit() %>% \\n    summarize(n = n()) %>% \\n    {. ->> papers_sm_by} %>% \\n    ggplot(., aes(x = year, y = n)) +\\n    geom_point() +\\n    geom_smooth(method = \"loess\") +\\n    labs(title = \"Papers using the term \\'Predictive Analytics\\' by Year\")\\n```\\n\\n## Data Science\\nThis is where the fun starts. No machine learning or artificial intelligence effort is possible without the application of data science. No dataset, no ML. No ML algorithm, no AI. Pretty simple.\\n\\n```{r}\\n# data science\\nlibrary(petro.One)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n\\n# provide two different set of keywords to combine as vectors\\nmajor   <- c(\"data science\")\\n\\nresults_ds <- run_papers_search(major, \\n                             get_papers = TRUE,       # return with papers\\n                             verbose = FALSE,         # show progress\\n                             len_keywords = 4,        # naming the data file\\n                             allow_duplicates = FALSE) # by paper title and id\\n\\n(papers_ds <- results_ds$papers)\\nout_dir <- \"./inst/rawdata\"\\nwrite.csv(papers_ds, file = file.path(out_dir, \"papers_ds.csv\"), \\n          row.names = FALSE)                               # write to CSV file\\n\\n# plot on DS by year of publication\\npapers_ds %>% \\n    group_by(year) %>% \\n    na.omit() %>% \\n    summarize(n = n()) %>% \\n    {. ->> papers_ds_by} %>% \\n    ggplot(., aes(x = year, y = n)) +\\n    geom_point() +\\n    geom_smooth(method = \"loess\") +\\n    labs(title = \"Papers on Data Science by Year\")\\n```\\n\\n### Precursors, derivatives of \"data science\"\\nData Science is a relatively new term; there are other that have been widely used in SPE papers, such \"data driven\", \"data-driven\", \"data-driven analytics\", \"data analysis\", etc.\\n\\nThe `data-driven` terms has been profusely present in petroleum engineering papers.\\n\\n```{r}\\n# data driven\\nlibrary(petro.One)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n\\n# provide two different set of keywords to combine as vectors\\nmajor   <- c(\"data driven\", \"data-driven\")\\n\\nresults_dd <- run_papers_search(major, \\n                             get_papers = TRUE,       # return with papers\\n                             verbose = TRUE,         # show progress\\n                             len_keywords = 4,        # naming the data file\\n                             allow_duplicates = FALSE) # by paper title and id\\n\\n(papers_dd <- results_dd$papers)\\nout_dir <- \"./inst/rawdata\"\\nwrite.csv(papers_dd, file = file.path(out_dir, \"papers_dd.csv\"), \\n          row.names = FALSE)                               # write to CSV file\\n\\n# plot on DD by year of publication\\npapers_dd %>% \\n    group_by(year) %>% \\n    na.omit() %>% \\n    summarize(n = n()) %>% \\n    {. ->> papers_dd_by} %>% \\n    ggplot(., aes(x = year, y = n)) +\\n    geom_point() +\\n    geom_smooth(method = \"loess\") +\\n    labs(title = \"Papers on Data Driven by Year\")\\n```\\n\\n### Analytics\\n`Analytics` is one of the preffered terms in the media, press, and also in some paper studies. Sounds kind of nice and \"sciency\". \\n\\n```{r}\\n# analytics\\nlibrary(petro.One)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n\\n# provide two different set of keywords to combine as vectors\\nmajor   <- c(\"analytics\")\\n\\nresults_an <- run_papers_search(major, \\n                             get_papers = TRUE,       # return with papers\\n                             verbose = TRUE,         # show progress\\n                             len_keywords = 4,        # naming the data file\\n                             allow_duplicates = FALSE) # by paper title and id\\n\\n(papers_an <- results_an$papers)\\nout_dir <- \"./inst/rawdata\"\\nwrite.csv(papers_an, file = file.path(out_dir, \"papers_an.csv\"), \\n          row.names = FALSE)                               # write to CSV file\\n\\n# plot on \\'analytics\\' by year of publication\\npapers_an %>% \\n    group_by(year) %>% \\n    na.omit() %>% \\n    summarize(n = n()) %>% \\n    {. ->> papers_an_by} %>% \\n    ggplot(., aes(x = year, y = n)) +\\n    geom_point() +\\n    geom_smooth(method = \"loess\") +\\n    labs(title = \"Papers on \\'Analytics\\' by Year\")\\n```\\n\\n## Data Analytics\\n`Data Analytics` is another term that is used as an equivalent of `data science`. They may not have the same meaning for different authors or companies, but let\\'s assume for the moment they are practically the same. Personally, I think they are not. You don\\'t say, for instance, I am going to study \"data analytics\". Do you?\\n\\n```{r}\\n# data analytics\\nlibrary(petro.One)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n\\n# provide two different set of keywords to combine as vectors\\nmajor   <- c(\"data analytics\")\\n\\nresults_da <- run_papers_search(major, \\n                             get_papers = TRUE,       # return with papers\\n                             verbose = TRUE,         # show progress\\n                             len_keywords = 4,        # naming the data file\\n                             allow_duplicates = FALSE) # by paper title and id\\n\\n(papers_da <- results_da$papers)\\nout_dir <- \"./inst/rawdata\"\\nwrite.csv(papers_da, file = file.path(out_dir, \"papers_da.csv\"), \\n          row.names = FALSE)                               # write to CSV file\\n\\n# plot on DA by year of publication\\npapers_da %>% \\n    group_by(year) %>% \\n    na.omit() %>% \\n    summarize(n = n()) %>% \\n    {. ->> papers_da_by} %>% \\n    ggplot(., aes(x = year, y = n)) +\\n    geom_point() +\\n    geom_smooth(method = \"loess\") +\\n    labs(title = \"Papers on \\'Data Analytics\\' by Year\")\\n```\\n\\n## Discussion\\nThere you go. Some data and some plots to start the discussion.\\n\\nAs in data science, is all addressing questions. A business question. \\n\\nWhat do you notice about the evolution of the terminology? How recent are they? Which is the oldest term? How far can you go back in time and say \\'it started here\\'? Where do you notice a most accentuated growing slope? Why data science is younger than data-driven? Do you think the authors were using the **data-driven** term to mean the future **data science**? Do you think **data-driven analytics**, **data science**, or just simply, **analytics**, are all the same?\\n\\n\\n## And what about **Big Data**?\\n\\n```{r}\\n# big data\\nlibrary(petro.One)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\n\\n# provide two different set of keywords to combine as vectors\\nmajor   <- c(\"big data\")\\n\\nresults_bd <- run_papers_search(major, \\n                             get_papers = TRUE,       # return with papers\\n                             verbose = TRUE,          # show progress\\n                             sleep = 5,               # wait time for OnePetro\\n                             len_keywords = 4,         # naming the data file\\n                             allow_duplicates = FALSE) # by paper title and id\\n\\n(papers_bd <- results_bd$papers)\\nout_dir <- \"./inst/rawdata\"\\nwrite.csv(papers_bd, file = file.path(out_dir, \"papers_bd.csv\"), \\n          row.names = FALSE)                               # write to CSV file\\n\\n# plot on BD by year of publication\\npapers_bd %>% \\n    group_by(year) %>% \\n    na.omit() %>% \\n    summarize(n = n()) %>% \\n    {. ->> papers_bd_by} %>% \\n    ggplot(., aes(x = year, y = n)) +\\n    geom_point() +\\n    geom_smooth(method = \"loess\") +\\n    labs(title = \"Papers on \\'Big Data\\' by Year\")\\n```\\n\\nFinally, do you think the term **big data** is equivalent to **data science**, or **data analytics**? How do you think the term **big data** was originated?\\n\\n## Summary of all keywords\\n\\n### Artificial Intelligence by Year\\n\\n```{r fig.width=9, fig.height=3}\\npapers_ai_by$keyword <- \"artificial intelligence\"\\npapers_ci_by$keyword <- \"computational intelligence\"\\npapers_ii_by$keyword <- \"intelligence\"\\n\\nall_papers_ai <- rbind(papers_ai_by, papers_ci_by, papers_ii_by)\\nall_papers_ai\\n\\nggplot(data = all_papers_ai, aes(x = year, y = n, color = n)) +\\n  geom_jitter(aes(size = as.numeric(n)), alpha = 0.3) +\\n  geom_rug() +\\n  facet_grid(. ~ keyword)\\n```\\n\\n### Machine Learning papers\\n\\n```{r fig.width=9, fig.height=3}\\npapers_ml_by$keyword <- \"machine learning\"\\npapers_algo_by$keyword <- \"algorithm\"\\npapers_pa_by$keyword <- \"predictive analytics\"\\npapers_sm_by$keyword <- \"statistical model\"\\n\\nall_papers_ml <- rbind(papers_ml_by, papers_algo_by, papers_pa_by, papers_sm_by)\\n\\nggplot(data = all_papers_ml, aes(x = year, y = n, color = n)) +\\n  geom_jitter(aes(size = as.numeric(n)), alpha = 0.3) +\\n  geom_rug() +\\n  facet_grid(. ~ keyword)\\n```\\n\\n### Data Science papers\\n\\n```{r fig.width=11, fig.height=4}\\npapers_ds_by$keyword <- \"data science\"\\npapers_da_by$keyword <- \"data analytics\"\\npapers_dd_by$keyword <- \"data driven\"\\npapers_bd_by$keyword <- \"big data\"\\n\\nall_papers_ds <- rbind(papers_ds_by, papers_da_by, papers_dd_by, papers_bd_by)\\n\\nggplot(data = all_papers_ds, aes(x = year, y = n, color = n)) +\\n  geom_jitter(aes(size = as.numeric(n)), alpha = 0.3) +\\n  geom_rug() +\\n  facet_grid(. ~ keyword)\\n```\\n\\n\\n## Datasets\\nThe dataset can be found under the folder `inst/rawdata`.\\n\\n'},\n",
       " {'repo': 'f0nzie/vlp-bottomhole-algorithm',\n",
       "  'language': None,\n",
       "  'readme_contents': '---\\ntitle: Descending into the bottomhole. A Marching Algorithm for Vertical Lift Performance\\n  in Petroleum Engineering\\noutput:\\n  github_document:\\n      pandoc_args: --webtex\\n---\\n\\n\\n\\n```{r setup, include = FALSE, error=TRUE, message=FALSE, warning=FALSE}\\nknitr::opts_chunk$set(echo = TRUE, \\n                      comment = \"#>\",\\n                      collapse = TRUE,\\n                      error = TRUE,\\n                      warning = FALSE,\\n                      message = FALSE,\\n                      fig.align = \\'center\\'\\n                      )\\n```\\n\\n\\n# Introduction\\nI have always been captivated by calculations performed at depth in wells. The numerous correlations and curves that were built in the golden years of production engineering are just fascinating. _Thank you Mr. Brown. Thank you Mr. Beggs._ From all the various algorithms, I particularly liked one calculating the pressure losses in the tubing as the hydrocarbon fluids ascend to the surface, also called tubing performance, or vertical lift performance (VLP). If you are a petroleum engineer who just started learning to code, there is no better exercise than calculating the pressure gradient at any depth point in the wellbore. It tests your petroleum engineering skills with fluid properties of oil, gas and water; correlations; multi-phase phenomena; pressure and temperature effects; heat transfer, etc. \\n\\nAlthough, we will not be seeing all the calculations to perform a full VLP using Hagendorn-Brown, Fancher-Brown, Duns-Ros, or Beggs-Brill correlations, this article will place you in condition to understand the whole application when it is publicly released. I will be using R as it has demonstrated a solid and reliable platform for developing rapid applications not only for statistics and data science, but also in engineering and science in general.\\n\\n\\n# Motivation\\nFor those colleagues that are looking to implement an engineering library to solve some problems arising during daily operations there is no better tool than a scripting language, be that [R](https://www.r-project.org/about.html), or [Python](https://www.python.org/). There has not been a better time to switch from Excel and VBA to do quick coding than today with all the tools provided by the __data science revolution__. Besides revision control of your work with Git, ease of sharing within your organization, building rapid prototypes, you are also following one of the key premises of data science. Actually, what adds the word __science__ to *data science*: __reproducibility__.\\n\\nThe code I am publishing here serves three purposes:\\n\\n* Familiarize the petroleum engineer with a fundamental algorithm for downhole calculations\\n* Learn the basics of scripting with R: vectors, lists, dataframes, loops and decision logic. And plotting with one of the best tools in data visualization: `ggplot2`.\\n* Show the basic structure of more complex iterating algorithms for calculating conditions at any depth point in the well.\\n\\n\\n# Computation Workflow\\nThis is a description of what the code will be doing.\\n\\n1. Read the well input data\\n1. Set the number of tubing segments\\n1. Calculate length of tubing segments\\n1. Set the starting calculation increment for the gradient\\n1. Set an starting value for the inlet and outlet pressures\\n1. Iterate through the number of tubing segments\\n    1. Calculate a depth point\\n    1. Calculate the outlet pressure at current depth\\n    1. Iterate until absolute error is within the defined tolerance\\n        1. Calculate the average pressure\\n        1. Calculate the fluid properties at P, T\\n        1. Calculate the pressure gradient \\n        1. Calculate a new outlet pressure\\n        1. Compare the absolute error vs the tolerance for the pressure\\n        1. If not converging, set inlet pressure to outlet pressure; \\n        repeat from step `6c`\\n        1. Store calculations at depth\\n    1. If more tubing segment repeat from step `6`\\n1. Make data table\\n\\n\\n* Calculating the length of the tubing segments involve dividing the well depth ($L$) by the number of depth points ($n$). In this example `n=30`.\\n\\n$$dL = L / n$$\\n\\n* For setting the starting calculation increment for the gradient we could assume 0.002 psi/ft:\\n\\n$$\\\\frac{dP}{dz} = 0.002$$\\n\\n\\n* Calculating the average pressure requires averaging the inlet and outlet pressure at the ends of the virtual pipe:\\n\\n$$p_{avg} = \\\\frac {(p_{in} + p_{out})} {2}$$\\n\\n* Calculating the pressure gradient `-dP/dz`:\\n$$\\\\left ( \\\\frac {dp} {dL} \\\\right ) = f(P_{avg}) $$\\n* Calculating a new pressure\\n\\n$$p_{i+1} = p_i - \\\\left ( -\\\\frac {dP}{dL} \\\\right )_i dL_i $$\\n\\n* Comparing the absolute error of the new pressure and the current outlet pressure. It should be less than the tolerance, otherwise, we proceed with a new iteration making $p_{in} = p_{out}$.\\n\\n$$\\\\ | \\\\frac { p_{out} - p_{i+1} } { p_{i+1}} | < \\\\epsilon$$\\n\\n# Implementation of marching algorithm for well gradient\\nFor demo purposes, only using a dummy function,  that will symbolize all the intricate calculations to find the fluid properties at the current pressure and temperature. The last thing to do is generating a dataframe with the calculated data. As a matter of fact, it could be two dataframes, one for the main results for each pipe segment; and the second dataframe -with more detail-, showing the iterations and absolute error.\\n\\n## Marching algorithm\\nI have added comments to the code as much as possible to be able to understand what every line of code is doing. There is no better way of coding than commenting code. Not only for whoever takes over your project but also keep in mind that you are documenting for your future self.\\n\\n```{r}\\n# load libraries\\nlibrary(latex2exp)\\nlibrary(ggplot2)\\n\\ntolerance = 0.00001      # will be compared against absolute error\\nthp       = 200          # initial pressure (tubing head pressure)\\ndepth_wh  = 0            # depth at wellhead\\ndepth_bh  = 9700         # depth at bottomhole\\nsegments  = 30           # calculation segments\\n\\n# rows have to be greater than segments to allocate the zero or initial value\\n# consider that in length.out parameter in the sequence below\\ndepth   <- seq.int(from = depth_wh, to = depth_bh, length.out = segments+1)\\nn       <- length(depth)   # depth points same as # rows or (segments+1)\\n\\n# dummy function that represents a lot of subsurface calculations\\nfPa <- function(x) 9e-02 + 1e-04 * x + 5e-08  * x^2 - 2e-11 * x^3\\n\\ndepth_top <- depth_wh\\ndp_dz     <- 0.002                    # 1st approximation of the gradient\\np_in      <- thp                      # the initial pressure\\noutput <- vector(\"list\")\\nfor (i in seq_len(n)) {               # n: is the number of depths or # rows\\n    depth_prev <- ifelse(i == 1, depth_top, depth[i-1])\\n    dL = depth[i] - depth_prev              # calculate dL\\n    p_out = p_in + dp_dz * dL               # calculate outlet pressure\\n    cat(sprintf(\"i=%2d depth=%8.0f dL=%8.1f segment=%d \\\\n\",  # header outer loop\\n                i, depth[i], dL, i-1))\\n    cat(sprintf(\"%8s %6s %6s %8s %8s %8s %10s \\\\n\",           # header inner loop\\n            \"iter\", \"p_in\", \"p_out\", \"p_avg\", \"p_calc\", \"dp/dz\", \"eps\"))\\n    epsilon <- 1   # initial values before inner loop\\n    iter <- 1\\n    # here we start iterating for the pressure gradient\\n    while (epsilon > tolerance) {       # loop until AE greater than tolerance\\n      p_avg <- (p_in + p_out) / 2       # calculate average pressure\\n      dp_dz <- fPa(p_avg)   # calculate gradient as function of average pressure\\n      p_calc <- p_in - (-dp_dz) * dL\\n      epsilon <- abs( (p_out - p_calc) / p_calc )  # absolute error\\n      cat(sprintf(\"%8d %6.1f %6.1f %8.2f %8.2f %8.5f %10.8f \\\\n\", \\n                  iter, p_in, p_out, p_avg, p_calc, dp_dz, epsilon))\\n      \\n      if (epsilon >= tolerance) p_out = p_calc # if error too big, iterate again\\n      iter <- iter + 1                         # with a new pressure\\n    } # end of while \\n    p_in = p_out      # assign p_out to the inlet pressure of new segment, p_in\\n    output[[i]] <- list(depth = depth[i], p_calc = p_calc,   # values to list\\n                        p_avg = p_avg, dp_dz = dp_dz)     \\n} # end-for\\n\\nout_df <- data.table::rbindlist(output)    # convert list to table\\n```\\n\\n## Plots\\nThe plots have been created using `ggplot2`, a very flexible, customizable and powerful visualization platform. I have made use of couple of advanced characteristics of ggplot: reverse the __y-axis__, and annotate the plot with [Latex](https://www.latex-project.org/) with the package `latextoexp`. Also, I am changing the default number of ticks on the y-axis using `breaks`, as well as sequences to mark the location of the ticks.\\n\\n```{r pressure-vs-gradient, fig.height=5, fig.width=5}\\n# plot pressure vs gradient\\nggplot(out_df, aes(x=dp_dz, y=p_calc)) +\\n    scale_y_continuous(limits = c(0, max(out_df$p_calc)),\\n                       breaks = seq(0, max(out_df$p_calc), 100)) + \\n    geom_line() + \\n    geom_point() + \\n    labs(title = TeX(\"Pressure vs $\\\\\\\\frac{dp}{dz}$\"))\\n```\\n\\n```{r depth-vs-gradient, fig.height=5, fig.width=5}\\n# reverse the y-axis\\nggplot(out_df, aes(x=dp_dz, y=depth)) +\\n    scale_y_reverse(limits = c(max(out_df$depth), 0), \\n                    breaks = seq(0, max(out_df$depth), 500)) +\\n    geom_line() +\\n    geom_point() + labs(title = TeX(\"Depth vs $\\\\\\\\frac{dp}{dz}$\"))\\n```\\n\\n## Results table\\nThere are 1001 ways of getting the same result in R. Here I am using one that is fast with help from the package `data.table`. It converts the vector-list `output` to a data table; pretty similar or equivalent to a dataframe.\\n\\n```{r}\\n# dataframe from row-vector\\nout_df\\n```\\n\\nThere it is. An algorithm to iterate through the production tubing to calculate fluid conditions at different depth points.\\n\\n# What\\'s Next\\n* Integrate this marching algorithm with real calculations of fluid properties at pressure and temperature at depth. Formation volume factors, viscosities, holdup, surface velocity, compressibility factor, etc. I will be using a package I wrote in R for the calculation of compressibility factor for gases, [zFactor](https://github.com/f0nzie/zFactor).\\n\\n* Add heat transfer effects to the fluid temperature as it moves up to the surface.\\n\\n* Add calculations for inclined wells.\\n\\n\\n# References\\n* 2006, Ovadia Shoham. Mechanistic Modeling of Gas Liquid Two-Phase flow in pipes.\\n* 1977, Kermit E. Brown and H. Dale Beggs. The Technology of Artificial Lift Methods\\n\\n\\n'},\n",
       " {'repo': 'drceph/petroleumgenerator',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': \"Petroleum Generator\\n===================\\n\\nversion 1.2.1\\n\\n### [DOWNLOAD THE LATEST VERSION](http://goo.gl/LzusY)\\nor [see all downloads](http://goo.gl/jya7H)\\n\\nA [buildcraft](http://www.mod-buildcraft.com)  <> [IC2](http://www.industrial-craft.net) crossover mod. \\nTasty on its own; Delicious served with a main course of [Forestry](http://forestry.sengir.net/wiki/).\\n\\nThis mod adds a Petroleum Generator to the game. This generator will produce EU directly from Buildcraft Fuel/Oil, which can be either pumped in using buildcraft liquid pipes, or manually deposited using containers. Want to burn some Biofuel in this? Tough. DrCeph Industries doesn't believe in the green revolution. It is just a fad that will pass, like hypercolor and scrunchies.\\n\\nAlso recently added was an IC2-based processing chain to turn raw materials from [TrainCraft](http://www.minecraftforum.net/topic/1587267-146sspsmplanforgetraincraft-formerly-trains-zeppelin-mod/) (Oily Sands and Crude Oil ore) into Buildcraft oil. This chain is only enabled if TrainCraft is installed.\\n\\nWhy another BC/IC conversion mod?\\n---------------------------------\\n\\n**Because Challenge maps!**\\n\\nThe focus of this mod is to make conversion of oil/fuel to EU a lossy process and to fix the scaling issues that come with using BC power directly.\\n\\nMost conversion mods out there use coal as a conversion point between BC and IC2. Whilst I understand this position, once you get to higher tier fuels you have a conversion ratio that gets out of control. For example, Fuel producing about 1.5 million EU using a combustion engine through a MJ to EU conversion block (last tested using MC1.4.2, BC3.1.8, IC1.108 and Transformers 1.6). [As SirSengir, of Forestry/Buildcraft points out](http://www.mod-buildcraft.com/forums/topic/inefficient-power-conversion/), using coal to build the ratio ignores the different scaling between the two mods' power systems.\\n\\nEnter Forestry: Forestry is great. It has an EU generator for Biofuels and a MJ engine that runs on EU. What it misses out however is EU generation using Buildcraft's oil resource. When combined with the Forestry mod this covers all aspects of the buildcraft power/fuel to EU conversions, with delicious inefficiencies. \\n\\nHow much EU will I get for oil and fuel?\\n----------------------------------------\\n\\nI've modelled this generator on the EU conversion rates on the Bio Generator in the Forestry mod. \\n\\nA bucket of oil will produce 30,000 EU at 10EU/t  \\nA bucket of fuel will produce 300,000 EU at 25EU/t  \\n\\nNote 1: These values were derived by looking at the MJ output of oil compared to biomass, and of fuel compared to biofuel. Then a boost due to oil being a non-renewable energy source.\\nNote 2: They have been tweaked slightly to make everything work nicely in integer space.  \\nNote 3: Notice that the power ratio between fuel and oil is 10:1, conserving the BC power differential.\\n\\nBy comparison, the Biogenerator currently produces (as of Forestry 1.6):  \\n\\n8,000 EU @ 8EU/t for Biomass  \\n32,000 EU @ 16EU/t for Biofuel  \\n\\n\\nCrafting Recipe\\n---------------\\n\\n### Petroleum Generator\\n\\nThe crafting recipe is as follows:\\n\\nx A x  \\nD B D  \\nD C D  \\n  \\nWhere:  \\nx = blank spot  \\nA = Generator  \\nB = Flint and Steel\\nC = Piston\\nD = Water Cell\\n\\nIn picture form:  \\n![xAx DBD DCD](https://raw.github.com/chrisduran/petroleumgenerator/6d5de2e67dcce4b41a191752f05ee6220276fa05/art/screenshots/crafting.png)\\n\\n### Traincraft oily sands and ore processing chain\\n\\nIf Traincraft is installed PetroGen will enable a processing chain to convert the Oil Sands (oily sands) and Crude Oil (oily ore) from Traincraft into Petroleum Generator compatible BC oil. The conversion rate is 10x sands or 5x ore, plus work, giving one bucket of oil.\\n\\nStep 1: Macerate ores into Bituminous Sludge (2x sands for 1 sludge, 1x ore for 1 sludge)   \\n![Ore to Sludge](https://raw.github.com/chrisduran/petroleumgenerator/523f1a6e3264c2066928331feabc5f3b1e43caab/art/screenshots/TC_ores_to_sludge.png)\\n\\nStep 2: Craft 5x Bituminous Sludge and 1x Empty Bucket to get 1x Bituminous Sludge Bucket (shapeless recipe, non-reversable)   \\n![Sludge to Sludge Bucket](https://raw.github.com/chrisduran/petroleumgenerator/523f1a6e3264c2066928331feabc5f3b1e43caab/art/screenshots/Sludge_to_sludge_bucket.png)\\n\\nStep 3: Extract 1x Bituminous Sludge Bucket into 1x Oil Bucket.   \\n![Sludge Bucket to Oil Bucket](https://raw.github.com/chrisduran/petroleumgenerator/523f1a6e3264c2066928331feabc5f3b1e43caab/art/screenshots/Sludge_bucket_to_oil.png)\\n\\nScreenshots\\n-----------\\n\\nScreenshots can be found in the github repository: [https://github.com/chrisduran/petroleumgenerator/tree/6d5de2e67dcce4b41a191752f05ee6220276fa05/art/screenshots](https://github.com/chrisduran/petroleumgenerator/tree/6d5de2e67dcce4b41a191752f05ee6220276fa05/art/screenshots)\\n\\nTODO\\n----\\n\\nThis isn't a completed product. Although the base functionality is there, DrCeph Industries always has more modules to pack in. A list of what is next in line:\\n\\n[See the isues list for the 'Version 1.0 Release' milestone](https://github.com/chrisduran/petroleumgenerator/issues?direction=asc&milestone=1&page=1&sort=created&state=open)\\n\\nRequirements\\n------------\\n\\nRequires IndustrialCraft2 v1.109 or greater and BuildCraft 3.2.0 or greater. Which means it also requires the recommended Forge version for those addons and Minecraft 1.4.5.\\n\\nInstallation\\n------------\\n\\nFollow the installation instructions for Forge, IndustrialCraft2 and BuildCraft. Once these are installed successfully, place the PetroGen mod file in the same 'mods' directory. Shake then stir.\\n\\nIf you prefer @mjramonru's texture, replace the blocks.png file in drceph/petrogen/sprites with the blocksAlt.png file (i.e. rename blocksAlt.png to blocks.png). **Yell at me if you want it to be the default texture. It is a pretty cool texture, check it out.** \\n\\nLicense\\n-------\\n\\nhttps://github.com/chrisduran/petroleumgenerator/blob/master/LICENCE\\n\\nThis program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\\n\\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\\n\\nYou should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n\\n@mjramonru retains all rights and ownership of his texture file, blocksAlt.png - I am simply hosting the file in the modpack for the convenience of end users.\\n\\nChangelog\\n---------\\n\\n1.2.1:\\n* Fixed bug where naturally generated TC ore blocks would not macerate due to non-standard ore generation\\n\\n1.2:\\n* Block front panel now faces player on placement\\n* Cross-mod compatability - now able to convert TrainCraft ores into Buildcraft oil.\\n\\n1.1:\\n* Updated to MC 1.4.6, IC v1.112 and BuildCraft 3.3.0\\n\\n1.0:\\n* Full release! Updated to IC v1.110 and BuildCraft 3.2.2. \\n\\n0.14:\\n* Updated default EU/bucket for fuel (again) to 300k. This is after some discussions on the IC2 and FTB forums with people who are better at balance than I.\\n* Also increased the output to 25 EU/t for fuel so that you don't need to shave after every bucket!\\n* Block now updates texture on activity - you can now tell at a glance that you're burning precious fuel!\\n* Also made the recipe in line with the costs of the Geothermal generator, and TE magma engines. Lava is fuel's closest 'competitor', so lessening the gap makes fuel less unattractive.\\n\\n0.10:\\n* Updated default EU/bucket for oil and fuel to 30k and 200k respectively. Due to relative scarcity of resource compared to other sources.\\n* Removed electronic circuit from recipe after some analysis of the relative costs of geothermal and biogenerators.\\n* Powering the generator by redstone will block emitting of EU packets. Tank and buffer will still fill, however.\\n* Updated alternative texture by @mjramonru.\\n\\n\\n0.7:\\n* Added configurable options. Fuel/oil now configurable in steps of 20k/10k EU respectively. Block ID also configurable (default is 3143).\\n* Added alternative texture, graciously created by @mjramonru. \\n\\n0.5:\\n* Added support for stackable containers of fuel and oil, notably Forestry capsules and cans.\\n\\n0.4:\\n* Initial release.\\n\\nCredits\\n-------\\n\\n@mjramonru for the awesome texture.\\n\\nResources\\n---------\\nIn the interest of full disclosure, here are other places this mod has a presence:\\n\\nIndustrialCraft2 forums (required dependancy) [http://forum.industrial-craft.net/index.php?page=Thread&threadID=8385](http://forum.industrial-craft.net/index.php?page=Thread&threadID=8385)  \\nFTB (Modpack) [http://forum.feed-the-beast.com/threads/…sover-mod.2246/](http://forum.feed-the-beast.com/threads/…sover-mod.2246/)  \\n\"},\n",
       " {'repo': 'fluidgeo/fluidgeo-simulator',\n",
       "  'language': 'FORTRAN',\n",
       "  'readme_contents': \"# fluidgeo-simulator: what is it?\\nFEM simulator of fluid-geomechanics coupling in unconventional petroleum reservoirs.\\n## Purpose\\nThe ideia is a construction of unconventional reservoir simulator under the finite element method.\\nThe coding structure is based on The Finite Element Method: Linear Static and Dynamic Finite Element Analysis (Thomas Hughes).\\nThe current code uses modular paradigm of programming by means of Fortran 90. It's important to say the code has a general purpose too. So the programmer can adapt the problem changing the computations of local stiffness matrix and local load vector to another problem.\\n## Uses\\nThe use is free, but please pay attention to Apache License before usage. And, of course, credit the authors when using our code, that\\nhas been developed at LNCC/MCTI (National Laboratory of Scientific Computing, Petrópolis/Brazil).\\n\\n## Contacts\\nIf you want to contact us, you're welcome to do by here (GitHub) or by our e-mails:\\n\\nEduardo Garcia: bidu@lncc.br\\nDiego Volpatto: volpatto@lncc.br\\n\"},\n",
       " {'repo': 'petrocode/blackoilmsv',\n",
       "  'language': 'Visual Basic',\n",
       "  'readme_contents': 'BLack Oil Petroleum Reservoir Simulator written by https://github.com/petronerd\\n'},\n",
       " {'repo': 'vehagn/tpg4155',\n",
       "  'language': 'Matlab',\n",
       "  'readme_contents': '# TPG4155\\nDemonstrations shown during my lectures in TPG4155 - Applied Computer Methods in Petroleum Science at NTNU.\\n\\nFeel free to use the examples under CC BY-SA 2.0\\n'},\n",
       " {'repo': 'aegis4048/Petroleum_Engineering',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'vishal-anand-1/Reservoir-recovery-prediction',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'ivarref/bp-diagrams',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': 'BP-diagrams\\n===========\\n\\nGoal\\n----\\nVisualize BP (British Petroleum) Statistical Review of the World 2015.\\nChoose between resource usage per capita, \\nrelative share of energy consumption and total energy consumption.\\n\\n[View result](http://ivarref.github.io/bp-diagrams/).\\n\\nAbout the data\\n--------------\\nBP defines the `other renewables` numbers as the following: `Based on gross generation from renewable sources including wind, geothermal, solar, biomass and waste, and not accounting for cross-border electricity supply.`\\n\\nThe data is available as a TSV (Tab Separated Values) file [here](https://github.com/ivarref/bp-diagrams/blob/master/data/data.tsv). This file contains BP energy data mapped to World Bank population data.\\n\\nThe data is generated based on the [Mazama Science BP data set](http://mazamascience.com/OilExport/data.html) and the [okfn World Bank population data set](http://data.okfn.org/data/core/population).\\nMapping between the World Bank countries and BP groups is specified in [gen_data.py](https://github.com/ivarref/bp-diagrams/blob/master/gen_data.py).\\n\\nNotes\\n--------\\n* Coal production data available since 1980.\\n* Gas production data available since 1970.\\n\\nFeatures\\n--------\\n\\n* Compare countries or group of countries.\\n* Toggle resources (coal, oil, gas, nuclear, hydro and other renewables).\\n* Permalinks (normal and printer friendly).\\n* Show actual numbers option.\\n* Choose year to show actual numbers.\\n* Show production data.\\n* Net import/export show.\\n\\nPlans for new features\\n----------------------\\n\\n* GDP/PPP as alternative left y-axis.\\n\\nTODOs\\n----------------------\\n\\n* Clean up data filtering.\\n* Remove config variable?\\n\\nAcknowledgements\\n----------------\\n[Mazama Science](http://mazamascience.com/OilExport/) for BP data set.\\n\\n[okfn](http://data.okfn.org/data/core/population) for World Bank data set.\\n\\n[Rune Likvern / Fractional Flow](http://fractionalflow.com/) for inspiration as well as feedback.\\n\\nUpdating the data\\n-----------------\\n\\nThis is a yearly task.\\n\\n### 1. Update EnergyExportDatabrowser\\nThis will update the `BP_*` files in `data/`.\\n\\n### 2. Update the rest of data\\n\\n`$ ./make.sh`\\n\\nComments\\n--------\\nrefsdal.ivar@gmail.com\\n\\n'},\n",
       " {'repo': 'abhishekdbihani/synthetic_well-log_polynomial_regression',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': '## Constructing a Synthetic NMR Well-log using Machine Learning\\n\\n\\n### By Abhishek Bihani\\n\\n### Final Project for PGE 383 – Subsurface Machine Learning taught by Dr. Michael Pyrcz (Fall - 2019)\\n\\n### Hildebrand Department of Petroleum and Geosystems Engineering\\n\\n### The University of Texas at Austin\\n\\n****\\n\\n\\n**Executive Summary:** \\n\\nThe nuclear magnetic resonance (NMR) log is a useful tool to understand lithological information such as the variation of pore size distribution with depth, but it may not be measured in all wells. The project attempts to predict a missing well log from other available well logs using machine learning tools, more specifically an NMR well log from the measured Gamma Ray (GR) log, Caliper log, Resistivity log, and the interpreted porosity from one well at the Keathley Canyon in the Gulf of Mexico. The constructed model is then used to predict the NMR log at Walker Ridge in Gulf of Mexico, which is another nearby site of methane hydrate accumulation. \\n\\nIn Keathley Canyon Block 151 (KC-151), the analyzed well was drilled and logged during Leg I of the U.S. Department of Energy/Chevron Gas Hydrate Joint Industry Project (JIP) (Ruppel et al., 2008). At Walker Ridge 313 (WR-313), the analyzed well was drilled and logged during JIP Leg II (Collett et al., 2012). The raw well logs for KC-151 are available [here](http://mlp.ldeo.columbia.edu/data/ghp/JIP1/KC151-2/index.html?) and for WR-313 are available [here](http://mlp.ldeo.columbia.edu/data/ghp/JIP2/WR313-H/). The processed well logs used in this project for KC-151 are available [here](https://github.com/abhishekdbihani/synthetic_well-log_polynomial_regression/blob/master/KC151_logs.csv) and for WR-313 are available [here](https://github.com/abhishekdbihani/synthetic_well-log_polynomial_regression/blob/master/WR313H_logs.csv).\\n\\n**Approach:**\\n\\n1) For an easier characterization of the NMR data, the NMR log, i.e. relaxation time distribution was converted into Mean of T2 (MLT2) and Standard Deviation of T2 (SDT2) which are considered as the two response features to be predicted. The other well logs: GR, Caliper, Resistivity, and the interpreted porosity are the predictor features used to train the model.\\n\\n2) An initial analysis is conducted on the well logs to check the univariate and bivariate distributions of the data, and the well-logs are plotted with depth. \\n\\n3) Then a linear regression is conducted for both MLT2 and SDT2 using the other predictor variables to observe the behavior with a basic model. It is seen that the linear regression could not capture the response behavior well due to noise, i.e. short-distance variations as well as non-linearities in the data relationships. \\n\\n4) This is followed by feature standardization before applying more complex models to reduce effect of outliers and predictor features having different units. Feature ranking was conducted to compare the order in which predictor variables affect the response variables.\\n\\n5) Then, the logs are processed to reduce noise, and after a train-test split, polynomial regression modeling is conducted to predict the NMR log at Keathley Canyon until a good fit is obtained.\\n\\n6) Finally, the trained model is used to predict the NMR log at Walker Ridge where it was not recorded.\\n\\n*Note: The codes and procedures used for this project have been adapted from the workflows followed by Dr. Pyrcz in the class (Pyrcz, 2019 a, b, c, d) and my Master\\'s thesis supervised by Dr. Daigle (Bihani, 2016).*\\n\\n<img src=\"https://github.com/abhishekdbihani/synthetic_well-log_polynomial_regression/blob/master/KC151-logs.png\" align=\"middle\" width=\"800\" height=\"600\" alt=\"Well-logs at KC-151\" >\\n\\n                                    Figure- Well logs from Keathley Canyon 151\\n\\n\\n**Assumptions:**\\n\\n1) The conditions at both KC-151 and WR-313 locations are assumed to be similar enough so the same model can be applied.\\n\\n2) The model is assumed to be sufficiently trained to make predictions but can be improved if more training data is available.\\n\\n3) The porosity has been calculated from the bulk density log since porosity is a function of the grain density of the formation (2.65 gm/cm3 in sands, 2.70 gm/cm3 in clays; Daigle et al., 2015) and of the pore-filled fluid (assumed to be water, with a density of 1.03 gm/cm3; Daigle et al., 2015).\\n\\n4) During polynomial regression, it was assumed that all the relationships between predictors and response features could be captured by basis expansion until the 3rd power.\\n\\n**Citation:**\\n \\n If you find this repository useful, please cite as-\\n \\n Bihani A. Constructing a Synthetic NMR Well-log using Machine Learning. Git code (2019)  https://github.com/abhishekdbihani/synthetic_well-log_polynomial_regression.\\n \\n**Related publications:**\\n\\nBihani A., Pore Size Distribution and Methane Equilibrium Conditions at Walker Ridge Block 313, Northern Gulf of Mexico, M.S. thesis, University of Texas, Austin, Texas, 2016. doi:10.15781/T2542J80Z\\n\\nBihani A., Daigle H., Cook A., Glosser D., Shushtarian A. (2015). OS23B-1999: Pore Size Distribution and Methane Equilibrium Conditions at Walker Ridge Block 313, Northern Gulf of Mexico. AGU Fall Meeting, 14-18 December, San Francisco, USA. \\n\\n**References:**\\n\\nCollett, T. S., Lee, M. W., Zyrianova, M. V., Mrozewski, S. a., Guerin, G., Cook, A. E., and Goldberg, D. S. (2012). Gulf of Mexico Gas Hydrate Joint Industry Project Leg II logging- while-drilling data acquisition and analysis. Marine and Petroleum Geology, 34(1),41-61, doi:10.1016/j.marpetgeo.2011.08.003\\n\\nDaigle, H., Cook, A., and Malinverno, A. (2015). Permeability and porosity of hydrate- bearing sediments in the northern Gulf of Mexico. Marine and Petroleum \\tGeology, 68, \\t551–564, doi:10.1016/j.marpetgeo.2015.10.004\\n\\nPyrcz M., (2019a) Feature Selection for Subsurface Data Analytics in Python. Retrieved December 5, 2019, from https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Feature_Ranking.ipynb\\n\\nPyrcz M., (2019b) Principal Component Analysis for Subsurface Data Analytics in Python. Retrieved December 5, 2019, from\\nhttps://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_PCA.ipynb\\n\\nPyrcz M., (2019c) Time Series Analysis for Subsurface Modeling in Python. Retrieved December 5, 2019, from\\nhttps://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_TimeSeries.ipynb\\n\\nPyrcz M., (2019d) Polygonal Regression for Subsurface Data Analytics in Python. Retrieved December 5, 2019, from\\nhttps://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_PolygonalRegression.ipynb\\n\\nRuppel, C., Boswell, R., and Jones, E. (2008). Scientific results from Gulf of Mexico Gas Hydrates Joint Industry Project Leg 1 drilling: Introduction and overview. Marine and Petroleum Geology, 25(9), 819–829. doi:10.1016/j.marpetgeo.2008.02.007\\n\\n*****\\n\\n\\n\\n\\n\\n\\n\\n\\n'},\n",
       " {'repo': 'RaminMoghadasi/compsim',\n",
       "  'language': 'C',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'SumedhaSingh/Petroleum-Price-Prediction-Model',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'pedrolinhares/Turtle-Flow-Simulator',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '+=== TURTLE FLOW SIMULATOR (TFS) ===+\\nDevelopers: Pedro Henrique Linhares (pedrolmota@gmail.com)\\n            Wagner Queiroz (wagnerqb@gmail.com)\\nInstitution:\\n            Laboratório de Desenvolvimento de Software Científico e Aplicado (LDSC)\\n            Laboratório de Engenharia, Exploração e Produção de Petróleo (LENEP)\\n            Universidade Estadual do Norte Fluminense Darcy Ribeiro (UENF)\\n\\nLicense: GPLv3 or later\\nLicense URI: http://www.gnu.org/licenses/gpl-3.0.html\\nCopyright (C) 2013-2014\\n\\n== Description ==\\n\\nThe TFS software is a reservoir simulator that is able to simulate the behaviour of an one-dimensional\\nsingle phase petroleum reservoir under production conditions.\\n\\nThe software presents a graphical interface where the user can configure the simulation parameters.\\nAfter that, the user can generate a set of configuration files needed by the simulator in order\\nto initiate the simulation. Finnished the simulation, the interface can present the results\\nthrough a set of graphics so the user can better understand the simulation.\\n\\n== Dependencies ==\\nIn order to build the TFS software, the following software are required:\\n\\n* Qt 5.0 or higher with development libraries (specially the qmake tool)\\n* C++11\\n* Gnuplot 4.4 or higher (http://www.gnuplot.info)\\n\\n== Building ==\\nYou can build the TFS executable either by:\\n\\n1. running the script build_all.sh:\\n    $ sh ./build_all.sh\\n\\n2. running make as follows:\\n    $ make -f Makefile\\n    $ cd ./src/kernel\\n    $ make -f Makefile\\n\\nThe binaries are going to be generated in the bin/ folder at the root project.\\n\\n== Running ==\\nTo run the software you should execute it from the bin/ folder, from the project root directory do as follows:\\n    $ cd ./bin/\\n    $ ./tfs\\n\\n== Examples ==\\nThe tutorial folder contains two examples of the TFS software:\\n\\n1. One-dimensional horizontal closed homogeneous reservoir.\\n2. Anticline two litology reservoir with specified pressure.\\n\\n== Changelog ==\\n\\n= 1.0 =\\n* Basic simulation of one-dimensional single phase petroleum reservoir implemented.\\n* Merged the interface and kernel.\\n* Created the kernel module.\\n* Created software interface.\\n'},\n",
       " {'repo': 'ovalles/PyPetroleum',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"=======\\n\\n######\\nPyPetroleum\\n######\\n\\n*It's a game about hunting for oil, oil exploration*\\n\\n=======\\n\\n\\n**Main Features**\\n\\n* This toy is based in a friendly Pygame interface\\n* Written in Python 2.7\\n* It's still under construction\\n* \\n* \\n.. contents:: **Table of Contents**\\n    :local:\\n    :backlinks: none\\n\\n============\\nInstallation\\n============\\n\\n-----\\nLinux\\n-----\\n\\n    ::\\n\\n    $ git clone https://github.com/ovalles/PyPetroleum.git   \\n\\n\\n*************\\nPrerequisites\\n*************\\n\\nPython 2.7 and module Pygame\\n\\n\\n*******\\nInstall\\n*******\\n\\nDownload the latest version from `GitHub`_. If you have ``git`` installed, you can use the following command:\\n\\n.. _GitHub: https://github.com/ovalles\\n\\n::\\n\\n$ git clone https://github.com/ovalles/PyPetroleum.git\\n\\nFinally, enter the newly created directory containing the source code and run:\\n\\n::\\n\\n$ python cheapoil.py\\n\\n\\n===========\\nVideos and screenshots\\n===========\\n\\n.. image:: https://github.com/ovalles/PyPetroleum/blob/master/80_SCREENSHOTS/03_Pypetrol.png\\n.. image:: https://github.com/ovalles/PyPetroleum/blob/master/80_SCREENSHOTS/05_Pypetrol.png\\n.. image:: https://github.com/ovalles/PyPetroleum/blob/master/80_SCREENSHOTS/10_Pypetrol.png\\n=======\\nLicense\\n=======\\n\\nLicensed under the `GPLv2`_ license.\\n\\n.. _GPLv2: http://www.gnu.org/licenses/gpl-2.0.html\\n\\n=======\\nAuthors\\n=======\\n\\n`Asdrubal Ovalles`_  \\n\\n.. _Asdrubal Ovalles: https://www.linkedin.com/in/asdr%C3%BAbal-ovalles-8401a352\\n\\nMade in Venezuela, 2015\\n\\n=======\\n\\n==========\\nReferences\\n==========\\n\\n.. [1] -\\n=======\\n\\n\\n\\n\"},\n",
       " {'repo': 'duwalanise/reactriot2017-fillmeup',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# FILL ME UP\\n\\nFill Me Up is a simply to use, user-friendly bridge between petroleum suppliers and the average motorist consumer. It is a platform that allows suppliers to engage their consumers and provide meaningful information that will help their consumers make informed decisions with regards to re-fueling their vechicles.\\n\\n## Prerequisites\\n\\n- MacOS\\n- Homebrew, install from https://brew.sh/\\n- Node.js, install with brew install node\\n\\n## Getting Started/Installing\\n\\nEnsure you have the minimum requirements specified in Prerequisites. To set up a development environment, run the following instructions.\\n\\n- ```npm install``` installs all npm package dependencies\\n- ```npm install -g http-server``` installs command-line http-server\\n\\n\\nDo these commands on separate terminals:\\n- ```npm run dev``` bundles code base through webpack\\n- ```http-server``` creates server with default port 8080, additional command-line options can be found at https://github.com/indexzero/http-server\\n\\n## Running the tests\\n\\nnpm run test\\n\\n## Deployment\\n\\nnpm run build\\n\\n## Tools & Technologies\\n\\n- Firebase\\n- React\\n- React-redux\\n- React-router\\n- Webpack\\n- Babel\\n- Eslint\\n- SCSS \\n\\n## Authors\\n\\n* **[Raman Amatya](https://github.com/ramanamatya)**\\n* **[Rajat Amatya](https://github.com/rajatamatya)**\\n* **[Anish Duwal](https://github.com/duwalanise)**\\n* **[Dhanit Paija](https://github.com/DhanitP)**\\n\\n\\n## License\\n\\nThis project is licensed under the MIT License\\n'},\n",
       " {'repo': 'danielbarr3ra/Petroleum_Enginering_Codes',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'ishita159/petroleum_review_system',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# petroleum_review_system'},\n",
       " {'repo': 'manjunath5496/Petroleum-Engineering-Books',\n",
       "  'language': None,\n",
       "  'readme_contents': '\\n<ul>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(1).pdf\" style=\"text-decoration:none;\">Petroleum Engineering Handbook, Volume VII: Indexes and Standards  </a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(2).pdf\" style=\"text-decoration:none;\">5000 Oilfield Terms: A Glossary of Petroleum Engineering Terms, Abbreviations and Acronyms</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(3).pdf\" style=\"text-decoration:none;\">Fluid Mechanics for Petroleum Engineers</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(4).pdf\" style=\"text-decoration:none;\">Petroleum Engineer\\'s Guide to Oil Field Chemicals and Fluids</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(5).pdf\" style=\"text-decoration:none;\">Rules of Thumb for Petroleum Engineers</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(6).pdf\" style=\"text-decoration:none;\">Fundamentals of Petroleum and Petrochemical Engineering</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(7).pdf\" style=\"text-decoration:none;\">Applied Petroleum Reservoir Engineering</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(8).pdf\" style=\"text-decoration:none;\">Advances in Petroleum Engineering and Petroleum Geochemistry: Proceedings of the 1st Springer Conference of the Arabian Journal of Geosciences (CAJG-1), Tunisia 2018</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(9).pdf\" style=\"text-decoration:none;\">Standard Handbook of Petroleum and Natural Gas Engineering</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(10).pdf\" style=\"text-decoration:none;\">Introduction to Petroleum Engineering</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(11).pdf\" style=\"text-decoration:none;\">Geophysics for Petroleum Engineers</a></b></li>\\n     <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(12).pdf\" style=\"text-decoration:none;\">Applying Nanotechnology to the Desulfurization Process in Petroleum Engineering</a></b></li> \\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(13).pdf\" style=\"text-decoration:none;\">Petroleum Geoscience</a></b></li>\\n  \\n<li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(14).pdf\" style=\"text-decoration:none;\">Physics of Petroleum Reservoirs</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(15).pdf\" style=\"text-decoration:none;\">Oil Politics: A Modern History of Petroleum</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(16).pdf\" style=\"text-decoration:none;\">Handbook of Petroleum Analysis</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(17).pdf\" style=\"text-decoration:none;\">Artificial Intelligent Approaches in Petroleum Geosciences</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(18).pdf\" style=\"text-decoration:none;\">Petroleum Engineering: Principles and Practice</a></b></li>  \\n  \\n<li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(19).pdf\" style=\"text-decoration:none;\">Basic Petroleum Geology</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(20).pdf\" style=\"text-decoration:none;\">Petroleum Fuels Manufacturing Handbook: including Specialty Products and Sustainable Manufacturing Techniques</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(21).pdf\" style=\"text-decoration:none;\">The Chemistry and Technology of Petroleum</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(22).pdf\" style=\"text-decoration:none;\">The Oil Curse: How Petroleum Wealth Shapes the Development of Nations</a></b></li>  \\n  \\n<li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(23).pdf\" style=\"text-decoration:none;\">Petroleum Refinery Process Economics</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(24).pdf\" style=\"text-decoration:none;\">Petroleum Reservoir Engineering Practice</a></b></li>  \\n  \\n <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(25).pdf\" style=\"text-decoration:none;\">Environmental Control in Petroleum Engineering</a></b></li>  \\n  \\n<li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(26).pdf\" style=\"text-decoration:none;\">Ludwig\\'s Applied Process Design for Chemical and Petrochemical Plants</a></b></li>\\n                <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(27).pdf\" style=\"text-decoration:none;\">The Fundamentals of Corrosion and Scaling for Petroleum and Environmental Engineers</a></b></li>  \\n      \\n  <li><b><a target=\"_blank\" href=\"https://github.com/manjunath5496/Petroleum-Engineering-Books/blob/master/pet(28).pdf\" style=\"text-decoration:none;\">Thermodynamics: Applications in Chemical Engineering and the Petroleum Industry</a></b></li>   \\n  \\n  \\n  \\n                \\n</ul>\\n'},\n",
       " {'repo': 'm2b/API11_1VCF',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# API11_1VCF\\nAmerica Petroleum Institute (API) Volume Correction Factor calculations.\\nImplementation of API Measurement standards computations.  Refer to API 11 - Section 1.\\n'},\n",
       " {'repo': 'einar90/tpg4162', 'language': 'C', 'readme_contents': 'No README'},\n",
       " {'repo': 'zentechnologygroup/libzen',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': \"# libzen\\n---\\n\\nlibzen is a C++ library for petroleum-domain units management and conversion.\\n\\n## Features\\n\\n* Runtime entirely written in C++14.\\n* Tested on gcc, clang and Intel (icc), compilable on any platform that supports any of these.\\n* The library is compilable for Windows, OSX, Linux, and Solaris operating systems.\\n* Fully reentrant code, which makes it multithreaded.\\n* Chainable to any system.\\n\\n#### Physical magnitudes supported by the converter\\n\\n|      Physical Magnitude     |    Unit symbol              |    LaTeX symbol                   |\\n| --------------------------  | --------------------------- | --------------------------------- |\\n|        Absolute Roughness   |     ft_ar                   |    ft_ar                          |           \\n|        Absolute Roughness   |     in_ar                   |    in_ar                          |\\n|        Absolute Roughness   |     mm_ar                   |    mm_ar                          |\\n|                    Angle    |     deg                     |    \\\\degree                        |\\n|                    Angle    |     rad                     |    rad                            |\\n|                     Area    |     ft2                     |    ft2                            |\\n|                     Area    |     in2                     |    in2                            |\\n|    Compressibility Factor   |     zFactor                 |    Undefined                      |\\n|                  Current    |     amp                     |    amp                            |\\n|                  Density    |     gr/cm3                  |    g/cm^{3}                       |\\n|                  Density    |     kg/m3                   |    kg/m^{3}                       |\\n|                  Density    |     kg/L                    |    kg/L                           |\\n|                  Density    |     lb/ft3                  |    lb/ft^{3}                      |\\n|                  Density    |     lb/gal                  |    lb/gal                         |\\n|                  Density    |     lb/inch3                |    lb/in^{3}                      |\\n|                  Density    |     Sg                      |    sg water=1\\\\ at\\\\ 60\\\\ \\\\degree{F} |\\n|                DummyUnit    |     TU                      |    TU                             |\\n|         Dynamic Viscosity   |     kg/m*s                  |    kg/m \\\\cdot s                   |\\n|         Dynamic Viscosity   |     lb/ft*s                 |    lb/ft \\\\cdot s                  |\\n|         Dynamic Viscosity   |     g/cm*s                  |    g/cm \\\\cdot s                   |\\n|         Dynamic Viscosity   |     cP                      |    cP                             |\\n|         Dynamic Viscosity   |     Pa*s                    |    Pa \\\\cdot s                     |\\n|         Dynamic Viscosity   |     lb/ft*h                 |    lb/ft \\\\cdot h                  |\\n|         Dynamic Viscosity   |     poise                   |    P                              |\\n|         Dynamic Viscosity   |     mP                      |    mP                             |\\n|           FVF volume Ratio  |     RB/STB                  |    RB/STB                         |\\n|           FVF volume Ratio  |     Rm3/Sm3                 |    Rm^{3}/Sm^{3}                  |\\n|           Flow Efficiency   |     FlowEff                 |    Dmnl                           |\\n|                Flow Rate    |     bpd                     |    bpd                            |\\n|                 Flow Rate   |     gpm                     |    gpm                            |\\n|                 Flow Rate   |     cmd                     |    cmd                            |\\n|                 Flow Rate   |     cms                     |    cms                            |\\n|                Frequency    |     Hz                      |    Hz                             |\\n|                Frequency    |     RPM                     |    RPM                            |\\n|           Friction Factor   |     fFactor                 |    Non-dimensional factor         |\\n|        GORGLR volume Ratio  |     scf/STB                 |    scf/STB                        |\\n|        GORGLR volume Ratio  |     Mscf/STB                |    Mscf/STB                       |\\n|        GORGLR volume Ratio  |     MMscf/STB               |    MMscf/STB                      |\\n|        GORGLR volume Ratio  |     Sm3/Sm3                 |    Sm^{3}/Sm^{3}                  |\\n|                   Gas FVF   |     rcf/scf                 |    rcf/scf                        |\\n|                   Gas FVF   |     rm3/sm3                 |    Rm^{3}/Sm^{3}                  |\\n|                   Gas FVF   |     rcf/Mscf                |    rcf/Mscf                       |\\n|                   Gas FVF   |     RB/scf                  |    RB/scf                         |\\n|                   Gas FVF   |     RB/Mscf                 |    RB/Mscf                        |\\n|              Gas Flow Rate  |     MMscf/d                 |    MMscf/d                        |\\n|       Gas Specific Gravity  |     Sgg                     |    air=1                          |\\n|       Gas Specific Gravity  |     kg/m3atStandCond        |    kg/m^{3}\\\\ at\\\\ STP              |\\n|       Gas Specific Gravity  |     lb/ft3atStandCond       |    lb/ft^{3}\\\\ at\\\\ STP             |\\n|       Geothermal Gradient   |     degF/Ft                 |    \\\\degree{F}/ft                  |\\n|       Geothermal Gradient   |     degC/Km                 |    \\\\degree{C}/km                  |\\n|                     Head    |     Hft                     |    Hft                            |\\n|                     Head    |     Hmts                    |    Hmts                           |\\n|  Heat Transfer Coefficient  |     Btu (th)/d/ft2/degF     |    Btu (th)/d/ft^{2}/^{\\\\circ}F    |\\n|       Interfacial Tension   |     dynes/cm                |    dyne/cm                        |\\n|       Interfacial Tension   |     N/m                     |    N/m                            |\\n|       Interfacial Tension   |     mN/m                    |    mN/m                           |\\n|       Interfacial Tension   |     gram-force/cm           |    gf/cm                          |\\n|       Interfacial Tension   |     pound-force/inch        |    lbf/in                         |\\n|       Interfacial Tension   |     pound-mass/s^2          |    lbm/s^2                        |\\n|Isothermal Compressibility   |     Pa^-1                   |    Pa^{-1}                        |\\n|Isothermal Compressibility   |     MPa^-1                  |    MPa^{-1}                       |\\n|Isothermal Compressibility   |     psia^-1                 |    psia^{-1}                      |\\n|Isothermal Compressibility   |     bar^-1                  |    bar^{-1}                       |\\n|Isothermal Compressibility   |     atm^-1                  |    atm^{-1}                       |\\n|            Liquid Gravity   |     g                       |    g                              |\\n|  Non Hydrocarbons Fraction  |     MoleFraction            |    mol\\\\ fraction                  |\\n|  Non Hydrocarbons Fraction  |     VolumeFraction          |    vol\\\\ fraction                  |\\n|  Non Hydrocarbons Fraction  |     MolePercent             |    mol\\\\%                          |\\n|  Non Hydrocarbons Fraction  |     VolumePercent           |    vol\\\\%                          |\\n|        OGRLGR volume Ratio  |     STB/scf                 |    STB/scf                        |\\n|        OGRLGR volume Ratio  |     STB/Mscf                |    STB/Mscf                       |\\n|        OGRLGR volume Ratio  |     STB/MMscf               |    STB/MMscf                      |\\n|        OGRLGR volume Ratio  |     Sm3 Liquid/Sm3          |    Sm^{3}/Sm^{3}                  |\\n|               Oil Gravity   |     api                     |    \\\\degree{API}                   |\\n|               Oil Gravity   |     sg_do water=1\\\\ at\\\\ 60\\\\  |    \\\\degree{F}                     |\\n|                 Pressure    |     bar                     |    bar                            |\\n|                 Pressure    |     Pa                      |    Pa                             |\\n|                 Pressure    |     kPa                     |    kPa                            |\\n|                 Pressure    |     MPa                     |    MPa                            |\\n|                 Pressure    |     psia                    |    psia                           |\\n|                 Pressure    |     psig                    |    psig                           |\\n|                 Pressure    |     atm                     |    atm                            |\\n|         Pressure Gradient   |     psi/ft                  |    psi/ft                         |\\n|         Pressure Gradient   |     kPa/m                   |    kPa/m                          |\\n|            Pressure Ratio   |     Ppr                     |    Dmnl                           |\\n|        Relative Roughness   |     Non-dimensional         |    roughness                      |\\n|             Specific Heat   |     kilojoule/kilogram/K    |    kilojoule/kilogram/K           |\\n|             Specific Heat   |     Btu (th)/pound/°F       |    Btu (th)/pound/degF            |\\n|              Temperature    |     degK                    |    K                              |\\n|              Temperature    |     degC                    |    \\\\degree{C}                     |\\n|              Temperature    |     degF                    |    \\\\degree{F}                     |\\n|              Temperature    |     degR                    |    \\\\degree{R}                     |\\n|         Temperature Ratio   |     Tpr                     |    Dmnl                           |\\n|      Thermal Conductivity   |     watt/mt/K               |    watt/mt/K                      |\\n|      Thermal Conductivity   |     Btu (th)/hour/foot/degF |    Btu (th)/hour/foot/degF        |\\n|      Thermal Conductivity   |     Btu (th)/day/foot/degF  |    Btu (th)/day/foot/degF         |\\n|                 Water Cut   |     wc_p                    |    Undefined                      |\\n|                 Water Cut   |     wc_f                    |    Undefined                      |\\n|     Water Specific Gravity  |     pwl_lb/ft3              |    lb/ft^{3}                      |\\n|     Water Specific Gravity  |     dissolvedSaltPercent    |    wt\\\\%                           |\\n|     Water Specific Gravity  |     swsg water=1\\\\ at\\\\ 60\\\\   |    \\\\degree{F}                     |\\n|     Water Specific Gravity  |     dissolvedSaltPPM        |    wt\\\\ ppm                        |\\n|     Water Specific Gravity  |     mol_NaCl/Kg_H2O         |    mol\\\\ NaCl/kg\\\\ H_2O             |\\n|     Water Specific Gravity  |     g_NaCl/L                |    g\\\\ NaCl/L\\\\ H_2O                |\\n|     Water Specific Gravity  |     dissolvedSaltFraction   |    Mass\\\\ fraction\\\\ NaCl           |\\n|                Wet Gas FVF  |     RCF Gas/STB             |    rcf/STB                        |\\n|                Wet Gas FVF  |     RB Gas/STB              |    RB/STB                         |           \\n\\n## Dependencies \\n\\n\\n### cmake\\n\\nFor a Debian-based distribution, it can be installed by executing:\\n\\n```\\n$ sudo apt-get install cmake\\n```\\n\\nFor other distributions, or if Debian's repositories don't include cmake versión 3.7+, download the [sources](https://cmake.org/download/) and build it by doing:\\n\\n```\\n$ ./bootstrap\\n$ make\\n$ sudo make install\\n```\\n\\n### Submodules\\n\\nIf you clone the porject, fetch submodules in order to be able to build it:\\n\\n```\\n$ git submodule init\\n$ git submodule update\\n```\\n\\n## Build\\n\\nTo build the project, do:\\n\\n```\\n$ mkdir build && cd build\\n$ cmake ..\\n$ make\\n```\\n\\n## Tests\\n\\nAfter compiling, the test executables are generated in the directory `bin`. To run all tests, do:\\n\\n```\\n$ ./bin/ztgunitmanagertest\\n```\\n\\nTo list the physical quantities implemented in the unit converter, use the following command:\\n\\n\\n```\\n$ ./bin/ztgunitmanagertest -P\\n```\\n\\nTo list all options available for testing, do:\\n\\n```\\n$ ./bin/ztgunitmanagertest --help\\n```\\n\\n## Authors\\n\\n####  Software Architect\\n* PhD. Leandro Rabindranath Leon - (leandro.r.leon@gmail.com)\\n\\n#### Developer\\n* Álvaro Araujo -  (alvaro.araujo@zentech.group)\\n\\n#### Developer\\n* Jhonathan Abreu -  (jhonathan.abreu@zentech.group)\\n\\n#### Domain Driven Analyst and  Designer\\n* MSc. Alberto Valderrama - (alberto.valderrama@zentech.group)\\n\\n#### Developer and Tester\\n* Ixhel Mejías -  (ixhel.mejias@zentech.group)\\n\\n\\n\\n## License\\n\\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE) file for details\\n\"},\n",
       " {'repo': 'kmcken/PetroThermo',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# PetroThermo\\nThermodynamics models for CSM PEGN511: Advanced Thermodynamics and Petroleum Fluids Phase Behavior\\n\\nFor Homework #2, the main file generates the figures for DeltaH and DeltaS.\\n\\nIf you want to evaluate the departure functions at specific values, you will need to edit main.py manually.\\n\\nReference the departure functions as the following:\\nSingle.departure_H(temp, press, temp_crit, press_crit, acentric_factor)\\nSingle.departure_S(temp, press, temp_crit, press_crit, acentric_factor)\\nSingle.departure_U(temp, press, temp_crit, press_crit, acentric_factor)\\nSingle.departure_A(temp, press, temp_crit, press_crit, acentric_factor)\\nSingle.departure_G(temp, press, temp_crit, press_crit, acentric_factor)\\n\\nTo display the value, using the \"print\" command. For example:\\nprint(Single.departure_H(temp, press, temp_crit, press_crit, acentric_factor))\\n\\n\\nPlease contact me if you have any questions.\\nKirt McKenna\\nkmckenna@mymail.mines.edu'},\n",
       " {'repo': 'rufuspollock/shell-oil-spills-niger-delta',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'Oil spills in the Niger Delta associated with Shell Petroleum Development Company (of Nigeria) operations since January 2011. Data comes from Shell as listed on [their website](http://www.shell.com.ng/home/content/nga/environment_society/environment_tpkg/oil_spills/).\\n\\nFrom the site: \"SPDC has publicly reported oil spill statistics annually since 1995 and this website further enhances transparency by recording as fully as possible every spill that happens from our facilities as soon as it is possible to get accurate information. It tracks the progress of our spill response from when we learn about the leak to when clean-up is completed and signed off.\"\\n\\n### Data\\n\\nUnfortunately the data is loaded into each web page via javascript (this javascript is broken in major browsers (e.g. Chrome) and page shows as empty -- but does work in Firefox) but a bit of digging revealed source xml files, which are of the form:\\n\\n<http://www.shell.com.ng/home/page/nga/environment_society/environment_tpkg/oil_spills/data_2011/data_{month}.xml>\\n\\nFor example: <http://www.shell.com.ng/home/page/nga/environment_society/environment_tpkg/oil_spills/data_2011/data_january.xml> with HTML page at http://www.shell.com.ng/home/content/nga/environment_society/environment_tpkg/oil_spills/data_2011/january.html\\n\\n### License\\n\\n* Assume Shell is asserting no exclusive rights in the data (though no explicit license). Maintainer is licensing under the PDDL\\n\\n'},\n",
       " {'repo': 'oilmap/oilmap',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# OilMap-Web 🗺🛢 🌟\\nVisualizing oil data on country maps using OilMap\\n---\\n\\n### OilMap is an independent initiative to monitor the Oil & Gas industry of world promoting transparency and accountability in decision making and investment.\\n---\\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Froqueleal%2Foilmap.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Froqueleal%2Foilmap?ref=badge_shield)\\n\\n[live demo](https://oilmap.xyz)\\n\\n![screenshot](screenshot.gif)\\n---\\n## License\\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details\\n\\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Froqueleal%2Foilmap.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2Froqueleal%2Foilmap?ref=badge_large)\\n---\\n## Authors\\n* **Roque Leal** - *Initial work* - [Roque](https://www.roqueleal.me/)\\n---\\n## Community\\n- [Tiwtter](https://twitter.com/oilmapxyz)\\n- [Medium](https://medium.com/@roqueleal/world-oil-map-e46b774ea82b)\\n---\\n\\n## Certifications\\n* **ODI** - *Open Data Certificate * - [ODI](https://certificates.theodi.org/en/datasets/220195/certificate)\\n\\n---\\n## Acknowledgments\\n* **MapBox** - *Maps* - [MapBox](https://www.mapbox.com/)\\n* **Inspiration** - *Amnesty Oil Spills* - [MapBox Labs](https://labs.mapbox.com/amnesty/)\\n'},\n",
       " {'repo': 'f0nzie/data_science_ptech',\n",
       "  'language': 'CSS',\n",
       "  'readme_contents': 'This is a minimal example of a book based on R Markdown and **bookdown** (https://github.com/rstudio/bookdown). Please see the page \"Get Started\" at https://bookdown.org/ for how to compile this example.\\n\\nYou can find the preview of this example at https://bookdown.org/yihui/bookdown-demo/\\n'},\n",
       " {'repo': 'simulkade/peteng',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': '# peteng\\nSome useful Matlab/Julia functions for Petroleum Engineering applications\\n'},\n",
       " {'repo': 'stackyism/hpcl_support',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'This is the project was aimed at making a basic support page for HPCL (Hindustan Petroleum Corporation Limited), as an Internship Project for summer2013 @HPCL.\\nIt was made with the help of Cisco Jabber Web APIs as the software was already available.\\nThe APIs used were jabberwerx.js and jabberwerx.ui.js.\\nThe project can be viewed from support.html.\\n'},\n",
       " {'repo': 'Eshichi/Petroleum',\n",
       "  'language': 'CSS',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'salmansust/MachineLearning-TSF-PetroleumProduction',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'ecate/petroleum',\n",
       "  'language': 'Ruby',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'gagetyrussell/petroleum_engineering',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': \"# petroleum_engineering\\n\\n<h2>Main Folder: Decline Curve Analysis</h2>\\n<p><h3>sub Folder: fbProphet_Decline</h3></p>\\n<p><h3><b>fbprophet_DCA2.0.py</b></h3></p>\\n<body>\\n  <p><b>Summary:</b><br>\\n  Does decline curve analysis for any number of wells downloaded as production time series csv from drillinginfo (download the production   time series csv from drillinginfo for any number of wells and leave it exactly as is). Only thing to change is path to file. The most     important final output is stored in the 'prediction' variable, which will contain a dataframe for each well in the csv. One forecast       figure and one DCA figure will be saved for each well.\\n   </P>\\n   <ul>\\n    <li>Libraries needed:</li> \\n      <ul>\\n        <li>pandas</li> <li>matplotlib</li> <li>numpy</li> <li>fbprophet</li>\\n      </ul>\\n    <li>Data Format:</li> \\n      <ul>\\n        <li>Production Time Series csv from DrillingInfo AS-IS (any number of wells)</li>\\n        <li>Example uses '2wellsGrady Production Time Series.csv'</li>\\n      </ul>\\n    <li>Uncomment:</li>\\n      <ul>\\n        <li>m.plot(fcst) to plot each forecast (not a good idea for lots of wells)</li>\\n      </ul>\\n    <li>Figures:</li>\\n      <ul>\\n        <li>DCA [API#].png are the final DCA (Actual+Forecast) figures created and saved by the program (example used two wells)</li>\\n        <li>fcst [API#].png are the forecast plots generated with fbprophet (example used two wells)</li>\\n      </ul>\\n    </ul>\\n</body>\\n<p><h3>sub Folder: fbProphet_Decline</h3></p>\\n<p><h3><b>ARPS_decline.py</b></h3></p>\\n<body>\\n  <p><b>Summary:</b><br>\\n  Does exponential, hyperbolic, and harmonic DCA\\n   </P>\\n\\n\\n<h2>Folder: Reservoir Fluid Characterization</h2>\\n<p><h3><b>Res_Fluid_Clustering.py</b></h3></p>\\n<body>\\n  <p><b>Summary:</b><br>\\n  Uses KMeans clustering to characterize reservoir fluid types. This particular example creates three clusters and then further divides     the cluster with the lowest average initial GOR because it is dealing specifically with oil wells in Grady County producing from the       Woodford. Folium is used to map each well on an interactive map in the browser. \\n  </p>\\n  <ul>\\n    <li>Libraries needed:</li>\\n      <ul>\\n        <li>pandas</li> <li>matplotlib</li> <li>numpy</li> <li>sklearn</li> <li>folium</li>\\n      </ul>\\n    <li>Data Format:</li> \\n      <ul>\\n        <li>Wells Table csv from DrillingInfo AS-IS (any number of wells)</li>\\n        <li>Example uses 'WellsTableGrady.csv'</li>\\n      </ul>\\n  <li>Figures:</li>\\n    <ul>\\n      <li>res_fluid_clustermap.html is the output of the example ran using wells in Grady County clustered into four fluid types</li>\\n    </ul>\\n  </ul>\\n  </body>\\n  \\n\"},\n",
       " {'repo': 'swethababurao/BritishPetroleum',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'navjotwarade/National-Petroleum',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# National-Petroleum\\nAndroid App\\n\\n   In order to solve the problems of gas station owners we have created an \\nandroid app which will compare and display the prices of fuels from \\ndifferent suppliers and provide the cheapest price to gas station owners\\n\\n   This will enable an independent gas station owners to look for cheapest fuel \\nprice on app with one-click button rather than searching for emails from \\ndifferent suppliers.\\n\\n\\nTechnologies used:\\n\\nAndroid Technology\\n\\nIMAP Protocol-(Internet Message Access Protocol)\\n\\nJava API\\n'},\n",
       " {'repo': 'paip/dark-petroleum-atom-syntax',\n",
       "  'language': 'CSS',\n",
       "  'readme_contents': \"# dark-petroleum-syntax theme\\n\\nDark Petroleum syntax is the perfect match for the 'Solarized Dark' UI Theme.\\n\\n(Grab 'Solarized Dark' from this link: https://atom.io/themes/solarized-dark-ui)\\n\\n![A screenshot of your theme](https://raw.githubusercontent.com/paip/dark-petroleum-atom-syntax/master/dark-petroleum-syntax.png)\\n\"},\n",
       " {'repo': 'rezaGIS/OpenPetroleumMVC',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'shwrthy/Petroleum-Engineering-Rock-Mechanics',\n",
       "  'language': 'MATLAB',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'almersawi/IPR',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'liuyibox/ML-aided-Petroleum-Production-Predictor',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# ML-aided-Petroleum-Production-Predictor'},\n",
       " {'repo': 'ParthKhanna07/HackFest2k19',\n",
       "  'language': 'PHP',\n",
       "  'readme_contents': '# PetroBYTES\\n ![LOGO](https://driller.000webhostapp.com/client/favicon.png)\\n \\n A one stop solution to prevent all SCAM in the Petroleum Distribution\\n \\n### Prototype\\n[PetroBYTES client](https://driller.000webhostapp.com/client/)\\n[PetroBYTES Administrator](https://driller.000webhostapp.com/administrator/)\\n\\n### CSS Framework\\n[Bootstrap-Italia](https://italia.github.io/bootstrap-italia/)\\n\\n \\n### :fuelpump: What are the Petroleum Scams ? \\n\\n##### Deviation (Specially for bikes):- (Involvement of two employees of gas station) :- \\nWhen your turn comes you go and park your vehicle firmly and say XXX rs then you open the tank lock of your bike/scooty at same movement the person standing close to you to collect money(second person) will say something to you just to deviate You But the moment you respond then fist person will start filling petrol and you will not able to see ZERO on it. Scam is done. Meter/machine will show 5L but you never know how much is actually in you tank. \\nPlace : Almost every petrol pump in Pune.\\n\\n##### Taking advantage of your comfort level (Only for four wheelers) :-  \\n It happens on a machine where one side two wheelers and other side four wheelers are taking fuel. When you park your car at gas station they will tell you exact location where you need to stop and you do that. They will tell you to open the tank and we just pull a button below our seat to open tank lock, then he will tell you to check zero and you do that after that very smartly he will stand in between your left side mirror and fuel tank so that you cannot see what is happening. He will not start fuelling but he will pertain that fuelling is going on and all of a sudden you see that meter/machine stops at around 400-500rs or 100 .. 200 rs and the person will tell you something sir light cut gayi hai I will come is a second and he will quickly go to somewhere pertaining restarting Genset switch or something and the moment he comes backs he will continue fuelling from last reading whatever was that or he might reset it to zero and in the end you pay total amount. Previous reading amount and latest reading amount. SCAM is done. Guess how? In your first reading fuelling was not actually happening into your car it was for the two wheelers standing on other side of machine. Since staff was standing between your left side mirror and gas tank so you could not see that actually and you end up as a victim.\\nmy exp: Indian Oil petrol pump hinjewadi, Pune.\\n\\n##### Deviation (Applicable for all type of vehicles):- (Single employee of gas station can do this) :-  \\nYou ask petrol pump employee to fill up petrol for 800 rs and they will stop filling petrol after rs 200 only. When you say that you have asked for 800 rs then they will say sorry or something like could not hear... and they will restart fueling petrol and they will make sure their body is in such a position so that you can not see the meter when they restart fueling and this time they will stop at 600 rs reading when you say something then they will say sir I set the meter at zero after first time fueling of 200 rs so this time added for 600 rs only. You can fight but you can not win most of the time.\\n\\n ##### Disengaging the automated nozzle before the fuel stops \\nThis happens in the automated nozzle petrol pumps. All of us think that these auto pumps are tamper proof but they are not. The auto nozzles have a nozzle trigger lock, which when triggered, stays on till there is flow of fuel in the pipe and automatically pops off when the fuel flow stops. The auto nozzle was designed to prevent fuel fraud but there is a small technicality that unscrupulous attendants use to their advantage. The technicality is – it’s possible to manually override the trigger lock.\\n\\n\\n##### So how do we save you from all this ? \\n\\nWe have a Hardware component installed at each persons fuel tank . The hardware component consists of a FlowRate Sensor A nodeMCU module and a Weight Sensor (Not present in our domenstartion model) \\nThis Sensor measures the amount of liquid flowing through a particular cross-section as well the mass of the liquid flown . Using this data an OTP is generated which sent to the owner of the vehicle . Using this OTP the user can view his details from the PetroBYTES Website\\nThe website then asks for the amount of petrol that he actually wanted and then calculates the amount of money he has been doped of \\n\\nThe user can next select the address of the petrol pump from the dropdown and submit this data to a database.\\n\\nUsing this data We aim to predict the average FLowrate Error and Adulteration for each Petrol Pump using Linerar Regression Model . This data will be Publicly Available and hence the user can decide which is the best petrolpump for him.It also creates a pressure on the petrol pump owners to act responsibly since the entire system becomes transparent.\\n\\nThe system has been Updated For Five Petrol Pumps namely A to E\\nAdditional Datasets have been used for correctly doing the data Analysis\\n\\n#### :coffee: Developed By BYTES\\nSuggestions and Contributions are highly Appreciated\\n'},\n",
       " {'repo': 'SEPC/Journal-of-Petroleum-Science-Research',\n",
       "  'language': None,\n",
       "  'readme_contents': 'Journal-of-Petroleum-Science-Research\\n=====================================\\n\\nhttp://www.jpsr.org/\\nJournal of Petroleum Science Research (JPSR) is an international open-access and refereed journal dedicated to publishing the latest advancements in petroleum science. The goal of this journal is to record the latest findings and promote further research in these areas. Scholars from all relevant academic fields are invited to submit high-quality manuscripts that describe the latest, state-of-the-art research results or innovations.\\n'},\n",
       " {'repo': 'hackettma/CIRES-PETRO',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'jimmyneutroon/alfajrsomix',\n",
       "  'language': 'CSS',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'kylesarre/Reverse-Auction',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# Auctionator\\nA reverse-auction system for general petroleum industry contracting business\\nTo compile and build, run the code as a project in netbeans.\\n'},\n",
       " {'repo': 'anandvaibhav/FormulaDeck',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# FormulaDeck\\nAndroid app containing formulas for mechanical, civil and petroleum students\\n'},\n",
       " {'repo': 'Bingohong/PE-DataMining',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': '# PE-DataMining\\nData Mining about Petroleum Engineering, by Rock Mechanics Lab\\n## 数据挖掘方法在石油工程领域的探索\\n- 数据集: 提供部分处理后的数据，可用于实践探索\\n- jupyter notebook: 提供自身对于具体问题思考的具体求解过程，仅供参考\\n'},\n",
       " {'repo': 'rashidwadani/Decline_Curve_Analysis_Tool',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': '# Decline Curve Analysis Tool\\n> A Tool for Petroleum Engineers to\\n> Do Decline Curve Analysis Quickly\\n\\n\\n![](DCA_tool_snapshot.png)\\n\\nDecline Curve Analysis is a process of fitting an analytical curve (exponential, harmonic or hyperbolic) to production history data. This analysis is important for characterizing the performance of each well as well as forecasting the production in future.\\n\\nFor this tool, a hyperbolic model (most general DCA model) is used. \\n\\n![](equation.png)\\n\\nUsing robust curve fitting and optimization tools in Python, the code finds the best DCA parameters. The estimated parameters are\\n\\n1) qi: Initial Production Rate\\n2) di: Initial Decline Rate\\n3) b: Hyperbolic Exponent\\n\\n## Running the Code\\n\\nPlease run this code using a Python complier like Spyder. This tool use TKinter library for the frontend.\\n'},\n",
       " {'repo': 'MosGeo/BPSMAutoToolbox',\n",
       "  'language': 'MATLAB',\n",
       "  'readme_contents': '# The BPSM Automation Toolbox for Probabilistic Interactions\\n\\nThe BPSM Automation toolbox is a library to automate the creation, modification, and running of models used in the Schlumberger PetroMod software. The code is written in MATLAB (Octave support has not been tested).\\n\\n<div align=\"center\">\\n    <img width=800 src=\"https://github.com/MosGeo/BPSMAutoToolbox/blob/master/ReadmeFigures/Workflow.png\" alt=\"TopImage\" title=\"Image of particle pack\"</img>\\n</div>\\n\\n## Capabilities\\n- A template PetroMod project can be loaded into Matlab. \\n- Lithologies can be duplicated, mixed, modified, and deleted. \\n- Models can be duplicated, deleted, and simulated directly from Matlab. \\n- 1D, 2D, and 3D models can be modified.\\n- Custom open simulator scripts can be ran (OpenSimulator license is required). Using this, results can be read back into Matlab.\\n\\n## Getting started\\n- Create your PetroMod 2017.2 or newer \"template project\" (recent older versions should work too).\\n- Create your output script in python to export output if needed (see below).\\n- Simulate your template project and make sure everything works.\\n- See the \"main.m\" for the procedure to load your template project in Matlab, modify parameters, duplicate models and simulate them in Matlab. You can also read the report accomponied in the repository and the workshop material included.\\n\\n## General tips\\n- Save your PetroMod project in a folder that does not require administrator privileges.\\n- It is better to create all the required lithologies in one go before updating the project as writing the lithology files takes relatively long time (a couple of seconds), i.e., do not update the project in a for loop.\\n- If you are using lithology mixing, it is better to make sure everything is consistant by creating some mixes manually and comparing it to the mixes created by Matlab.\\n- Some parameters are internally saved with different unit than the one that is displayed in the PetroMod GUI.\\n\\n## Open Simulator scripts tips\\n- Open Simulator requires Python to be installed in the system. The version of required Python in 2018 or older is 2.7.\\n- Check out scripts folder in PetroMod script folder for example (e.g., \"C:\\\\Program Files\\\\Schlumberger\\\\PetroMod 2016.2\\\\scripts\")\\n- To activate your script, in the Simulator window, choose \"Output\", \"Open Simulator\" and select your script. Make sure your script run in the template project. Save the simulator window.\\n- Another option is to run the written script directly from matlab as given in the example file.\\n\\n## Future plans\\nThe basic framework is implemented now. Most needed functionalites can be automated in Matlab. Feature addition is on hold for now and new features will be added as needed. If you have suggestions, bugs, or feedback, please contact me through Github or Email. I would love to hear from you. What are the functionalities that you use the most? What are the things that you cannot do with it? Do you have a bug to report? You can create an Issue on GitHub or reach me directly at Mustafa.Geoscientist@outlook.com\\n\\n## Referencing\\nAl Ibrahim, M. A., 2019, Petroleum system modeling of heterogeneous organic-rich mudrocks, PhD Thesis, Stanford University, p. 131-135.\\n'},\n",
       " {'repo': 'doneria-anjali/genome',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Environmental Genome\\n\\nCSC 491/591 project on Environmental Genome for prediction of location of oil and petroleum refineries in United States based on PESTLE (Political, Economic, Social, Technical, Legal and Environmental) framework, using Python to develop the front & back-end of the application, and MySQL as the relational database.\\n\\nProblem Statement - \"Based on external factors such as population density, land conditions and other plant locations, is it favorable to build a chemical plant (oil and petroleum refinery) in a given location in the United States?\"\\n\\nThe objective of this project is to learn making data-driven decisions based on quantitative and qualitative nature of the data. The project helps in critically thinking about the given problem statement, the external factors contributing to the decision, deciding upon the right data sources, and building decision rules. The major steps involved were crawling the different web resources to collect the data, processing that data for storing in the relational data format, building decision rules for attributes, training the decision model and making predictions based on the zipcodes of United States.\\n\\nAll the data collected to build this application can be found under resources folder. The pre-requisites for running the application is availability of training and testing data in the database that is located under application/exported_data. It can be imported into the mysql database using addAllModelData.py script from application folder. More details for importing data can be found at https://docs.google.com/document/d/1D9NLfbDC19MmgJblQlQS4dHkf-cXh6s61VdUP156Mxg/edit?usp=sharing\\n\\nThe application runs from GUI.py under application folder taking following inputs:\\n1. Zipcode for which the prediction has to be made for building an oil and petroleum refinery\\n2. Radius (in miles) to collect attribute data for the given location\\n\\nThe GUI triggers a call to an API(http://www.zipcodeapi.com/) that dynamically fetches the list of zipcodes lying within the given radius (in miles) around the given location. Using this list of zipcodes, we fetch seaport, oil reserves, land prices, population density, elevation (using Google API), railroad, natural disaster, and existing plant data present in the database. This data is standardized to provide corresponding weights to these factors to be fed into the trained model (Random Forest Model) for prediction of \\'Yes\\' or \\'No\\'. If the system approves the decision of constructing an oil refinery in a given zipcode, few more details are given to the user for intelligence augumentation which includes frequency of earthquakes reported in that area in last 10 years, number of water sources surrounding that area and the general rules imposed by the local authority for drilling oil wells in the given location.\\n\\nMore details can be found in the final report at https://docs.google.com/document/d/199hdLw8dYUL4S65YVm6j8bpr_kjYe5dq8O2xgvdTOQM/edit?usp=sharing\\n\\n\\n\\nDepartment of Computer Science,\\nNorth Carolina State University,\\nRaleigh, NC.\\n\\n\\n\\n\\n'},\n",
       " {'repo': 'huiyi-outsourcing/diagram',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'diagram\\n=======\\n\\npetroleum project, dynamic diagram for real-time monitoring the drilling-well..\\n\\nIt is written in C#, using WPF framework, it provides a configuration file to configure almost everything.\\nSuch as diagram height, width, data types that the diagram support..'},\n",
       " {'repo': 'reallysaurabh/Optimization-Problems',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'dummybigjj/sis',\n",
       "  'language': 'PHP',\n",
       "  'readme_contents': '###################\\nWhat is CodeIgniter\\n###################\\n\\nCodeIgniter is an Application Development Framework - a toolkit - for people\\nwho build web sites using PHP. Its goal is to enable you to develop projects\\nmuch faster than you could if you were writing code from scratch, by providing\\na rich set of libraries for commonly needed tasks, as well as a simple\\ninterface and logical structure to access these libraries. CodeIgniter lets\\nyou creatively focus on your project by minimizing the amount of code needed\\nfor a given task.\\n\\n*******************\\nRelease Information\\n*******************\\n\\nThis repo contains in-development code for future releases. To download the\\nlatest stable release please visit the `CodeIgniter Downloads\\n<https://codeigniter.com/download>`_ page.\\n\\n**************************\\nChangelog and New Features\\n**************************\\n\\nYou can find a list of all changes for each release in the `user\\nguide change log <https://github.com/bcit-ci/CodeIgniter/blob/develop/user_guide_src/source/changelog.rst>`_.\\n\\n*******************\\nServer Requirements\\n*******************\\n\\nPHP version 5.6 or newer is recommended.\\n\\nIt should work on 5.3.7 as well, but we strongly advise you NOT to run\\nsuch old versions of PHP, because of potential security and performance\\nissues, as well as missing features.\\n\\n************\\nInstallation\\n************\\n\\nPlease see the `installation section <https://codeigniter.com/user_guide/installation/index.html>`_\\nof the CodeIgniter User Guide.\\n\\n*******\\nLicense\\n*******\\n\\nPlease see the `license\\nagreement <https://github.com/bcit-ci/CodeIgniter/blob/develop/user_guide_src/source/license.rst>`_.\\n\\n*********\\nResources\\n*********\\n\\n-  `User Guide <https://codeigniter.com/docs>`_\\n-  `Language File Translations <https://github.com/bcit-ci/codeigniter3-translations>`_\\n-  `Community Forums <http://forum.codeigniter.com/>`_\\n-  `Community Wiki <https://github.com/bcit-ci/CodeIgniter/wiki>`_\\n-  `Community Slack Channel <https://codeigniterchat.slack.com>`_\\n\\nReport security issues to our `Security Panel <mailto:security@codeigniter.com>`_\\nor via our `page on HackerOne <https://hackerone.com/codeigniter>`_, thank you.\\n\\n***************\\nAcknowledgement\\n***************\\n\\nThe CodeIgniter team would like to thank EllisLab, all the\\ncontributors to the CodeIgniter project and you, the CodeIgniter user.\\n'},\n",
       " {'repo': 'ssicard/well-mapping',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'Well Mapping Application\\nfor Louisiana State University Petroleum Engineering Department\\n\\n### Features\\n* mapping of all production and injection wells in louisiana\\n* display of all oil and gas fields in Louisiana\\n* Parish boundaries and names are displayed for easy reference\\n* charts per well and field to show production information by year for gas and oil charts\\n* statewide chart to display production information for the whole state\\n* ability to change layers visible on map\\n\\n### Preparing the Data\\n*\\n\\n\\n### Things to Note\\n* the parish boundary is defined by counties.json. If the boundaries change, this will need to be updated.\\n\\n### Data Sources\\n* [Louisiana Gas and Oil Fields with Cumulative Production from 1977](https://data.doi.gov/dataset/louisiana-gas-and-oil-fields-with-cumulative-production-from-1977-20143f413)\\n* [Department of Natural Resources Data Subscription](https://adfprodadm.dnr.state.la.us/DataSubscription/faces/login.jsf)\\n\\n\\n### Thanks to...\\n\\n* [Leaflet](https://github.com/Leaflet/Leaflet)\\n* [Leaflet.TileLayer.GeoJSON](https://github.com/glenrobertson/leaflet-tilelayer-geojson)\\n* [Twitter Bootstrap](http://twitter.github.io/bootstrap/)\\n* [jQuery](http://jquery.com/)\\n\\n\\nThis web portal has been compiled for educational use only, and my not be reproduced without permission. Please contact Jyotsna Sharma (jsharma@lsu.edu) for more information or permissions.\\n'},\n",
       " {'repo': 'hellopteromyini/Cutting-edge-technology-RNN-introduction',\n",
       "  'language': 'TeX',\n",
       "  'readme_contents': '# Cutting-edge-technology-RNN-introduction\\n\\nThis is the conclusion report of the computer science advanced technology lecture course of China University of petroleum submitted by myself, for reference only. If there are similarities, it is also a reference. \\\\^v^\\n\\n\\n## file structure\\n\\n* All pictures are media files(.png .jpg……).\\n* references.bib file holds references.\\n* main.tex file is the source code.\\n* 1808010217吴树晖.pdf file is a compiled Report.\\n\\n'},\n",
       " {'repo': 'softlandia/glasio',\n",
       "  'language': 'Lasso',\n",
       "  'readme_contents': '# golang las library #\\n\\n(c) softlandia@gmail.com\\n\\n>download: go get -u github.com/softlandia/glasio  \\n>install: go install\\n\\nThe library makes it easy to read or write data in LAS format.\\nThe main reason for the development was the need to read and bring in a uniform form a large number of LAS files obtained from various sources\\n\\nFeatures:\\n\\n1. The encoding is determined automatically\\n2. On reading performed validation of the key parameters and is integrity of the structure LAS file\\n3. Messages are generated for all inconsistencies:\\n    - zero value of important parameters\\n    - depth step change in data section\\n    - lack of curves section\\n    - conversion errors to a numerical value\\n    - mismatch of the step parameter declared in the header to the actual\\n    - duplication of curve names\\n4. Excluding critical errors, the library allows you to read the file and get data\\n5. Saving a file ensures the integrity of the structure and its readability for most other programs\\n6. It is possible to specify a dictionary of standard mnemonics; when reading a file, messages about curves that do not match the specified ones will be generated\\n7. It is possible to specify a dictionary of automatic substitution of mnemonics, respectively, curves with the given names will be renamed\\n\\n__WRAP__ las file not support\\n\\n## dependences ##\\n\\n- github.com/softlandia/cpd\\n- github.com/softlandia/xlib\\n\\n## examples ##\\n\\nsimple\\n\\n- make empty LAS file\\n- reads sample file \"expand_points_01.las\", write md file with messages\\n- saves the recovered LAS file \"expand_points_01+.las\"\\n\\nrepaire\\n\\n- reads all LAS files in current folder\\n- saves the recovered files to the same folder\\n\\n## tests ##\\n\\ncoverage 91%  \\nfolder \"data\" contain files for testing, no remove/change/add\\n'},\n",
       " {'repo': 'lehoangha/tomo2d_HeriotWatt',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# tomo2d_HeriotWatt\\nThis is a fork project from the tomo2d, created by Jun Korenaga at Yale university, aimed for helping seismic project(s) of the Institute of Petroleum, the Heriot Watt University \\n\\n## ACKNOWLEDGEMENT\\nThe project was originally forked from tomo2d repository at the Yale university, website: [http://people.earth.yale.edu/software/jun-korenaga](http://people.earth.yale.edu/software/jun-korenaga). It is created for supporting the work of Ms. Kim Phung Nguyen, a research assistant at the Institute of Petroleum, the Heriot Watt University. All changes and contribution should be reported back to the mentioned 2 parties.\\n\\n## TECHNICAL REVIEW\\n\\nThere are changes of source code in order to make the code be possibly compiled by the GNU Compiler Collection (gcc) version 4.9.2:\\n* the iostream.h declared in several \"error.cc\" were replace by iostream, which is a newer library\\n* Added the library cstdlib, and declared \"using namespace std;\" in some source files:\\n * util.cc\\n * corrlen.cc\\n * lsqr.cc\\n* in \"heap_deque.h\", any of push\\\\_back actions were replaced by this->push\\\\_back\\n'},\n",
       " {'repo': 'softlandia/glasio',\n",
       "  'language': 'Lasso',\n",
       "  'readme_contents': '# golang las library #\\n\\n(c) softlandia@gmail.com\\n\\n>download: go get -u github.com/softlandia/glasio  \\n>install: go install\\n\\nThe library makes it easy to read or write data in LAS format.\\nThe main reason for the development was the need to read and bring in a uniform form a large number of LAS files obtained from various sources\\n\\nFeatures:\\n\\n1. The encoding is determined automatically\\n2. On reading performed validation of the key parameters and is integrity of the structure LAS file\\n3. Messages are generated for all inconsistencies:\\n    - zero value of important parameters\\n    - depth step change in data section\\n    - lack of curves section\\n    - conversion errors to a numerical value\\n    - mismatch of the step parameter declared in the header to the actual\\n    - duplication of curve names\\n4. Excluding critical errors, the library allows you to read the file and get data\\n5. Saving a file ensures the integrity of the structure and its readability for most other programs\\n6. It is possible to specify a dictionary of standard mnemonics; when reading a file, messages about curves that do not match the specified ones will be generated\\n7. It is possible to specify a dictionary of automatic substitution of mnemonics, respectively, curves with the given names will be renamed\\n\\n__WRAP__ las file not support\\n\\n## dependences ##\\n\\n- github.com/softlandia/cpd\\n- github.com/softlandia/xlib\\n\\n## examples ##\\n\\nsimple\\n\\n- make empty LAS file\\n- reads sample file \"expand_points_01.las\", write md file with messages\\n- saves the recovered LAS file \"expand_points_01+.las\"\\n\\nrepaire\\n\\n- reads all LAS files in current folder\\n- saves the recovered files to the same folder\\n\\n## tests ##\\n\\ncoverage 91%  \\nfolder \"data\" contain files for testing, no remove/change/add\\n'},\n",
       " {'repo': 'cjayidoko/AAPG_Hackaton',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'Ahmed-Alhosany/OPA',\n",
       "  'language': None,\n",
       "  'readme_contents': '# OPA\\n\\nOPA is a powerful online platform for petroleum courses. Every course on OPA is instructed by top engineers in the oil and gas industry. OPA Is More Than Just An Academy. It’s Also a Community Of Like-Minded, Creative People, Looking To Make The learning better Better.\\n\\nLive Website preview\\n-----------\\n* (http://opacourses.com/opa/) \\n\\n Key features: \\n-----------\\n\\n* Responsive website\\n* Using framework Bootstrap\\n* Using JQuery Plugins \\n\\n\\nScreenshot\\n-----------\\n<p>\\n  <img src=\"screencapture-hyper-design-opa-2019-09-16-11_25_50.png\" width=\"100%\" />\\n</p>\\n\\n\\nAbout me\\n-----------\\n You can follow me at:\\n1. [Linkedin](https://www.linkedin.com/in/ahmed-alhoseny/)\\n2. [Behance](https://www.behance.net/ahmed-alhosany)\\n3. [Facebook](https://www.facebook.com/mido.hisham.777)\\n\\n  \\n\\n\\n'},\n",
       " {'repo': 'lakhanimanan111/DataAnalysis_SemanticWeb',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# DataAnalysis_SemanticWeb\\nThe aim of this project is to find out which is the most efficient energy source for electricity production and reduction of air pollution. The 3 datasets used in this project are about US electricity generation, consumption and pollution for the years 1990 – 2016 at Federal level. The datasets contain details regarding the energy sources used for the generation or production of electricity like Coal, Natural Gas, Petroleum and Geothermal. The pollution dataset consists of data about 3 different pollutants: CO2, SO2 and NOX which are produced during the usage of energy source to generate electricity.\\n'},\n",
       " {'repo': 'bcgov/mem-mmti',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# mem-mmti (nrs)\\nmem-mmti is a project of the Ministry of Energy and Mines in partnership with the OCIO DevOps Pathfinder, Ministry of Environment, and the Environmental Assessment Office.\\n\\nMEM MMTI is an offshoot derivative of the ESM application to support the Major Mines Transparency project of the Ministry of Energy and Mines.\\n\\nThis application is a tool to support the work of EAO staff, project proponents, and other stakeholders as environmental assessments are conducted.\\n\\n## Features\\n\\nThe features provided by the web-based ESM application include:\\n\\n* a public-facing view of documents\\n\\n## Usage\\n\\n## Requirements\\n\\n## Installation\\n\\n## Project Status\\n\\n## Goals/Roadmap\\n\\n## Getting Help or Reporting an Issue\\n\\n## How to Contribute\\n\\n## License\\n\\n    Copyright 2015 Province of British Columbia\\n\\n    Licensed under the Apache License, Version 2.0 (the \"License\");\\n    you may not use this file except in compliance with the License.\\n    You may obtain a copy of the License at \\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n    Unless required by applicable law or agreed to in writing, software\\n    distributed under the License is distributed on an \"AS IS\" BASIS,\\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n    See the License for the specific language governing permissions and\\n    limitations under the License.\\n   \\n'},\n",
       " {'repo': 'lastqxw/petroleum',\n",
       "  'language': 'Vue',\n",
       "  'readme_contents': '# petroleum\\n\\n> A Mpvue project\\n\\n## Build Setup\\n\\n``` bash\\n# install dependencies\\nnpm install\\n\\n# serve with hot reload at localhost:8080\\nnpm run dev\\n\\n# build for production with minification\\nnpm run build\\n\\n# build for production and view the bundle analyzer report\\nnpm run build --report\\n```\\n\\nFor detailed explanation on how things work, checkout the [guide](http://vuejs-templates.github.io/webpack/) and [docs for vue-loader](http://vuejs.github.io/vue-loader).\\n'},\n",
       " {'repo': 'jmarcelogimenez/petroSym',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# petroSym\\nGraphical User Interface for OpenFOAM oriented to petroleum simulations.\\n\\nRepository\\n----------\\n\\npetroSym is hosted on github: https://github.com/jmarcelogimenez/petroSym\\n\\nRequirements\\n------------\\n\\n- OpenFOAM 2.4\\n- Paraview\\n\\nrun:\\n\\n    sudo add-apt-repository http://www.openfoam.org/download/ubuntu\\n    sudo apt-get update -qq\\n    sudo apt-get install -y --force-yes openfoam240\\n    sudo apt-get install paraviewopenfoam410\\n    sudo apt-get install libfreetype6-dev libpng12-dev\\n    sudo apt-get install python-pip python2.7-dev libxext-dev python-qt4 pyqt4-dev-tools build-essential\\n\\nInstallation in the system (Option 1)\\n-------------------------------------\\n\\nIf you want to install PetroSym in your system, please follow this steps.\\n\\n1.0) First of all, clone the repository in your system:\\n\\n    git clone https://github.com/jmarcelogimenez/petroSym.git\\n    cd petroSym\\n\\n1.1) Package petroSym requires python 2.7, matplotlib>=1.5.0, PyQt and pyFoam 0.6.4:\\n\\n    sudo pip install -r requirements.txt\\n\\n1.2) Before installing or using petroSym, make sure the OpenFOAM environment variables are set by the following command:\\n\\n    source /opt/openfoam240/etc/bashrc\\n\\n1.3) After setting the OpenFOAM variables, run this script:\\n\\n    ./install_extras.sh\\n\\n1.4) Installation is based on setuptools. The --record option is important to uninstall files successfully:\\n\\n    sudo python setup.py install --record installation_files.txt\\n\\nInstallation in a virtual environment (Option 2)\\n------------------------------------------------\\n\\n2.0) To install the virtualenv packages, run the following script:\\n    \\n    sudo apt-get install python-virtualenv\\n\\n2.1) Then, to create the virtual environment, run:\\n\\n    virtualenv \"Myvirtualenv\"\\n\\n2.2) To activate the virtual environment, run:\\n\\n    cd \"Myvirtualenv\"\\n    source bin/activate\\n\\n2.3) Then, follow the steps 1.0 to 1.3 (In step 1.1, run the command without sudo)\\n\\n2.4) In order to make it work, you must create symbolic link of PyQt4 and sip:\\n\\n    ln -s /usr/lib/python2.7/dist-packages/PyQt4/ PATH_OF_THE_VIRTUALENV/lib/python2.7/site-packages/\\n    ln -s /usr/lib/python2.7/dist-packages/sip.so PATH_OF_THE_VIRTUALENV/lib/python2.7/site-packages/\\n\\n2.5) Then, you must run the setup.py without \"sudo\":\\n\\n    python setup.py install --record installation_files.txt\\n\\nTesting\\n-------\\n\\nTry:\\n\\n    petroSym\\n\\nUpdating\\n--------\\n\\nIn order to update petroSym, run the following script:\\n\\n    ./update_petroSym.sh\\n\\nCleaning\\n-------\\n\\nTo run the cleaner run:\\n\\n    python setup.py clean\\n'},\n",
       " {'repo': 'VFedyaev/RedPetroleum',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# RedPetroleum\\n'},\n",
       " {'repo': 'GitAsura/petroleum',\n",
       "  'language': 'PHP',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'yousuf190/Petroleum',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': '# Welcome to GitHub\\n\\nWelcome to GitHub—where millions of developers work together on software. Ready to get started? Let’s learn how this all works by building and publishing your first GitHub Pages website!\\n\\n## Repositories\\n\\nRight now, we’re in your first GitHub **repository**. A repository is like a folder or storage space for your project. Your project\\'s repository contains all its files such as code, documentation, images, and more. It also tracks every change that you—or your collaborators—make to each file, so you can always go back to previous versions of your project if you make any mistakes.\\n\\nThis repository contains three important files: The HTML code for your first website on GitHub, the CSS stylesheet that decorates your website with colors and fonts, and the **README** file. It also contains an image folder, with one image file.\\n\\n## Describe your project\\n\\nYou are currently viewing your project\\'s **README** file. **_README_** files are like cover pages or elevator pitches for your project. They are written in plain text or [Markdown language](https://guides.github.com/features/mastering-markdown/), and usually include a paragraph describing the project, directions on how to use it, who authored it, and more.\\n\\n[Learn more about READMEs](https://help.github.com/en/articles/about-readmes)\\n\\n## Your first website\\n\\n**GitHub Pages** is a free and easy way to create a website using the code that lives in your GitHub repositories. You can use GitHub Pages to build a portfolio of your work, create a personal website, or share a fun project that you coded with the world. GitHub Pages is automatically enabled in this repository, but when you create new repositories in the future, the steps to launch a GitHub Pages website will be slightly different.\\n\\n[Learn more about GitHub Pages](https://pages.github.com/)\\n\\n## Rename this repository to publish your site\\n\\nWe\\'ve already set-up a GitHub Pages website for you, based on your personal username. This repository is called `hello-world`, but you\\'ll rename it to: `username.github.io`, to match your website\\'s URL address. If the first part of the repository doesn’t exactly match your username, it won’t work, so make sure to get it right.\\n\\nLet\\'s get started! To update this repository’s name, click the `Settings` tab on this page. This will take you to your repository’s settings page. \\n\\n![repo-settings-image](https://user-images.githubusercontent.com/18093541/63130482-99e6ad80-bf88-11e9-99a1-d3cf1660b47e.png)\\n\\nUnder the **Repository Name** heading, type: `username.github.io`, where username is your username on GitHub. Then click **Rename**—and that’s it. When you’re done, click your repository name or browser’s back button to return to this page.\\n\\n<img width=\"1039\" alt=\"rename_screenshot\" src=\"https://user-images.githubusercontent.com/18093541/63129466-956cc580-bf85-11e9-92d8-b028dd483fa5.png\">\\n\\nOnce you click **Rename**, your website will automatically be published at: https://your-username.github.io/. The HTML file—called `index.html`—is rendered as the home page and you\\'ll be making changes to this file in the next step.\\n\\nCongratulations! You just launched your first GitHub Pages website. It\\'s now live to share with the entire world\\n\\n## Making your first edit\\n\\nWhen you make any change to any file in your project, you’re making a **commit**. If you fix a typo, update a filename, or edit your code, you can add it to GitHub as a commit. Your commits represent your project’s entire history—and they’re all saved in your project’s repository.\\n\\nWith each commit, you have the opportunity to write a **commit message**, a short, meaningful comment describing the change you’re making to a file. So you always know exactly what changed, no matter when you return to a commit.\\n\\n## Practice: Customize your first GitHub website by writing HTML code\\n\\nWant to edit the site you just published? Let’s practice commits by introducing yourself in your `index.html` file. Don’t worry about getting it right the first time—you can always build on your introduction later.\\n\\nLet’s start with this template:\\n\\n```\\n<p>Hello World! I’m [username]. This is my website!</p>\\n```\\n\\nTo add your introduction, copy our template and click the edit pencil icon at the top right hand corner of the `index.html` file.\\n\\n<img width=\"997\" alt=\"edit-this-file\" src=\"https://user-images.githubusercontent.com/18093541/63131820-0794d880-bf8d-11e9-8b3d-c096355e9389.png\">\\n\\n\\nDelete this placeholder line:\\n\\n```\\n<p>Welcome to your first GitHub Pages website!</p>\\n```\\n\\nThen, paste the template to line 15 and fill in the blanks.\\n\\n<img width=\"1032\" alt=\"edit-githuboctocat-index\" src=\"https://user-images.githubusercontent.com/18093541/63132339-c3a2d300-bf8e-11e9-8222-59c2702f6c42.png\">\\n\\n\\nWhen you’re done, scroll down to the `Commit changes` section near the bottom of the edit page. Add a short message explaining your change, like \"Add my introduction\", then click `Commit changes`.\\n\\n\\n<img width=\"1030\" alt=\"add-my-username\" src=\"https://user-images.githubusercontent.com/18093541/63131801-efbd5480-bf8c-11e9-9806-89273f027d16.png\">\\n\\nOnce you click `Commit changes`, your changes will automatically be published on your GitHub Pages website. Refresh the page to see your new changes live in action.\\n\\n:tada: You just made your first commit! :tada:\\n\\n## Work with GitHub on your computer using GitHub Desktop\\n\\n**GitHub Desktop** is a free app from GitHub for Windows and Mac that allows you to easily work with your GitHub repositories from your computer. You just saw how you can commit to a repository from GitHub.com, but most developers do the majority of their work from their computer (locally) before pushing it up to GitHub. So let’s try that out!\\n\\n[Download GitHub Desktop](https://desktop.github.com/)\\n\\n## Practice: Use GitHub Desktop and an editor to make a change from your computer\\n\\nStart by downloading GitHub Desktop if you haven’t already done so, and install it on your computer. Go through the GitHub Desktop onboarding steps, and when you get to the “Let’s get started” screen, go ahead and choose the repository you were just working with on GitHub.com, and click “Clone.”\\n\\n### Using an editor to make changes\\n\\nLet’s make sure you have a text editor on your computer - this is what you\\'ll use to actually make changes to your files. If you already know you have an editor, then skip to the next step. Otherwise, download and install either [Visual Studio Code](https://code.visualstudio.com/) or [Atom](https://atom.io/) and restart GitHub Desktop before proceeding to the next step.\\n\\nLet’s make a change to your GitHub Pages site, just like you did on GitHub.com, except this time we’re going to do it all from your computer. From GitHub Desktop, click the “Open in…” button in the middle of the screen to “open the repository in your external editor” that you just downloaded. \\n\\n![Open-in-editor](https://user-images.githubusercontent.com/721500/63188833-82fb9600-c030-11e9-8777-a67c1713d59f.png)\\n\\nIn the left sidebar, click the `index.html` file to open it, and go ahead and add another line. Maybe, “Building websites is fun! You should try it too!” or whatever you want to add. \\n\\n![Make-changes](https://user-images.githubusercontent.com/721500/63188832-82fb9600-c030-11e9-9f7b-7d15385a16f0.png)\\n\\nNow switch back to GitHub Desktop, and you should see the change you made.\\n\\n![View-changes](https://user-images.githubusercontent.com/721500/63188835-82fb9600-c030-11e9-8980-43a8231ca23a.png)\\n\\n### Commit your changes\\n\\nNow you can commit your changes by typing a message in the `Summary` box at the bottom left, and then click the blue `Commit` button below that.\\n\\n![Commit-changes](https://user-images.githubusercontent.com/721500/63188831-8262ff80-c030-11e9-809a-f87d8b544935.png)\\n\\n### Push your changes to GitHub.com\\n\\nOne of the great things about working on things on your computer is that you get to control when other people see them. Now let’s push your commit to GitHub.com as well so it’s saved there and you can publish it to your site. Click the “Push origin” button to push your commit to GitHub.com. \\n\\n![Push-to-GitHub](https://user-images.githubusercontent.com/721500/63188834-82fb9600-c030-11e9-9d8e-6c6ed6d48504.png)\\n\\nNow click the “View on GitHub” button to get back to your repository’s page on GitHub.com.\\n\\n![View-on-GitHub](https://user-images.githubusercontent.com/721500/63188836-82fb9600-c030-11e9-9bc5-cf304398500d.png)\\n\\n### Deploy and see your changes live on your GitHub Pages website!\\n\\nOnce you commit your changes, they are automatically published on your GitHub Pages website. Refresh your browser to see it live!\\n\\n### Celebrate!\\n\\nHooray! Now you have your repository linked between your computer and GitHub.com. In the future, you can use GitHub Desktop to push any changes you decide to make from your computer.\\n\\n## Extra Credit: Keep on building!\\n\\nChange the placeholder Octocat gif on your GitHub Pages website by [creating your own personal Octocat emoji](https://myoctocat.com/build-your-octocat/) or [choose a different Octocat gif from our logo library here](https://octodex.github.com/). Add that image to line 12 of your `index.html` file, in place of the `<img src=` link.\\n\\nWant to add even more code and fun styles to your GitHub Pages website? [Follow these instructions](https://github.com/github/personal-website) to build a fully-fledged static website.\\n\\n![octocat](./images/create-octocat.png)\\n\\n## Everything you need to know about GitHub\\n\\nGetting started is the hardest part. If there’s anything you’d like to know as you get started with GitHub, try searching [GitHub Help](https://help.github.com). Our documentation has tutorials on everything from changing your repository settings to configuring GitHub from your command line.\\n'},\n",
       " {'repo': '929528/PetroleumAccounting',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'PetrileumAccounting\\n===================\\n\\nPetroleum accounting for XADO Corp.\\n'},\n",
       " {'repo': 'r4mbo7/petroleum',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'Rakesh2017/PetroleumDriver',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'ChaoLab/PetroleumMicrobiome',\n",
       "  'language': None,\n",
       "  'readme_contents': 'Petroleum reservior MiSeq 16S rRNA gene sequencing (archaeal 16S rRNA gene and prokaryotic 16S rRNA gene)    \\n\\nMiSeq 16S rRNA gene (archaeal 16S rRNA gene and prokaryotic 16S rRNA gene) sequences of aqueous and oil phases from eight petroleum reservoir samples across China    \\n\\nThe 14 DNAs from aqueous and oil phases of petroleum reservoir samples across China have been extracted and amplified by Arch347F/Arch806R primer pair (for archaeal 16S rRNA gene) and 515F/909R primer pair (for prokaryotic 16S rRNA gene). The corresponding microbial communities of these 14 samples were obtained.    \\n\\nArch347F/Arch806R: GYGCASCAGKCGMGAAW/GGACTACVSGGGTATCTAAT    \\n515F/909R: GTGCCAGCMGCCGCGGTAA/GGACTACHVGGGTWTCTAAT    \\n\\nOriginal sequencing files: Archaea_sequencing_fasta_and_fastq_files.zip (splitted into 11 files)    \\nOriginal sequencing files: Prokaryotes_sequencing_fasta_and_fastq_files.zip    \\n'},\n",
       " {'repo': 'pdspooner/spooner-petro',\n",
       "  'language': 'CSS',\n",
       "  'readme_contents': 'spooner-petro\\n=============\\n\\nSpooner Petroleum\\n'},\n",
       " {'repo': 'jcellashton/Petroleum',\n",
       "  'language': 'CSS',\n",
       "  'readme_contents': '# Petroleum\\nPetroleum site\\n'},\n",
       " {'repo': 'zkn365/petroleum_books',\n",
       "  'language': None,\n",
       "  'readme_contents': '# petroleum_books\\npetroleum_books\\n'},\n",
       " {'repo': 'khatabKhatab/petroleumfuture3.github.io',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'morganmadell/PetroleumDeclineAnalysis',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# PetroleumDeclineAnalysis\\n\\nVarious functions to perform decline analysis on monthly Canadian oil and gas production data pulled from standard data sources.  Includes functions to perform exponential, harmonic and generalized Arps declines.  Also includes functions to estimate abandonment rates, perform boot strap sampling.\\n\\n'},\n",
       " {'repo': 'Turbo93/petroleumPiplinePatrol',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'paloukari/Petroleum',\n",
       "  'language': 'R',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'IGDML/PE',\n",
       "  'language': 'Vue',\n",
       "  'readme_contents': '# PE\\nPetroleum Explorer\\n'},\n",
       " {'repo': 'rainedream/BGC-PetroleumExploration',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'aislen404/petroleum',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'XiGongZi/petroleum',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'bmsingh12/yipl-challenge',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# yipl-intern-petroleum-report\\nThis repository contains the source code for the YIPL intern coding challenge.\\n\\n## Getting Started\\nThese instructions will get you a copy of the project up and running on your local machine.\\n\\n### Prerequisites\\n* Python : version 3\\n* Pandas Library\\n* MySql: 8.0\\n\\n### Installation\\n\\n```\\n$ git clone https://github.com/<username>/yipl-challenge.git\\n$ cd yipl_challenge\\n```\\n\\n### Creating Virtual Environment\\nTo create a virtual environment, go to your project's directory and run virtualenv.\\n\\n#### On macOS and Linux:\\n```\\npython3 -m virtualenv venv\\n```\\n#### On windows:\\n```\\npy -m virtualenv venv\\n```\\nNote : The second argument is the location to create the virtualenv. Generally, you can just create this in your project and call it venv. [ virtualenv will create a virtual Python installation in the venv folder.]\\n\\n### Activating Virtual Environment\\nBefore you can start installing or using packages in your virtualenv, you'll need to activate it.\\n\\n#### On macOS and Linux:\\n```\\nsource venv/bin/activate\\n```\\n#### On windows:\\n```\\n.\\\\venv\\\\Scripts\\\\activate\\n```\\n\\n### Confirming virtualenv by checking location\\n#### On macOS and Linux:\\n```\\nwhich python\\n```\\nOutput : .../venv/bin/python\\n\\n#### On windows:\\n```\\nwhere python\\n```\\nOutput : .../venv/bin/python.exe\\n\\n### Installing required packages with pip\\nFor installing mysql connector:\\n```\\npip3 install mysql-connector-python\\n```\\nFor installing pandas library:\\n```\\npip install pandas\\n```\\n### Creating the MySQL table \\nRun the following query to create the sql table:\\n```\\ncreate table yipl.challenge (\\n    year bigint,\\n    petroleumProduct varchar(30),\\n    sale bigint\\n)\\n```\\nIn the driver code, set the respective fields:\\n* host\\n* user\\n* password\\n* database name\\n##### Example for local environment:\\n* host=localhost\\n* user=root\\n* password=password\\n* database= yipl\\n\\n### Running the App\\nGet inside the project directory and enter the following command line code at terminal for Linux/MacOS or command prompt for Windows.\\n```\\npython report.py\\n```\\n\\n### Running Test Cases\\nThe test cases can be found inside the report_test.py file.\\nTo run the test cases, simply run the following command in the terminal:\\n```\\npython -m unittest report_test.py\\n```\\n* The -m option instructs python to run the unit test module as a script.\\n\"},\n",
       " {'repo': 'phamthanhnghia/de',\n",
       "  'language': 'PHP',\n",
       "  'readme_contents': '# Yii-framework-1.1.13-SPC-demo\\n'},\n",
       " {'repo': 'nsankouakou/PscSystem',\n",
       "  'language': 'CSS',\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'osgirl/BpReports',\n",
       "  'language': None,\n",
       "  'readme_contents': 'No README'},\n",
       " {'repo': 'tehblooguy/Petroleum-Expansion',\n",
       "  'language': None,\n",
       "  'readme_contents': 'Petroleum-Expansion\\n===================\\n\\nWIP addon to the Minecraft mod BuildCraft.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acquire.scrape_github_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
